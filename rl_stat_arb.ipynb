{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1bb7b54",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Agent for Statistical Arbitrage\n",
    "\n",
    "This notebook demonstrates how to configure and train a reinforcement learning agent for statistical arbitrage using a snapshot model. The agent will learn to identify and exploit statistical arbitrage opportunities in financial markets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e79612d",
   "metadata": {},
   "source": [
    "## Install necessary libraries\n",
    "\n",
    "The following libraries are required for this notebook. If you haven't installed them yet, you can do so using running the cell below or by using pip install in your terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b434b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install -U numpy pandas pyarrow gdown gymnasium stable-baselines3 torch matplotlib tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4664e80a",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "The configuration section sets up the parameters for the reinforcement learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34fbc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"DATA\": {\n",
    "        \"drive_folder_id\": \"1uXEBUyySypdsW_ZqL-RZ3d1bWdIZisij\", # google drive folder ID (can be found in the URL)\n",
    "        \"structure\": {\"1d\":\"ohlcv_1d\", # subfolder structure inside the drive folder\n",
    "                      \"1h\":\"ohlcv_1h\",\n",
    "                      \"15m\":\"ohlcv_15m\",\n",
    "                      \"5m\":\"ohlcv_5m\",\n",
    "                      \"1m\":\"ohlcv_1m\"},\n",
    "        \"file_pattern\": \"{TICKER}_{FREQ}.parquet\", # file naming pattern\n",
    "        \"tickers\": [\"BTC\",\"ETH\"], # pairs of assets to trade\n",
    "        \"sampling\": \"1h\", # data sampling frequency (1m, 5m, 15m, 1h, 1d)\n",
    "        \"price_point\": \"close\", # what price point to use for returns calculation (open, high, low, close, etc)\n",
    "        \"forward_fill\": True, # forward fill missing data\n",
    "        \"drop_na_after_ffill\": True, # drop rows with NA values after forward filling\n",
    "        \"cache_dir\": \"./data_cache\", # local cache directory to store downloaded files\n",
    "        \"file_ids\": {\"BTC_1h\": \"1-sBNQpEFGEpVO3GDFCkSZiV3Iaqp2vB_\", # map and define files to download (all data will be downloaded if left empty)\n",
    "                     \"ETH_1h\": \"1kj8G1scpFuEYTTXKEUzF9pwgGI2WFFL9\"\n",
    "                     },\n",
    "    },\n",
    "    \"ENV\": {\n",
    "        \"include_cash\": True, # include cash as a third asset\n",
    "        \"shorting\": True, # allow shorting\n",
    "        \"lookback_window\": 64, # lookback for feature slicing, meaning that the observation for each step will include data from the last 'lookback_window' time steps\n",
    "        \"features\": {\n",
    "            \"vol_window\": 64, # rolling volatility window\n",
    "            \"rsi_period\": 14, # RSI lookback period\n",
    "            \"volume_change\": True, # include volume change as a feature\n",
    "            \"normalize\": True # normalize features using rolling z-score\n",
    "        },\n",
    "        \"transaction_costs\": { # transaction costs settings for exchange (e.g. Hyperliquid)\n",
    "            \"commission_bps\": 5.0, # commission in basis points (bps)\n",
    "            \"slippage_bps\": 5.0, # slippage in basis points (bps)\n",
    "        },\n",
    "        \"reward\": {\n",
    "            \"risk_lambda\":0.001 # risk penalty coefficient (lambda), a.k.a. risk aversion factor\n",
    "            },\n",
    "        \"constraints\": {\n",
    "            \"min_weight\":0.0,\n",
    "            \"max_weight\":1.0,\n",
    "            \"sum_to_one\":True\n",
    "            },\n",
    "        \"seed\": 42\n",
    "    },\n",
    "    \"SPLITS\": { # date splits for training, validation, and testing\n",
    "        \"data_start\":\"2024-09-02\", # start date of the entire dataset\n",
    "        \"data_end\":\"2025-09-02\", # end date of the entire dataset\n",
    "        \"train\":[\"2024-09-02\",\"2025-06-15\"], # training period\n",
    "        \"val\":[\"2025-06-16\",\"2025-07-15\"], # validation period\n",
    "        \"test\":[\"2025-07-16\",\"2025-09-02\"], # testing period\n",
    "        \"walk_forward\": True, # whether to use walk-forward splits (e.g. sliding window)\n",
    "        \"wf_train_span_days\": 180, # training window span in days for walk-forward\n",
    "        \"wf_test_span_days\": 30, # testing window span in days for walk-forward\n",
    "        \"wf_step_days\": 30 # step size in days to move the window forward\n",
    "    },\n",
    "    \"RL\": {\n",
    "        \"timesteps\":10,\n",
    "        \"policy\":\"MlpPolicy\",\n",
    "        \"gamma\":0.99,\n",
    "        \"gae_lambda\":0.95,\n",
    "        \"clip_range\":0.2,\n",
    "        \"n_steps\":1024,\n",
    "        \"batch_size\":256,\n",
    "        \"learning_rate\":3e-4,\n",
    "        \"ent_coef\":0.0,\n",
    "        \"vf_coef\":0.5,\n",
    "        \"max_grad_norm\":0.5\n",
    "    },\n",
    "    \"EVAL\": {\n",
    "        \"plots\":True,\n",
    "        \"reports_dir\":\"./reports\"\n",
    "    },\n",
    "    \"IO\": {\n",
    "        \"models_dir\":\"./models\",\n",
    "        \"tb_logdir\":\"./tb\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568820c4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4143bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gdown\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import pytz\n",
    "\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reinforcement Learning\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# Deep Learning\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f12ee23",
   "metadata": {},
   "source": [
    "## Set Computation Device\n",
    "\n",
    "This section sets the computation device for training the model. It checks if a GPU is available and sets it as the device; otherwise, it defaults to CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bc4ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on cuda GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"CUDA is available. Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# run on Apple Silicon\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS is available. Using Apple Silicon GPU.\")\n",
    "\n",
    "# run on CPU (slow)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA and MPS are not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63981a61",
   "metadata": {},
   "source": [
    "## Set seeds for reproducibility\n",
    "\n",
    "This section sets the random seeds for various libraries to ensure that the results are reproducible.\n",
    "\n",
    "Note: It is good practice to type (set data types) for function and method parameters for better code maintainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775695a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed: int = 42):\n",
    "    random.seed(seed) # seed for random module\n",
    "    np.random.seed(seed) # seed for numpy module\n",
    "    try:\n",
    "        torch.manual_seed(seed) # seed for torch module\n",
    "        if torch.cuda.is_available(): # seed for CUDA device\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        elif torch.backends.mps.is_available(): # seed for Apple Silicon device\n",
    "            torch.backends.mps.manual_seed_all(seed)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "set_all_seeds(CONFIG[\"ENV\"][\"seed\"]) # set all seeds for reproducibility\n",
    "\n",
    "# set annualization factors for different timeframes\n",
    "ANNUALIZATION = {\"1m\":365*24*60,\n",
    "                 \"5m\":365*24*12,\n",
    "                 \"15m\":365*24*4,\n",
    "                 \"1h\":365*24,\n",
    "                 \"1d\":365}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b556fc3",
   "metadata": {},
   "source": [
    "## Fetch Data from Google Drive\n",
    "\n",
    "This section handles downloading data from Google Drive. It supports two methods: downloading an entire folder or downloading specific files by their IDs. The data will be cached locally for faster subsequent loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e7db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_ID = CONFIG[\"DATA\"][\"drive_folder_id\"] # suffix for Google Drive download URL\n",
    "CACHE_DIR = CONFIG[\"DATA\"][\"cache_dir\"] # data will be cached here for faster subsequent loading\n",
    "FILE_IDS = CONFIG[\"DATA\"][\"file_ids\"] # mapping of data to its URL suffix\n",
    "\n",
    "download_all = True if FILE_IDS == {} else False # this determines whether to download entire folder or specific files by their IDs (s. CONFIG)\n",
    "\n",
    "# create directory if it doesn't already exist\n",
    "def ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# download entire folder from Google Drive\n",
    "def download_drive_folder(root_id: str, out_dir: str):\n",
    "    print(\"Mirroring Google Drive folder locally...\")\n",
    "    gdown.download_folder(id=root_id, output=out_dir, quiet=False, use_cookies=False)\n",
    "    print(\"Folder mirroring complete.\")\n",
    "\n",
    "# download specific files from Google Drive by their file IDs\n",
    "def targeted_download_by_ids(file_id_map: Dict[str, str], out_dir: str):\n",
    "    ensure_dir(out_dir)\n",
    "    for name, fid in file_id_map.items():\n",
    "        # check if file already exist in local cache folder\n",
    "        if os.path.exists(os.path.join(out_dir, name)) or os.path.exists(os.path.join(out_dir, f\"{name}.parquet\")):\n",
    "            print(f\"File {name} already exists in cache. Skipping download.\")\n",
    "            continue\n",
    "        \n",
    "        suffix = name if name.endswith(\".parquet\") else f\"{name}.parquet\" # ensure .parquet suffix\n",
    "        out_path = os.path.join(out_dir, suffix) # full output path\n",
    "        print(f\"Downloading {name} -> {out_path}\") \n",
    "        url = f\"https://drive.google.com/uc?id={fid}\" # prefix download URL\n",
    "        gdown.download(url, out_path, quiet=False, use_cookies=False) # download file\n",
    "\n",
    "# check if cache directory exists, if not create it\n",
    "ensure_dir(CACHE_DIR)\n",
    "\n",
    "if download_all: # download entire folder\n",
    "    download_drive_folder(ROOT_ID, CACHE_DIR)\n",
    "else: # download specific files by their IDs\n",
    "    targeted_download_by_ids(FILE_IDS, CACHE_DIR)\n",
    "\n",
    "print(\"Download step complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b151dd54",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Once the data is downloaded, this section loads the data into a pandas DataFrame for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c9541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling = CONFIG[\"DATA\"][\"sampling\"] # data sampling frequency (1m, 5m, 15m, 1h, 1d)\n",
    "subfolder = CONFIG[\"DATA\"][\"structure\"][sampling] # subfolder name inside the drive folder is determined by the sampling frequency\n",
    "pattern_fmt = CONFIG[\"DATA\"][\"file_pattern\"] # file naming pattern (s. CONFIG)\n",
    "tickers = CONFIG[\"DATA\"][\"tickers\"] # list of tickers to load\n",
    "forward_fill = CONFIG[\"DATA\"][\"forward_fill\"] # whether to forward fill missing data\n",
    "drop_na_after_ffill = CONFIG[\"DATA\"][\"drop_na_after_ffill\"] # whether to drop NA values after forward filling\n",
    "\n",
    "# function to find parquet file path\n",
    "def find_parquet_path(ticker: str, sampling: str) -> str:\n",
    "    fname = pattern_fmt.format(TICKER=ticker, FREQ=sampling)\n",
    "    # try subfolder first\n",
    "    candidates = glob.glob(os.path.join(CACHE_DIR, \"**\", subfolder, fname), recursive=True)\n",
    "    # if not found, try flat cache\n",
    "    if not candidates:\n",
    "        candidates = glob.glob(os.path.join(CACHE_DIR, \"**\", fname), recursive=True)\n",
    "    # if still not found, try direct file in cache (for file_ids downloads)\n",
    "    if not candidates:\n",
    "        direct_path = os.path.join(CACHE_DIR, fname)\n",
    "        if os.path.exists(direct_path):\n",
    "            candidates = [direct_path]\n",
    "    # if nothing is found at all, raise an error\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f\"Could not find {fname} under {CACHE_DIR}.\")\n",
    "    return candidates[0]\n",
    "\n",
    "# function to localize timestamps and align dataframes\n",
    "def localize_and_align(df: pd.DataFrame, tz_in: str = None, tz_out: str = None) -> pd.DataFrame:\n",
    "    # convert millisecond timestamps to datetime\n",
    "    if 'datetime' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['datetime'], unit='ms', utc=True)\n",
    "        df = df.set_index('timestamp')\n",
    "    # make column names lowercase for consistency\n",
    "    cols = {c: c.lower() for c in df.columns}\n",
    "    df = df.rename(columns=cols)\n",
    "    return df.sort_index()\n",
    "\n",
    "dfs = {}\n",
    "for t in tickers:\n",
    "    pth = find_parquet_path(t, sampling) # find file path\n",
    "    tmp = pd.read_parquet(pth) # read parquet file\n",
    "    tmp = localize_and_align(tmp) # standardize timestamps and column names\n",
    "    if forward_fill: # forward fill missing data if specified\n",
    "        tmp = tmp.ffill()\n",
    "    if drop_na_after_ffill: # drop rows with NA values after forward filling\n",
    "        tmp = tmp.dropna()\n",
    "    dfs[t] = tmp # store in dictionary\n",
    "\n",
    "common_index = None\n",
    "# find common index across all dataframes\n",
    "for t, df in dfs.items():\n",
    "    common_index = df.index if common_index is None else common_index.intersection(df.index)\n",
    "# reindex all dataframes to the common index and drop any remaining NA values\n",
    "for t in tickers:\n",
    "    dfs[t] = dfs[t].reindex(common_index).dropna()\n",
    "\n",
    "# print the shape of each dataframe to ensure alignment\n",
    "print({t: dfs[t].shape for t in tickers})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a77e6",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "This section performs feature engineering on the loaded data. It includes creating new features, normalizing data, and preparing the dataset for training the reinforcement learning agent. To ensure statistical arbitrage strategy is imitated, create relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b326bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cfg = CONFIG[\"ENV\"][\"features\"]\n",
    "price_col = CONFIG[\"DATA\"][\"price_point\"]\n",
    "\n",
    "# function to compute relative strength index (momentum indicator)\n",
    "def compute_rsi(series: pd.Series, period: int = 14) -> pd.Series:\n",
    "    delta = series.diff() # compute price changes\n",
    "    up = (delta.clip(lower=0)).ewm(alpha=1/period, adjust=False).mean() # average of upward price changes\n",
    "    down = (-delta.clip(upper=0)).ewm(alpha=1/period, adjust=False).mean() # average of downward price changes\n",
    "    rs = up / (down + 1e-12) # relative strength ratio indicates if upward or downward momentum is stronger\n",
    "    rsi = 100 - (100 / (1 + rs)) # converts RS to RSI bounded between 0 and 100\n",
    "    return rsi\n",
    "\n",
    "# function to create features dataframe\n",
    "def make_features(df: pd.DataFrame, price_col: str, vol_window: int, rsi_period: int, volume_change: bool):\n",
    "    out = pd.DataFrame(index=df.index) # initialize output dataframe\n",
    "    out[\"ret\"] = np.log(df[price_col]).diff(1) # log returns\n",
    "    out[\"vol\"] = out[\"ret\"].rolling(vol_window).std().fillna(0.0) # rolling volatility\n",
    "    out[\"rsi\"] = compute_rsi(df[price_col], rsi_period).fillna(50.0) # relative strength index\n",
    "    if \"volume\" in df.columns and volume_change: # log volume change if volume data is available\n",
    "        out[\"volchg\"] = np.log(df[\"volume\"].replace(0, np.nan)).diff().fillna(0.0) # log volume change\n",
    "    else:\n",
    "        out[\"volchg\"] = 0.0 # if no volume data, set to zero\n",
    "    return out\n",
    "\n",
    "features_by_ticker = {}\n",
    "for t in tickers: # create features for each ticker\n",
    "    fdf = make_features(dfs[t], price_col, feat_cfg[\"vol_window\"], feat_cfg[\"rsi_period\"], feat_cfg[\"volume_change\"])\n",
    "    features_by_ticker[t] = fdf # store features in dictionary\n",
    "\n",
    "panel_cols = []\n",
    "for t in tickers: # create multi-index columns\n",
    "    for col in [\"ret\",\"vol\",\"rsi\",\"volchg\"]:\n",
    "        panel_cols.append((t, col))\n",
    "panel = pd.concat([features_by_ticker[t][[\"ret\",\"vol\",\"rsi\",\"volchg\"]] for t in tickers], axis=1) # combine features horizontally\n",
    "panel.columns = pd.MultiIndex.from_tuples(panel_cols, names=[\"ticker\",\"feature\"]) # set multi-index columns\n",
    "panel = panel.dropna() # remove rows with missing values\n",
    "\n",
    "print(panel.tail())\n",
    "panel.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da1a418",
   "metadata": {},
   "source": [
    "## Feature scaling and state tensor construction\n",
    "\n",
    "This section normalizes the features and constructs the state tensors required for training the reinforcement learning agent. State tensors are multi-dimensional arrays that represent the current state of the environment that the RL agent uses to make decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f681542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = CONFIG[\"ENV\"][\"lookback_window\"] # lookback window \n",
    "normalize = CONFIG[\"ENV\"][\"features\"][\"normalize\"] # normalization method\n",
    "\n",
    "# rolling z-score normalization for features\n",
    "def rolling_zscore(df: pd.DataFrame, window: int = 256) -> pd.DataFrame:\n",
    "    mu = df.rolling(window).mean() # rolling mean\n",
    "    sigma = df.rolling(window).std().replace(0, np.nan) # rolling std dev, replace 0 with NaN to avoid division by zero\n",
    "    z = (df - mu) / (sigma + 1e-12) # z-score normalization\n",
    "    return z.fillna(0.0) # fill NaNs with 0.0 and return z-scored dataframe\n",
    "\n",
    "# build state tensor\n",
    "def build_state_tensor(panel: pd.DataFrame, lookback: int, normalize: bool = False):\n",
    "    # normalization step for each feature\n",
    "    if normalize: # group by feature and apply z-score while preserving MultiIndex\n",
    "        normalized = pd.DataFrame(index=panel.index) # empty dataframe with same index as features dataframes to hold scaled features\n",
    "        for feature in panel.columns.unique(level=1): # iterate over features\n",
    "            feature_data = panel.xs(feature, level=1, axis=1)\n",
    "            z_scored = rolling_zscore(feature_data, window=max(lookback*2, 256)) # take double the lookback value for rolling\n",
    "            for ticker in z_scored.columns: # reconstruct MultiIndex columns\n",
    "                normalized[(ticker, feature)] = z_scored[ticker]\n",
    "        normalized.columns = pd.MultiIndex.from_tuples(normalized.columns, names=[\"ticker\", \"feature\"]) # set MultiIndex columns\n",
    "    else: # no normalization, use raw features\n",
    "        normalized = panel.copy()\n",
    "\n",
    "    # organize data by tickers, features, and time\n",
    "    tickers = sorted(panel.columns.unique(level=0)) # ensure consistent order\n",
    "    features = sorted(panel.columns.unique(level=1)) # feature order\n",
    "    times = normalized.index # time index\n",
    "\n",
    "    # create sliding windows of lookback length\n",
    "    # extract feature data for the window and stack into a 3D tensor (tickers x features x lookback)\n",
    "    X, y_ret, inst_vol = [], [], [] # lists to hold state tensors, next returns, and current volatilities\n",
    "    for i in range(lookback, len(times)-1): # iterate over time index in lookback window to have a sliding window with each time point as a step\n",
    "        window_slice = normalized.iloc[i-lookback:i]\n",
    "        frames = []\n",
    "        for t in tickers: # save the windows of each ticker\n",
    "            frames.append(window_slice[t].T.values)\n",
    "        tensor = np.stack(frames, axis=0) # stack into 3D tensor\n",
    "        X.append(tensor) # append tensor to list\n",
    "        nxt = panel.iloc[i+1] # next period returns\n",
    "        y_ret.append(np.array([nxt[(t, \"ret\")] for t in tickers], dtype=float)) # save to list\n",
    "        cur = panel.iloc[i] # current period volatilities\n",
    "        inst_vol.append(np.array([cur[(t, \"vol\")] for t in tickers], dtype=float)) # save to list\n",
    "\n",
    "    X = np.array(X, dtype=np.float32) # convert list of state tensors of tensors to 3D numpy array\n",
    "    y_ret = np.array(y_ret, dtype=np.float32) # convert list of next returns to 2D numpy array\n",
    "    inst_vol = np.array(inst_vol, dtype=np.float32) # convert list of volatilities to 2D numpy array\n",
    "    return X, y_ret, inst_vol, tickers, features, times[lookback+1:]\n",
    "\n",
    "X_all, R_all, VOL_all, TICKER_ORDER, FEAT_ORDER, TIME_INDEX = build_state_tensor(\n",
    "    panel, lookback=lookback, normalize=normalize\n",
    ")\n",
    "\n",
    "print(\"State tensor:\", X_all.shape, \"Returns:\", R_all.shape, \"InstVol:\", VOL_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745f60af",
   "metadata": {},
   "source": [
    "## Define Splits and Adjust Timezones\n",
    "\n",
    "This section defines the training and validation splits for the dataset. It ensures that the data is divided appropriately to train the model, validate its performance during training, and test its final performance on unseen data. Also, it adjusts the timezones of the datetime indices to ensure consistency across the dataset. This is necessary in case the data comes from multiple sources with different timezone settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a8a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create boolean mask for date slicing\n",
    "def date_slice_mask(times: pd.DatetimeIndex, start: str, end: str):\n",
    "    # convert input dates to UTC timestamps\n",
    "    start_ts = pd.Timestamp(start).tz_localize('UTC')\n",
    "    end_ts = pd.Timestamp(end).tz_localize('UTC')\n",
    "    \n",
    "    # ensure time index is in UTC\n",
    "    if times.tz is None: # if naive, localize to UTC\n",
    "        times = times.tz_localize('UTC')\n",
    "    elif times.tz != pytz.UTC: # if timezone-aware but not UTC, convert to UTC\n",
    "        times = times.tz_convert('UTC')\n",
    "        \n",
    "    return (times >= start_ts) & (times <= end_ts) # return boolean mask for splits later on\n",
    "\n",
    "def build_splits(times: pd.DatetimeIndex, cfg: dict):\n",
    "    splits_cfg = CONFIG[\"SPLITS\"]\n",
    "    \n",
    "    # ensure time index is in UTC\n",
    "    if times.tz is None: # if naive, localize to UTC\n",
    "        times = times.tz_localize('UTC')\n",
    "    elif times.tz != pytz.UTC: # if timezone-aware but not UTC, convert to UTC\n",
    "        times = times.tz_convert('UTC')\n",
    "    \n",
    "    # if not using walk-forward, create a single static split\n",
    "    if not splits_cfg[\"walk_forward\"]:\n",
    "        m_train = date_slice_mask(times, splits_cfg[\"train\"][0], splits_cfg[\"train\"][1]) # create boolean mask for training period\n",
    "        m_val   = date_slice_mask(times, splits_cfg[\"val\"][0], splits_cfg[\"val\"][1]) # create boolean mask for validation period\n",
    "        m_test  = date_slice_mask(times, splits_cfg[\"test\"][0], splits_cfg[\"test\"][1]) # create boolean mask for testing period\n",
    "        return [{\"name\":\"BaseSplit\",\"train\":m_train,\"val\":m_val,\"test\":m_test}]\n",
    "    # if using walk-forward, create multiple overlapping splits\n",
    "    else:\n",
    "        # create UTC timestamps for start and end of entire dataset\n",
    "        start = pd.Timestamp(splits_cfg[\"data_start\"]).tz_localize('UTC')\n",
    "        end   = pd.Timestamp(splits_cfg[\"data_end\"]).tz_localize('UTC')\n",
    "        spans = [] # list to hold all splits\n",
    "        cur_train_start = start # initialize current training start date\n",
    "        while True: # loop to create overlapping splits\n",
    "            train_end = cur_train_start + timedelta(days=splits_cfg[\"wf_train_span_days\"]) # calculate training end date\n",
    "            test_end  = train_end + timedelta(days=splits_cfg[\"wf_test_span_days\"]) # calculate testing end date\n",
    "            if test_end > end: # if testing end date exceeds dataset end, stop\n",
    "                break\n",
    "            m_train = (times >= cur_train_start) & (times <= train_end) # create boolean mask for training period\n",
    "            m_val   = (times > train_end) & (times <= train_end) # create boolean mask for validation period\n",
    "            m_test  = (times > train_end) & (times <= test_end) # create boolean mask for testing period\n",
    "            # append current split as dictionaries to list\n",
    "            spans.append({\n",
    "                \"name\": f\"WF_{cur_train_start.strftime('%Y%m%d')}_{test_end.strftime('%Y%m%d')}\",\n",
    "                \"train\": m_train,\n",
    "                \"val\": m_val,\n",
    "                \"test\": m_test\n",
    "            })\n",
    "            cur_train_start = cur_train_start + timedelta(days=splits_cfg[\"wf_step_days\"]) # move training start date forward by step size\n",
    "        return spans\n",
    "\n",
    "SPLITS = build_splits(TIME_INDEX, CONFIG[\"SPLITS\"])\n",
    "print(f\"Built {len(SPLITS)} split(s). Example:\", SPLITS[0][\"name\"] if SPLITS else \"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be710c8e",
   "metadata": {},
   "source": [
    "## Environment Configuration\n",
    "\n",
    "This is a custom Gymnasium environment for portfolio optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aab18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioWeightsEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": []}\n",
    "\n",
    "    def __init__(self, X, R, VOL, tickers, lookback, cfg_env, sampling=\"1h\"):\n",
    "        super().__init__()\n",
    "        self.X = X # state tensor\n",
    "        self.R = R # next period returns\n",
    "        self.VOL = VOL # current period volatilities\n",
    "        self.tickers = tickers # list of assets\n",
    "        self.lookback = lookback # lookback window\n",
    "        self.cfg = cfg_env # environment configuration taken from CONFIG\n",
    "        self.sampling = sampling # data sampling frequency\n",
    "\n",
    "        self.n_assets = len(tickers) # number of assets\n",
    "        self.include_cash = cfg_env[\"include_cash\"] # whether to include cash as an asset\n",
    "        self.dim_action = self.n_assets + (1 if self.include_cash else 0) # action space dimension: one weight per asset (+1 for cash if included)\n",
    "\n",
    "        obs_dim = self.n_assets * self.X.shape[2] * self.lookback # observation space dimension: tickers x features x lookback\n",
    "        self.observation_space = spaces.Box(low=-10, high=10, shape=(obs_dim,), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(low=0.0, high=1.0, shape=(self.dim_action,), dtype=np.float32) # action space: weights between 0 and 1 (no shorting possible here)\n",
    "\n",
    "        self.commission = cfg_env[\"transaction_costs\"][\"commission_bps\"] / 1e4 # commission in decimal form\n",
    "        self.slippage = cfg_env[\"transaction_costs\"][\"slippage_bps\"] / 1e4 # slippage in decimal form\n",
    "        self.risk_lambda = cfg_env[\"reward\"][\"risk_lambda\"] # defines risk aversion in reward function\n",
    "\n",
    "        # weight constraints\n",
    "        self.min_w = cfg_env[\"constraints\"][\"min_weight\"] # minimum weight per asset\n",
    "        self.max_w = cfg_env[\"constraints\"][\"max_weight\"] # maximum weight per asset\n",
    "        self.sum_to_one = cfg_env[\"constraints\"][\"sum_to_one\"] # whether weights should sum to one (irrelevant if shorting is allowed)\n",
    "\n",
    "        self.reset(seed=cfg_env.get(\"seed\", 42)) # ??????\n",
    "\n",
    "    def _to_obs(self, t): # ??????\n",
    "        arr = self.X[t].reshape(-1).astype(np.float32)\n",
    "        return arr\n",
    "\n",
    "    # project raw actions to valid weights\n",
    "    def _project_weights(self, a):\n",
    "        if self.sum_to_one: # if weights must sum to one, use softmax\n",
    "            expo = np.exp(a - np.max(a))\n",
    "            w = expo / np.sum(expo)\n",
    "        else: # otherwise, just clip to min and max weights\n",
    "            w = np.clip(a, self.min_w, self.max_w)\n",
    "        if not self.cfg[\"shorting\"]: # if shorting is not allowed, clip weights to [0, 1]\n",
    "            w = np.clip(w, 0.0, 1.0)\n",
    "        return w # return projected weights\n",
    "\n",
    "    # reset environment to initial state\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.t = 0 # reset time index\n",
    "        self.portfolio_value = 1.0 # reset portfolio value\n",
    "        self.w = np.ones(self.dim_action) / self.dim_action # initialize equal weights\n",
    "        obs = self._to_obs(self.t) # ???????\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        w_target = self._project_weights(action) # get new target weights from raw actions\n",
    "        turnover = np.sum(np.abs(w_target - self.w)) # calculate turnover as sum of absolute weight changes\n",
    "        trading_cost = (self.commission + self.slippage) * turnover # total trading cost based on turnover\n",
    "\n",
    "        asset_w_prev = self.w[:self.n_assets]\n",
    "        asset_ret = np.dot(asset_w_prev, self.R[self.t])\n",
    "        inst_vol = np.dot(asset_w_prev, self.VOL[self.t])\n",
    "\n",
    "        reward = asset_ret - trading_cost - self.risk_lambda * inst_vol\n",
    "\n",
    "        self.portfolio_value *= math.exp(asset_ret - trading_cost)\n",
    "\n",
    "        self.w = w_target\n",
    "        self.t += 1\n",
    "        terminated = (self.t >= len(self.R)-1)\n",
    "        truncated = False\n",
    "\n",
    "        obs = self._to_obs(self.t) if not terminated else self._to_obs(self.t-1)\n",
    "        info = {\"portfolio_value\": self.portfolio_value, \"turnover\": turnover, \"inst_vol\": inst_vol, \"asset_ret\": asset_ret}\n",
    "        return obs, reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7281e10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def slice_by_mask(X, R, VOL, mask: np.ndarray):\n",
    "    idx = np.where(mask)[0]\n",
    "    return X[idx], R[idx], VOL[idx]\n",
    "\n",
    "def make_env_from_mask(mask, name=\"env\"):\n",
    "    X_s, R_s, V_s = slice_by_mask(X_all, R_all, VOL_all, mask)\n",
    "    env = PortfolioWeightsEnv(X_s, R_s, V_s, TICKER_ORDER, CONFIG[\"ENV\"][\"lookback_window\"], CONFIG[\"ENV\"], sampling=CONFIG[\"DATA\"][\"sampling\"])\n",
    "    env = Monitor(env, filename=None)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadc07b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annualize_factor(sampling: str):\n",
    "    return ANNUALIZATION.get(sampling, 365*24)\n",
    "\n",
    "def compute_metrics(equity_curve: pd.Series, sampling: str, turnover_series: pd.Series = None):\n",
    "    ret = equity_curve.pct_change().dropna()\n",
    "    ann = annualize_factor(sampling)\n",
    "    mu = ret.mean() * ann\n",
    "    sigma = ret.std() * math.sqrt(ann)\n",
    "    sharpe = mu / (sigma + 1e-12)\n",
    "    downside = ret[ret < 0].std() * math.sqrt(ann)\n",
    "    sortino = mu / (downside + 1e-12)\n",
    "    if len(equity_curve) > 1:\n",
    "        # Calculate years based on number of samples and sampling frequency\n",
    "        if isinstance(equity_curve.index, pd.DatetimeIndex):\n",
    "            dt_years = (equity_curve.index[-1] - equity_curve.index[0]).total_seconds() / (365 * 24 * 3600)\n",
    "        else:\n",
    "            # If using RangeIndex, calculate based on sampling frequency\n",
    "            samples = len(equity_curve)\n",
    "            samples_per_year = annualize_factor(sampling)\n",
    "            dt_years = samples / samples_per_year\n",
    "        dt_years = float(dt_years) if float(dt_years) != 0 else 1e-12\n",
    "        cagr = (equity_curve.iloc[-1] / equity_curve.iloc[0]) ** (1/dt_years) - 1\n",
    "    else:\n",
    "        cagr = 0.0\n",
    "    cummax = equity_curve.cummax()\n",
    "    dd = (equity_curve / cummax - 1).min()\n",
    "    maxdd = float(dd)\n",
    "    calmar = mu / (abs(maxdd) + 1e-12)\n",
    "    hit_ratio = (ret > 0).mean()\n",
    "    turnover = turnover_series.mean() if turnover_series is not None and len(turnover_series)>0 else np.nan\n",
    "    return {\"CAGR\": cagr, \"Sharpe\": sharpe, \"Sortino\": sortino, \"MaxDrawdown\": maxdd, \"Calmar\": calmar, \"Volatility\": sigma, \"Turnover\": turnover, \"HitRatio\": hit_ratio}\n",
    "\n",
    "def plot_series(series: pd.Series, title: str):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(series.index, series.values)\n",
    "    plt.title(title); plt.xlabel(\"Time\"); plt.ylabel(\"Value\"); plt.show()\n",
    "\n",
    "def backtest_env(env: gym.Env, model=None):\n",
    "    # Get the unwrapped environment\n",
    "    unwrapped = env.unwrapped if hasattr(env, 'unwrapped') else env\n",
    "    \n",
    "    obs, _ = env.reset()\n",
    "    pv, turns = [], []\n",
    "    for t in range(len(unwrapped.R)-1):\n",
    "        if model is None:\n",
    "            action = np.ones(unwrapped.dim_action)/unwrapped.dim_action\n",
    "        else:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, trunc, info = env.step(action)\n",
    "        pv.append(info[\"portfolio_value\"])\n",
    "        turns.append(info[\"turnover\"])\n",
    "        if done:\n",
    "            break\n",
    "    idx = pd.RangeIndex(start=0, stop=len(pv), step=1)\n",
    "    ec = pd.Series(pv, index=idx)\n",
    "    to = pd.Series(turns, index=idx)\n",
    "    return ec, to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc41478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_dir(CONFIG[\"IO\"][\"models_dir\"])\n",
    "ensure_dir(CONFIG[\"EVAL\"][\"reports_dir\"])\n",
    "\n",
    "RESULTS = []\n",
    "\n",
    "for split in SPLITS:\n",
    "    print(f\"\\n=== Training on split: {split['name']} ===\")\n",
    "    train_env = make_env_from_mask(split[\"train\"], name=f\"{split['name']}_train\")\n",
    "    eval_env  = make_env_from_mask(split[\"test\"], name=f\"{split['name']}_test\")\n",
    "\n",
    "    vec_train = DummyVecEnv([lambda: train_env])\n",
    "    vec_eval  = DummyVecEnv([lambda: eval_env])\n",
    "\n",
    "    model = PPO(\n",
    "        policy=CONFIG[\"RL\"][\"policy\"],\n",
    "        env=vec_train,\n",
    "        gamma=CONFIG[\"RL\"][\"gamma\"],\n",
    "        gae_lambda=CONFIG[\"RL\"][\"gae_lambda\"],\n",
    "        clip_range=CONFIG[\"RL\"][\"clip_range\"],\n",
    "        n_steps=CONFIG[\"RL\"][\"n_steps\"],\n",
    "        batch_size=CONFIG[\"RL\"][\"batch_size\"],\n",
    "        learning_rate=CONFIG[\"RL\"][\"learning_rate\"],\n",
    "        ent_coef=CONFIG[\"RL\"][\"ent_coef\"],\n",
    "        vf_coef=CONFIG[\"RL\"][\"vf_coef\"],\n",
    "        max_grad_norm=CONFIG[\"RL\"][\"max_grad_norm\"],\n",
    "        tensorboard_log=CONFIG[\"IO\"][\"tb_logdir\"],\n",
    "        device=device,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    eval_callback = EvalCallback(\n",
    "        vec_eval, \n",
    "        best_model_save_path=CONFIG[\"IO\"][\"models_dir\"],\n",
    "        log_path=CONFIG[\"IO\"][\"models_dir\"], \n",
    "        eval_freq=1000,  # Reduced from 10000 to 1000 for more frequent feedback\n",
    "        deterministic=True, \n",
    "        render=False\n",
    "    )\n",
    "    \n",
    "    model.learn(total_timesteps=CONFIG[\"RL\"][\"timesteps\"], callback=eval_callback)\n",
    "    model_path = os.path.join(CONFIG[\"IO\"][\"models_dir\"], f\"ppo_{split['name']}.zip\")\n",
    "    model.save(model_path)\n",
    "    print(\"Saved model:\", model_path)\n",
    "\n",
    "    test_env = make_env_from_mask(split[\"test\"], name=f\"{split['name']}_test\")\n",
    "    ec, to = backtest_env(test_env, model=model)\n",
    "\n",
    "    idx = np.where(split[\"test\"])[0]\n",
    "    R_test = R_all[idx]\n",
    "    ew = np.ones(len(TICKER_ORDER))/len(TICKER_ORDER)\n",
    "    ec_bench = [1.0]\n",
    "    # Only iterate through the same number of returns as we have in ec.index\n",
    "    for i in range(len(ec.index)-1):\n",
    "        ec_bench.append(ec_bench[-1]*math.exp(np.dot(ew, R_test[i])))\n",
    "    ec_bench = pd.Series(ec_bench, index=ec.index)\n",
    "\n",
    "    bh_btc, bh_eth = [1.0], [1.0]\n",
    "    # Only iterate through the same number of returns as we have in ec.index\n",
    "    for i in range(len(ec.index)-1):\n",
    "        bh_btc.append(bh_btc[-1]*math.exp(R_test[i][0]))\n",
    "        bh_eth.append(bh_eth[-1]*math.exp(R_test[i][1]))\n",
    "    bh_btc = pd.Series(bh_btc, index=ec.index)\n",
    "    bh_eth = pd.Series(bh_eth, index=ec.index)\n",
    "\n",
    "    m_model = compute_metrics(ec, CONFIG[\"DATA\"][\"sampling\"], to)\n",
    "    m_ew    = compute_metrics(ec_bench, CONFIG[\"DATA\"][\"sampling\"])\n",
    "    m_btc   = compute_metrics(bh_btc, CONFIG[\"DATA\"][\"sampling\"])\n",
    "    m_eth   = compute_metrics(bh_eth, CONFIG[\"DATA\"][\"sampling\"])\n",
    "\n",
    "    RESULTS.append({\"split\": split[\"name\"], \"model\": m_model, \"equal_weight\": m_ew, \"buy_and_hold_BTC\": m_btc, \"buy_and_hold_ETH\": m_eth})\n",
    "\n",
    "    if CONFIG[\"EVAL\"][\"plots\"]:\n",
    "        plot_series(ec, f\"Equity Curve — PPO ({split['name']})\")\n",
    "        plot_series((ec / ec.cummax()) - 1.0, f\"Drawdown — PPO ({split['name']})\")\n",
    "        plot_series(ec_bench, f\"Equity Curve — Equal-Weight Hold ({split['name']})\")\n",
    "\n",
    "print(\"Done. RESULTS collected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eec7971",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rows = []\n",
    "for res in RESULTS:\n",
    "    row = {\"split\": res[\"split\"]}\n",
    "    for k, metrics in res.items():\n",
    "        if k == \"split\":\n",
    "            continue\n",
    "        for mname, mval in metrics.items():\n",
    "            row[f\"{k}_{mname}\"] = mval\n",
    "    rows.append(row)\n",
    "\n",
    "df_results = pd.DataFrame(rows)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f15943",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "ensure_dir(CONFIG[\"EVAL\"][\"reports_dir\"])\n",
    "out_json = os.path.join(CONFIG[\"EVAL\"][\"reports_dir\"], f\"metrics_{ts}.json\")\n",
    "out_csv  = os.path.join(CONFIG[\"EVAL\"][\"reports_dir\"], f\"metrics_{ts}.csv\")\n",
    "df_results.to_csv(out_csv, index=False)\n",
    "with open(out_json, \"w\") as f:\n",
    "    json.dump(RESULTS, f, indent=2)\n",
    "print(\"Saved:\", out_json, \"and\", out_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
