{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1bb7b54",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Agent for Statistical Arbitrage\n",
    "\n",
    "This notebook demonstrates how to configure and train a reinforcement learning agent for statistical arbitrage using a snapshot model. The agent will learn to identify and exploit statistical arbitrage opportunities in financial markets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e79612d",
   "metadata": {},
   "source": [
    "## Install Packages\n",
    "\n",
    "The following libraries are required for this notebook. If you haven't installed them yet, you can do so using running the cell below or by using pip install in your terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b434b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and install packages once\n",
    "# %pip -q install -U numpy pandas pyarrow gdown gymnasium stable-baselines3 torch matplotlib tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568820c4",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4143bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from gdown import download\n",
    "import math\n",
    "import random\n",
    "import pytz\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from typing import Tuple, Set, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from IPython.display import display\n",
    "\n",
    "# Reinforcement Learning\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback, BaseCallback, CallbackList\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "\n",
    "from config import CONFIG\n",
    "\n",
    "print(\"Imports complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c6388",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69053e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set annualization factors for different timeframes\n",
    "ANNUALIZATION = {\"1m\":365*24*60,\n",
    "                 \"5m\":365*24*12,\n",
    "                 \"15m\":365*24*4,\n",
    "                 \"1h\":365*24,\n",
    "                 \"1d\":365}\n",
    "\n",
    "# create directory if it doesn't already exist\n",
    "def ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f12ee23",
   "metadata": {},
   "source": [
    "## Set Computation Device\n",
    "\n",
    "This section sets the computation device for training the model. It checks if a GPU is available and sets it as the device; otherwise, it defaults to CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bc4ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on cuda GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"CUDA is available. Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# run on Apple Silicon GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS is available. Using Apple Silicon GPU.\")\n",
    "\n",
    "# run on CPU (slow)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA and MPS are not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63981a61",
   "metadata": {},
   "source": [
    "## Set Seeds\n",
    "\n",
    "This section sets the random seeds for various libraries to ensure that the results are reproducible.\n",
    "\n",
    "Note: It is good practice to type (set data types) for function and method parameters for better code maintainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775695a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed: int = 42):\n",
    "    random.seed(seed) # seed for random module\n",
    "    np.random.seed(seed) # seed for numpy module\n",
    "    try:\n",
    "        torch.manual_seed(seed) # seed for torch module\n",
    "        if torch.cuda.is_available(): # seed for CUDA device\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        elif torch.backends.mps.is_available(): # seed for Apple Silicon device\n",
    "            torch.backends.mps.manual_seed_all(seed)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "set_all_seeds(CONFIG[\"ENV\"][\"seed\"]) # set all seeds for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e449a3db",
   "metadata": {},
   "source": [
    "## Download Feature Data\n",
    "\n",
    "In this section, we retrieve the .csv file created during feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee5ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(file_name: str, file_id: str, out_dir: str):\n",
    "    ensure_dir(out_dir)\n",
    "\n",
    "    out_path = os.path.join(out_dir, f\"{file_name}.csv\")  # full output path\n",
    "\n",
    "    if os.path.exists(out_path):\n",
    "        print(f\"Skipping download. File {file_name} already exists in cache.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        print(f\"Downloading {file_name} -> {out_path}\")\n",
    "        url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "        success = download(url, out_path, quiet=False, use_cookies=False, verify=True)\n",
    "        print(\"Download complete.\")\n",
    "        return success\n",
    "    except Exception as e:\n",
    "        print(f\"Download attempt failed for {file_name}. Error: {str(e)}\")\n",
    "    \n",
    "# donwload features\n",
    "file_name = CONFIG[\"DATA\"][\"features\"][\"file_name\"]\n",
    "file_id = CONFIG[\"DATA\"][\"features\"][\"file_id\"]\n",
    "cache_dir = CONFIG[\"DATA\"][\"cache_dir\"]\n",
    "\n",
    "success = download_file(file_name, file_id, cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b151dd54",
   "metadata": {},
   "source": [
    "## Load Feature Data\n",
    "\n",
    "Once the data is downloaded, this section loads the data into a pandas DataFrame for later processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cc3f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_to_df(\n",
    "    path: str,\n",
    "    sep: str = \",\",\n",
    "    timestamp_index_col: str | None = \"datetime\",\n",
    "    encoding: str = \"utf-8-sig\",\n",
    "    **read_csv_kwargs,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a CSV into a pandas DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Filesystem path to the CSV.\n",
    "    parse_timestamp_col : str | None\n",
    "        If provided and present in the CSV, this column will be parsed to datetime.\n",
    "        Set to None to skip datetime parsing.\n",
    "    **read_csv_kwargs :\n",
    "        Extra arguments passed to `pd.read_csv` (e.g., sep, dtype, usecols).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse header-only to check for timestamp col presence\n",
    "    head = pd.read_csv(path, sep=sep, encoding=encoding, nrows=0)\n",
    "    if timestamp_index_col and timestamp_index_col in head.columns:\n",
    "        read_csv_kwargs = {\n",
    "            **read_csv_kwargs,\n",
    "            \"parse_dates\": [timestamp_index_col],\n",
    "        }\n",
    "\n",
    "    df = pd.read_csv(path, sep=sep, encoding=encoding, engine=\"pyarrow\", **read_csv_kwargs)\n",
    "\n",
    "    df = df.set_index(\"datetime\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# load features\n",
    "file_name = CONFIG[\"DATA\"][\"features\"][\"file_name\"]\n",
    "cache_dir = CONFIG[\"DATA\"][\"cache_dir\"]\n",
    "index = CONFIG[\"DATA\"][\"features\"][\"index\"]\n",
    "sep = CONFIG[\"DATA\"][\"features\"].get(\"seperator\", \",\")\n",
    "file_path = os.path.join(cache_dir, f\"{file_name}.csv\")\n",
    "features_df = load_csv_to_df(file_path, sep, timestamp_index_col=index)\n",
    "\n",
    "# print dataframe info\n",
    "print(\"Features DataFrame Info:\")\n",
    "print(features_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916f09ef",
   "metadata": {},
   "source": [
    "## Identify Feature Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30901f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_assets_features_pairs(\n",
    "    df: pd.DataFrame,\n",
    "    single_asset_format: str,\n",
    "    pair_feature_format: str,\n",
    ") -> tuple[list[str], list[str], list[str], list[tuple[str, str]]]:\n",
    "    \"\"\"\n",
    "    Returns distinct\n",
    "      1. assets\n",
    "      2. single-asset feature names (ARB_closeUpperShadow → closeUpperShadow)\n",
    "      3. pair feature names (ARB_ETH_spreadNorm → spreadNorm)\n",
    "      4. unordered asset pairs\n",
    "    \"\"\"\n",
    "\n",
    "    def format_to_regex(fmt: str) -> re.Pattern:\n",
    "        escaped = re.escape(fmt)\n",
    "\n",
    "        def repl(match: re.Match[str]) -> str:\n",
    "            name = match.group(1)\n",
    "            char_class = r\"[A-Za-z0-9_]+\" if \"FEATURE\" in name.upper() else r\"[A-Za-z0-9]+\"\n",
    "            return f\"(?P<{name}>{char_class})\"\n",
    "\n",
    "        escaped = re.sub(r\"\\\\\\{(\\w+)\\\\\\}\", repl, escaped)\n",
    "        return re.compile(f\"^{escaped}$\")\n",
    "\n",
    "    single_asset_pattern = format_to_regex(single_asset_format)\n",
    "    pair_feature_pattern = format_to_regex(pair_feature_format)\n",
    "    generic_single_pattern = re.compile(r\"^(?P<ASSET>[A-Za-z0-9]+)_(?P<FEATURE>[A-Za-z0-9_]+)$\")\n",
    "\n",
    "    assets: Set[str] = set()\n",
    "    single_features: Set[str] = set()\n",
    "    pair_features: Set[str] = set()\n",
    "    pairs: Set[Tuple[str, str]] = set()\n",
    "\n",
    "    literal_feature = None\n",
    "    if \"{FEATURE}\" not in single_asset_format:\n",
    "        literal_feature = single_asset_format.replace(\"{ASSET}\", \"\").lstrip(\"_\")\n",
    "\n",
    "    skip_cols = {\"timestamp\", \"datetime\", \"date\"}\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col in skip_cols:\n",
    "            continue\n",
    "\n",
    "        match_pair = pair_feature_pattern.match(col)\n",
    "        if match_pair:\n",
    "            a1, a2, feat = match_pair.group(\"ASSET1\"), match_pair.group(\"ASSET2\"), match_pair.group(\"FEATURE\")\n",
    "            assets.update((a1, a2))\n",
    "            pairs.add(tuple(sorted((a1, a2))))\n",
    "            pair_features.add(feat)\n",
    "            continue\n",
    "\n",
    "        match_single = single_asset_pattern.match(col)\n",
    "        if match_single:\n",
    "            asset = match_single.group(\"ASSET\")\n",
    "            assets.add(asset)\n",
    "            feat = match_single.groupdict().get(\"FEATURE\") or literal_feature\n",
    "            if feat:\n",
    "                single_features.add(feat)\n",
    "            continue\n",
    "\n",
    "        match_generic = generic_single_pattern.match(col)\n",
    "        if match_generic:\n",
    "            asset, feat = match_generic.group(\"ASSET\"), match_generic.group(\"FEATURE\")\n",
    "            assets.add(asset)\n",
    "            single_features.add(feat)\n",
    "            continue\n",
    "\n",
    "    return (\n",
    "        sorted(assets),\n",
    "        sorted(single_features),\n",
    "        sorted(pair_features),\n",
    "        sorted(pairs),\n",
    "    )\n",
    "\n",
    "# identify assets, features, and asset pairs\n",
    "single_asset_format = CONFIG[\"DATA\"][\"asset_price_format\"]\n",
    "pair_feature_format = CONFIG[\"DATA\"][\"pair_feature_format\"]\n",
    "assets, single_asset_features, pair_features, asset_pairs = identify_assets_features_pairs(\n",
    "    features_df,\n",
    "    CONFIG[\"DATA\"][\"asset_price_format\"],\n",
    "    CONFIG[\"DATA\"][\"pair_feature_format\"],\n",
    ")\n",
    "\n",
    "print(f\"Identified {len(assets)} assets: {assets}\")\n",
    "print(f\"Identified {len(single_asset_features)} single-asset features: {single_asset_features}\")\n",
    "print(f\"Identified {len(pair_features)} pair features: {pair_features}\")\n",
    "print(f\"Identified {len(asset_pairs)} asset pairs: {asset_pairs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bd4a34",
   "metadata": {},
   "source": [
    "## Build Time Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f391580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_time_intervals(\n",
    "    df: pd.DataFrame,\n",
    "    window: pd.Timedelta | str,\n",
    "    step: Optional[pd.Timedelta | str] = None,\n",
    "    timestamp_col: str = \"datetime\",\n",
    "    include_last_partial: bool = False,\n",
    ") -> list[tuple[pd.Timestamp, pd.Timestamp]]:\n",
    "    \"\"\"\n",
    "    Return fixed-length time intervals over the DataFrame's time span.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Must contain a datetime-like 'timestamp' column or have a DatetimeIndex.\n",
    "    window : pd.Timedelta | str\n",
    "        Size of each window, e.g. '2D', '60min', '15T'.\n",
    "    step : pd.Timedelta | str | None\n",
    "        Step between consecutive window starts. Defaults to `window` (non-overlapping).\n",
    "        Use a smaller step than `window` for sliding/overlapping windows.\n",
    "    timestamp_col : str\n",
    "        Name of the timestamp column (ignored if index is a DatetimeIndex).\n",
    "    include_last_partial : bool\n",
    "        If True, include the trailing partial window shorter than `window`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[tuple[pd.Timestamp, pd.Timestamp]]\n",
    "        Half-open intervals [start, end).\n",
    "    \"\"\"\n",
    "    W = pd.Timedelta(window)\n",
    "    S = pd.Timedelta(step) if step is not None else W\n",
    "\n",
    "    # Extract, sanitize, and sort timestamps\n",
    "    if timestamp_col in df.columns:\n",
    "        ts = pd.to_datetime(df[timestamp_col]).dropna().sort_values()\n",
    "    elif isinstance(df.index, pd.DatetimeIndex):\n",
    "        ts = pd.Series(df.index).dropna().sort_values()\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Timestamp column '{timestamp_col}' not found and index is not DatetimeIndex.\"\n",
    "        )\n",
    "\n",
    "    intervals: list[tuple[pd.Timestamp, pd.Timestamp]] = []\n",
    "    if ts.empty:\n",
    "        return intervals\n",
    "\n",
    "    t_min = ts.iloc[0]\n",
    "    t_max = ts.iloc[-1]\n",
    "    cur = t_min\n",
    "\n",
    "    while cur < t_max:\n",
    "        end = cur + W\n",
    "        if end <= t_max:\n",
    "            intervals.append((cur, end))\n",
    "        elif include_last_partial:\n",
    "            intervals.append((cur, t_max))\n",
    "            break\n",
    "        else:\n",
    "            break\n",
    "        cur = cur + S\n",
    "\n",
    "    return intervals\n",
    "\n",
    "# build intervals\n",
    "window = CONFIG[\"ENV\"][\"trading_window_days\"]\n",
    "step = CONFIG[\"ENV\"][\"sliding_window_step\"]\n",
    "timestamp_col = CONFIG[\"DATA\"][\"timestamp_col\"]\n",
    "\n",
    "intervals = build_time_intervals(\n",
    "    features_df,\n",
    "    window,\n",
    "    step,\n",
    "    timestamp_col,\n",
    "    include_last_partial=False\n",
    ")\n",
    "\n",
    "# print interval info\n",
    "print(f\"Built {len(intervals)} time intervals with window={window} and step={step}.\")\n",
    "print(\"First 3 intervals:\")\n",
    "for start, end in intervals[:3]:\n",
    "    print(f\"  {start} to {end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9febee86",
   "metadata": {},
   "source": [
    "## Identify Feature Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3148ff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_timeframe_valid(\n",
    "    df: pd.DataFrame,\n",
    "    pair: tuple[str, str],\n",
    "    start: pd.Timestamp,\n",
    "    end: pd.Timestamp,\n",
    "    feature_name: str,\n",
    "    pair_feature_format: str = \"{ASSET1}_{ASSET2}_{FEATURE}\",\n",
    "    timestamp_col: str | None = \"datetime\",\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Check if the given time frame has complete data for the specified asset pair.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing the data.\n",
    "    pair : tuple[str, str]\n",
    "        Asset pair (asset1, asset2).\n",
    "    start : pd.Timestamp\n",
    "        Start of the time frame (inclusive).\n",
    "    end : pd.Timestamp\n",
    "        End of the time frame (exclusive).\n",
    "    pair_feature_format : str\n",
    "        Format string for pair-feature columns (e.g., \"{ASSET1}_{ASSET2}_{FEATURE}\").\n",
    "    timestamp_col : str\n",
    "        Name of the timestamp column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the time frame is valid (no missing data), False otherwise.\n",
    "    \"\"\"\n",
    "    feature_col = pair_feature_format.format(\n",
    "        ASSET1=pair[0], ASSET2=pair[1], FEATURE=feature_name\n",
    "    )\n",
    "\n",
    "    if timestamp_col and timestamp_col in df.columns:\n",
    "        ts = df[timestamp_col]\n",
    "    else:\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            raise ValueError(\n",
    "                \"DataFrame neither has the timestamp column nor a DatetimeIndex.\"\n",
    "            )\n",
    "        ts = df.index\n",
    "\n",
    "    mask = (ts >= start) & (ts < end)\n",
    "    if mask.sum() == 0:\n",
    "        return False\n",
    "\n",
    "    data_slice = df.loc[mask, feature_col]\n",
    "    return not data_slice.isna().any()\n",
    "\n",
    "\n",
    "pair_identifier = CONFIG[\"DATA\"][\"features\"][\"pair_identifier\"]\n",
    "\n",
    "valid_intervals_per_pair = {}\n",
    "for pair in asset_pairs:\n",
    "    valid_intervals = []\n",
    "    for start, end in intervals:\n",
    "        if is_timeframe_valid(\n",
    "            features_df,\n",
    "            pair,\n",
    "            start,\n",
    "            end,\n",
    "            feature_name=pair_identifier,\n",
    "            pair_feature_format=pair_feature_format,\n",
    "            timestamp_col=timestamp_col):\n",
    "\n",
    "            valid_intervals.append((start, end))\n",
    "    valid_intervals_per_pair[pair] = valid_intervals\n",
    "\n",
    "    print(f\"Pair {pair} has {len(valid_intervals)} valid intervals out of {len(intervals)} total intervals.\")\n",
    "print(f\"Total valid intervals: {sum(len(v) for v in valid_intervals_per_pair.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf36443",
   "metadata": {},
   "source": [
    "## Feature Panel Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b326bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_panel(\n",
    "    df: pd.DataFrame,\n",
    "    assets: list[str],\n",
    "    single_asset_features: list[str],\n",
    "    pair_features: list[str],\n",
    "    asset_pairs: list[tuple[str, str]],\n",
    "    pair_feature_format: str,\n",
    "    timestamp_col: str | None = \"datetime\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Assemble a MultiIndex feature panel from pre-normalized inputs.\"\"\"\n",
    "    if timestamp_col and timestamp_col in df.columns:\n",
    "        working_df = df.set_index(timestamp_col)\n",
    "    else:\n",
    "        working_df = df\n",
    "        if not isinstance(working_df.index, pd.DatetimeIndex):\n",
    "            raise ValueError(\n",
    "                \"DataFrame needs either `timestamp_col` or a DatetimeIndex in order to build the panel.\"\n",
    "            )\n",
    "\n",
    "    # Ensure index is timezone-aware UTC from the start\n",
    "    if working_df.index.tz is None:\n",
    "        working_df.index = working_df.index.tz_localize('UTC')\n",
    "    elif working_df.index.tz != pytz.UTC:\n",
    "        working_df.index = working_df.index.tz_convert('UTC')\n",
    "\n",
    "    panel_columns: list[tuple[str, str]] = []\n",
    "    panel_series: list[pd.Series] = []\n",
    "\n",
    "    for asset in assets:\n",
    "        for feature in single_asset_features:\n",
    "            col_name = f\"{asset}_{feature}\"\n",
    "            if col_name in working_df.columns:\n",
    "                panel_series.append(working_df[col_name])\n",
    "                panel_columns.append((asset, feature))\n",
    "\n",
    "    for asset1, asset2 in asset_pairs:\n",
    "        for feature in pair_features:\n",
    "            col_name = pair_feature_format.format(\n",
    "                ASSET1=asset1, ASSET2=asset2, FEATURE=feature\n",
    "            )\n",
    "            if col_name in working_df.columns:\n",
    "                panel_series.append(working_df[col_name])\n",
    "                panel_columns.append((f\"{asset1}_{asset2}\", feature))\n",
    "\n",
    "    if not panel_series:\n",
    "        raise ValueError(\"No matching feature columns found to construct the panel.\")\n",
    "\n",
    "    panel = pd.concat(panel_series, axis=1)\n",
    "    panel.columns = pd.MultiIndex.from_tuples(panel_columns, names=[\"asset\", \"feature\"])\n",
    "    return panel.fillna(0)\n",
    "\n",
    "\n",
    "# Create feature panel\n",
    "panel = create_feature_panel(\n",
    "    features_df,\n",
    "    assets,\n",
    "    single_asset_features,\n",
    "    pair_features,\n",
    "    asset_pairs,\n",
    "    CONFIG[\"DATA\"][\"pair_feature_format\"],\n",
    ")\n",
    "\n",
    "print(\"Feature panel structure:\")\n",
    "print(\"\\nShape:\", panel.shape)\n",
    "print(\"\\nSample of the data:\")\n",
    "print(panel.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da1a418",
   "metadata": {},
   "source": [
    "## Feature scaling and state tensor construction\n",
    "\n",
    "This section normalizes the features and constructs the state tensors required for training the reinforcement learning agent. State tensors are multi-dimensional arrays that represent the current state of the environment that the RL agent uses to make decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f681542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_state_tensor_for_interval(\n",
    "        panel: pd.DataFrame, \n",
    "        pair: tuple, \n",
    "        start: pd.Timestamp,\n",
    "        end: pd.Timestamp, \n",
    "        lookback: int\n",
    ") -> tuple[Optional[np.ndarray], Optional[np.ndarray], Optional[np.ndarray], Optional[list]]:\n",
    "    \"\"\"\n",
    "    Build state tensor for a specific time interval and asset pair.\n",
    "    Returns (X, R, VOL, timestamps) or (None, None, None, None) if insufficient data.\n",
    "    \n",
    "    OPTIMIZED: Avoids copying entire panel, uses vectorized operations where possible.\n",
    "    \"\"\"\n",
    "    # Get relevant columns for this pair\n",
    "    asset1, asset2 = pair\n",
    "    pair_str = f\"{asset1}_{asset2}\"\n",
    "    \n",
    "    # Get all features for this pair AND individual asset features\n",
    "    # 1. Pair-level features (e.g., BTC_ETH_spreadNorm)\n",
    "    pair_columns = [col for col in panel.columns if col[0] == pair_str]\n",
    "    \n",
    "    # 2. Individual asset features (e.g., BTC_close, ETH_close)\n",
    "    # These are stored with asset name as first level of MultiIndex\n",
    "    asset1_columns = [col for col in panel.columns if col[0] == asset1]\n",
    "    asset2_columns = [col for col in panel.columns if col[0] == asset2]\n",
    "    \n",
    "    if not pair_columns:\n",
    "        return None, None, None, None\n",
    "    \n",
    "    # Combine all relevant columns\n",
    "    all_columns = pair_columns + asset1_columns + asset2_columns\n",
    "    \n",
    "    # Extract feature names\n",
    "    pair_features = sorted(list(set([col[1] for col in pair_columns])))\n",
    "    asset1_features = sorted(list(set([col[1] for col in asset1_columns])))\n",
    "    asset2_features = sorted(list(set([col[1] for col in asset2_columns])))\n",
    "    \n",
    "    # Check if spreadNorm feature exists\n",
    "    if 'spreadNorm' not in pair_features:\n",
    "        print(f\"Warning: 'spreadNorm' feature not found for pair {pair}\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    # Check if we have close prices for both assets (needed for actual returns)\n",
    "    if 'close' not in asset1_features or 'close' not in asset2_features:\n",
    "        print(f\"Warning: Price data not found for pair {pair}. Need 'close' for both assets\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    # Total features: pair features + asset1 features + asset2 features\n",
    "    all_feature_names = pair_features + [f\"{asset1}_{f}\" for f in asset1_features] + [f\"{asset2}_{f}\" for f in asset2_features]\n",
    "    n_total_features = len(all_feature_names)\n",
    "    \n",
    "    # Ensure start and end are timezone-aware UTC (do this once)\n",
    "    start = pd.Timestamp(start, tz='UTC')\n",
    "    end = pd.Timestamp(end, tz='UTC')\n",
    "    \n",
    "    # Create mask for time window - work directly on panel without copy\n",
    "    mask = (panel.index >= start) & (panel.index <= end)\n",
    "    \n",
    "    # Extract only the relevant subset (all columns: pair + asset1 + asset2)\n",
    "    window_data = panel.loc[mask, all_columns]\n",
    "    window_timestamps = panel.index[mask]\n",
    "\n",
    "    if len(window_data) < lookback + 1:\n",
    "        return None, None, None, None\n",
    "    \n",
    "    # Convert to numpy array once for faster access\n",
    "    window_values = window_data.values\n",
    "    \n",
    "    # Prepare dimensions\n",
    "    n_samples = len(window_data) - lookback\n",
    "    \n",
    "    # Initialize tensors with correct feature dimension\n",
    "    X = np.zeros((n_samples, 1, n_total_features, lookback), dtype=np.float32)\n",
    "    R = np.zeros((n_samples, 2), dtype=np.float32)  # [asset1_return, asset2_return]\n",
    "    VOL = np.zeros((n_samples, 1), dtype=np.float32)\n",
    "    \n",
    "    # Create mapping from columns to indices\n",
    "    col_to_idx = {col: idx for idx, col in enumerate(all_columns)}\n",
    "    \n",
    "    # Build feature index mapping for filling X tensor\n",
    "    feature_col_indices = []\n",
    "    \n",
    "    # Add pair features\n",
    "    for feat in pair_features:\n",
    "        col = (pair_str, feat)\n",
    "        feature_col_indices.append(col_to_idx.get(col, -1))\n",
    "    \n",
    "    # Add asset1 features\n",
    "    for feat in asset1_features:\n",
    "        col = (asset1, feat)\n",
    "        feature_col_indices.append(col_to_idx.get(col, -1))\n",
    "    \n",
    "    # Add asset2 features\n",
    "    for feat in asset2_features:\n",
    "        col = (asset2, feat)\n",
    "        feature_col_indices.append(col_to_idx.get(col, -1))\n",
    "    \n",
    "    # Get special column indices\n",
    "    spread_col = (pair_str, 'spreadNorm')\n",
    "    spread_idx = col_to_idx.get(spread_col, -1)\n",
    "    \n",
    "    asset1_close_col = (asset1, 'close')\n",
    "    asset2_close_col = (asset2, 'close')\n",
    "    asset1_close_idx = col_to_idx.get(asset1_close_col, -1)\n",
    "    asset2_close_idx = col_to_idx.get(asset2_close_col, -1)\n",
    "    \n",
    "    # Vectorized filling of tensors\n",
    "    for t in range(n_samples):\n",
    "        # Fill ALL features (pair + asset1 + asset2)\n",
    "        for j, feat_idx in enumerate(feature_col_indices):\n",
    "            if feat_idx != -1:\n",
    "                X[t, 0, j, :] = window_values[t:t+lookback, feat_idx]\n",
    "        \n",
    "        # Calculate ACTUAL asset returns from log prices\n",
    "        if asset1_close_idx != -1 and asset2_close_idx != -1:\n",
    "            curr_price1 = window_values[t+lookback-1, asset1_close_idx]\n",
    "            next_price1 = window_values[t+lookback, asset1_close_idx]\n",
    "            curr_price2 = window_values[t+lookback-1, asset2_close_idx]\n",
    "            next_price2 = window_values[t+lookback, asset2_close_idx]\n",
    "            \n",
    "            # Log returns (since prices are already in log space)\n",
    "            R[t, 0] = next_price1 - curr_price1  # asset1 log return\n",
    "            R[t, 1] = next_price2 - curr_price2  # asset2 log return\n",
    "        \n",
    "        # Calculate volatility using spreadNorm\n",
    "        if spread_idx != -1:\n",
    "            VOL[t, 0] = abs(window_values[t+lookback-1, spread_idx])\n",
    "    \n",
    "    # Create timestamps list once at the end\n",
    "    timestamps = window_timestamps[lookback:lookback+n_samples].tolist()\n",
    "    \n",
    "    return X, R, VOL, timestamps\n",
    "\n",
    "# Initialize lists to store tensors and metadata\n",
    "all_X, all_R, all_VOL = [], [], []\n",
    "all_pairs = []\n",
    "all_features = set()\n",
    "all_sample_timestamps = []  # Track timestamps for each sample\n",
    "ticker_set = set()  # Keep track of unique tickers\n",
    "\n",
    "# Process each pair and its valid intervals\n",
    "total_pairs = len(valid_intervals_per_pair)\n",
    "for pair_idx, (pair, intervals) in enumerate(valid_intervals_per_pair.items(), 1):\n",
    "    \n",
    "    # Add both assets to the ticker set\n",
    "    ticker_set.update(pair)\n",
    "    \n",
    "    skipped_intervals = 0\n",
    "    valid_intervals = 0\n",
    "    for interval_idx, (start, end) in enumerate(intervals, 1):\n",
    "        \n",
    "        X, R, VOL, timestamps = build_state_tensor_for_interval(\n",
    "            panel, pair, start, end, CONFIG[\"ENV\"][\"lookback_window\"]\n",
    "        )\n",
    "        \n",
    "        if X is not None:\n",
    "            all_X.append(X)\n",
    "            all_R.append(R)\n",
    "            all_VOL.append(VOL)\n",
    "            # Store pair index for each sample\n",
    "            for _ in range(len(X)):\n",
    "                all_pairs.append(pair)\n",
    "            # Add timestamps for each sample\n",
    "            all_sample_timestamps.extend(timestamps)\n",
    "            # Get features for this pair\n",
    "            pair_str = f\"{pair[0]}_{pair[1]}\"\n",
    "            pair_features = [col[1] for col in panel.columns if col[0] == pair_str]\n",
    "            all_features.update(pair_features)\n",
    "            valid_intervals += 1\n",
    "        else:\n",
    "            skipped_intervals += 1\n",
    "    print(f\"Finished pair {pair} ({pair_idx}/{total_pairs}). Valid intervals: {valid_intervals}, Skipped intervals: {skipped_intervals}, Total: {len(intervals)}\")\n",
    "\n",
    "# Combine all tensors\n",
    "if all_X:\n",
    "    X_all = np.concatenate(all_X, axis=0)\n",
    "    R_all = np.concatenate(all_R, axis=0)\n",
    "    VOL_all = np.concatenate(all_VOL, axis=0)\n",
    "    FEAT_ORDER = sorted(list(all_features))\n",
    "    TICKER_ORDER = sorted(list(ticker_set))  # Create sorted list of unique tickers\n",
    "    SAMPLE_TIMESTAMPS = pd.DatetimeIndex(all_sample_timestamps)  # Convert to DatetimeIndex\n",
    "    \n",
    "    print(\"\\nFinal tensor shapes:\")\n",
    "    print(f\"X (states): {X_all.shape}\")\n",
    "    print(f\"R (returns): {R_all.shape}\")\n",
    "    print(f\"VOL (volatilities): {VOL_all.shape}\")\n",
    "    print(f\"\\nTotal samples: {len(all_pairs)}\")\n",
    "    print(f\"Total unique pairs: {len(set(all_pairs))}\")\n",
    "    print(f\"Features: {FEAT_ORDER}\")\n",
    "    print(f\"Timestamps range: {SAMPLE_TIMESTAMPS.min()} to {SAMPLE_TIMESTAMPS.max()}\")\n",
    "else:\n",
    "    print(\"No valid data found for any pair and interval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745f60af",
   "metadata": {},
   "source": [
    "## Define Splits and Adjust Timezones\n",
    "\n",
    "This section defines the training and validation splits for the dataset. It ensures that the data is divided appropriately to train the model, validate its performance during training, and test its final performance on unseen data. Also, it adjusts the timezones of the datetime indices to ensure consistency across the dataset. This is necessary in case the data comes from multiple sources with different timezone settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a8a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create boolean mask for date slicing\n",
    "def date_slice_mask(times: pd.DatetimeIndex, start: str, end: str):\n",
    "    # convert input dates to UTC timestamps\n",
    "    start_ts = pd.Timestamp(start).tz_localize('UTC')\n",
    "    end_ts = pd.Timestamp(end).tz_localize('UTC')\n",
    "    \n",
    "    # ensure time index is in UTC\n",
    "    if times.tz is None: # if naive, localize to UTC\n",
    "        times = times.tz_localize('UTC')\n",
    "    elif times.tz != pytz.UTC: # if timezone-aware but not UTC, convert to UTC\n",
    "        times = times.tz_convert('UTC')\n",
    "        \n",
    "    mask = (times >= start_ts) & (times <= end_ts) # return boolean mask for splits later on\n",
    "\n",
    "    if not mask.any():\n",
    "        raise ValueError(f\"No data points found in period {start} to {end}\")\n",
    "        \n",
    "    return mask\n",
    "\n",
    "# Create time splits based on configuration\n",
    "TIME_INDEX = panel.index\n",
    "\n",
    "# Create simple train/val/test split based on CONFIG dates\n",
    "try:\n",
    "    train_mask = date_slice_mask(TIME_INDEX, CONFIG[\"SPLITS\"][\"train\"][0], CONFIG[\"SPLITS\"][\"train\"][1])\n",
    "    val_mask = date_slice_mask(TIME_INDEX, CONFIG[\"SPLITS\"][\"val\"][0], CONFIG[\"SPLITS\"][\"val\"][1])\n",
    "    test_mask = date_slice_mask(TIME_INDEX, CONFIG[\"SPLITS\"][\"test\"][0], CONFIG[\"SPLITS\"][\"test\"][1])\n",
    "\n",
    "    print(f\"\\nTraining windows: {train_mask.sum()} timesteps\")\n",
    "    print(f\"Training period: {TIME_INDEX[train_mask].min()} to {TIME_INDEX[train_mask].max()}\")\n",
    "    \n",
    "    print(f\"\\nValidation windows: {val_mask.sum()} timesteps\")\n",
    "    print(f\"Validation period: {TIME_INDEX[val_mask].min()} to {TIME_INDEX[val_mask].max()}\")\n",
    "    \n",
    "    print(f\"\\nTest windows: {test_mask.sum()} timesteps\")\n",
    "    print(f\"Test period: {TIME_INDEX[test_mask].min()} to {TIME_INDEX[test_mask].max()}\")\n",
    "\n",
    "    # Create splits for training\n",
    "    SPLITS = [{\n",
    "        \"name\": \"DateSplit\",\n",
    "        \"train\": train_mask,\n",
    "        \"val\": val_mask,\n",
    "        \"test\": test_mask\n",
    "    }]\n",
    "except ValueError as e:\n",
    "\n",
    "    print(\"\\nError creating splits:\", str(e))\n",
    "    print(f\"Data available: {TIME_INDEX.min()} to {TIME_INDEX.max()}\")\n",
    "\n",
    "    print(\"\\nPlease check your date ranges in CONFIG:\")    \n",
    "    print(f\"Test: {CONFIG['SPLITS']['test']}\")\n",
    "\n",
    "    print(f\"Train: {CONFIG['SPLITS']['train']}\")    \n",
    "    print(f\"Validation: {CONFIG['SPLITS']['val']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be710c8e",
   "metadata": {},
   "source": [
    "## Environment Configuration\n",
    "\n",
    "This is a custom Gymnasium environment for portfolio optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aab18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioWeightsEnvReturn(gym.Env):\n",
    "    \"\"\"\n",
    "    Statistical Arbitrage Environment with continuous action space:\n",
    "    - Action value ranges from -1 to 1\n",
    "    - Action = -1: Maximum short asset1, long asset2\n",
    "    - Action = 0: Close all positions (100% cash)\n",
    "    - Action = 1: Maximum long asset1, short asset2\n",
    "    - Intermediate values: Proportional position sizing\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\": []}\n",
    "\n",
    "    def __init__(self, X, R, VOL, tickers, lookback, cfg_env):\n",
    "        super().__init__()\n",
    "        self.X = X # state tensor (samples, pairs, features, lookback)\n",
    "        self.R = R # asset returns (samples, 2) where R[:,0]=asset1, R[:,1]=asset2\n",
    "        self.VOL = VOL # current period volatilities (samples, pairs)\n",
    "        self.tickers = tickers # list of assets\n",
    "        self.lookback = lookback # lookback window\n",
    "        self.cfg = cfg_env # environment configuration taken from CONFIG\n",
    "\n",
    "        # We work with just the first pair (single pair per sample)\n",
    "        self.active_pair_idx = 0\n",
    "        \n",
    "        self.n_assets = 2  # Pair trading: 2 assets\n",
    "        self.include_cash = cfg_env[\"include_cash\"]\n",
    "        self.shorting = cfg_env[\"shorting\"]\n",
    "\n",
    "        # Calculate observation space dimension\n",
    "        n_features = X.shape[2]  # number of features per asset\n",
    "        market_obs_dim = n_features * lookback  # observation dimension for one pair\n",
    "        position_obs_dim = 3  # Current portfolio weights: [asset1, asset2, cash]\n",
    "        obs_dim = market_obs_dim + position_obs_dim\n",
    "        \n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-5, \n",
    "            high=5, \n",
    "            shape=(obs_dim,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Continuous action space: -1 to 1\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-1.0,\n",
    "            high=1.0,\n",
    "            shape=(1,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.commission = cfg_env[\"transaction_costs\"][\"commission_bps\"] / 1e4\n",
    "        self.slippage = cfg_env[\"transaction_costs\"][\"slippage_bps\"] / 1e4\n",
    "        self.risk_lambda = cfg_env[\"reward\"][\"risk_lambda\"]\n",
    "\n",
    "        self.reset(seed=cfg_env.get(\"seed\", 42))\n",
    "\n",
    "    def _continuous_to_weights(self, action: float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert continuous action to portfolio weights [asset1, asset2, cash]\n",
    "        Action ranges from -1 to 1:\n",
    "        - Action = -1: Short asset1 (50%), Long asset2 (50%), No cash\n",
    "        - Action = 0:  No position on assets, 100% cash\n",
    "        - Action = 1:  Long asset1 (50%), Short asset2 (50%), No cash\n",
    "        - Intermediate values scale proportionally\n",
    "        \"\"\"\n",
    "        # Clip action to valid range\n",
    "        action = np.clip(action, -1.0, 1.0)\n",
    "        \n",
    "        # Scale action to position size (max 50% per leg)\n",
    "        position_size = action * 0.5\n",
    "        \n",
    "        # Asset1 weight = position_size, Asset2 weight = -position_size\n",
    "        # Cash fills the remainder\n",
    "        asset1_weight = position_size\n",
    "        asset2_weight = -position_size\n",
    "        cash_weight = 1.0 - abs(asset1_weight) - abs(asset2_weight)\n",
    "        \n",
    "        return np.array([asset1_weight, asset2_weight, cash_weight])\n",
    "    \n",
    "    def _to_obs(self, t):\n",
    "        # Get market features for the current active pair only\n",
    "        market_features = self.X[t, self.active_pair_idx, :, :].reshape(-1).astype(np.float32)\n",
    "        \n",
    "        # Get current portfolio weights [asset1, asset2, cash]\n",
    "        position_features = self.w.astype(np.float32)\n",
    "        \n",
    "        # Concatenate market features and position features\n",
    "        obs = np.concatenate([market_features, position_features])\n",
    "        \n",
    "        return np.clip(obs, -5.0, 5.0)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if options is not None:\n",
    "            # Future-proof: handle options if needed\n",
    "            pass\n",
    "        self.t = 0\n",
    "        self.portfolio_value = 1.0\n",
    "        self.w = np.array([0.0, 0.0, 1.0])  # Start with 100% cash\n",
    "        self.last_action = 0.0  # Start with neutral position (all cash)\n",
    "        obs = self._to_obs(self.t)\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # Extract scalar action from array if needed\n",
    "        if isinstance(action, np.ndarray):\n",
    "            action = float(action[0])\n",
    "        else:\n",
    "            action = float(action)\n",
    "        \n",
    "        # Convert continuous action to weights\n",
    "        w_target = self._continuous_to_weights(action)\n",
    "        \n",
    "        # Calculate turnover (change in positions)\n",
    "        turnover = np.sum(np.abs(w_target[:2] - self.w[:2]))\n",
    "        trading_cost = (self.commission + self.slippage) * turnover\n",
    "\n",
    "        # Update weights FIRST (before calculating returns)\n",
    "        # This represents taking the position at the beginning of the period\n",
    "        self.w = w_target\n",
    "\n",
    "        # Get returns and volatility for the active pair\n",
    "        # NOTE: R now contains actual asset log returns: R[:,0]=asset1, R[:,1]=asset2\n",
    "        asset1_ret = self.R[self.t, 0]  # Actual asset1 log return\n",
    "        asset2_ret = self.R[self.t, 1]  # Actual asset2 log return\n",
    "        pair_vol = self.VOL[self.t, self.active_pair_idx]\n",
    "        \n",
    "        # Clip individual asset returns to prevent numerical instability from data errors\n",
    "        # Log return of 0.1 = ~10.5% simple return per step (reasonable for crypto)\n",
    "        asset1_ret = np.clip(asset1_ret, -0.1, 0.1)\n",
    "        asset2_ret = np.clip(asset2_ret, -0.1, 0.1)\n",
    "        \n",
    "        # Calculate portfolio log return using actual weights and asset returns\n",
    "        # This is the correct way: weight each asset's actual return\n",
    "        portfolio_log_ret = self.w[0] * asset1_ret + self.w[1] * asset2_ret\n",
    "        \n",
    "        # The cash position (w[2]) earns 0 return, so we don't include it\n",
    "        # portfolio_log_ret is already the weighted sum of risky assets\n",
    "        \n",
    "        # Calculate instantaneous volatility (position-weighted)\n",
    "        inst_vol = pair_vol * (np.abs(self.w[0]) + np.abs(self.w[1]))\n",
    "\n",
    "        # Reward function: log return minus trading costs and risk penalty\n",
    "        reward = portfolio_log_ret - trading_cost - self.risk_lambda * inst_vol\n",
    "\n",
    "        # Update portfolio value using exponential of log return\n",
    "        # portfolio_value_new = portfolio_value_old * exp(log_return - costs)\n",
    "        self.portfolio_value *= math.exp(portfolio_log_ret - trading_cost)\n",
    "        self.last_action = action\n",
    "        \n",
    "        self.t += 1\n",
    "        terminated = (self.t >= len(self.R)-1)\n",
    "        truncated = False\n",
    "\n",
    "        obs = self._to_obs(self.t) if not terminated else self._to_obs(self.t-1)\n",
    "        \n",
    "        # Calculate leverage (sum of absolute weights, excluding cash)\n",
    "        leverage = np.sum(np.abs(self.w[:2]))\n",
    "        \n",
    "        info = {\n",
    "            \"portfolio_value\": self.portfolio_value,\n",
    "            \"turnover\": turnover,\n",
    "            \"total_leverage\": leverage,\n",
    "            \"portfolio_log_ret\": portfolio_log_ret,\n",
    "            \"inst_vol\": inst_vol,\n",
    "            \"action_taken\": action  # Continuous action value for logging\n",
    "        }\n",
    "        \n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f8dd97",
   "metadata": {},
   "source": [
    "## Environment with Quadratic Utility Reward Function\n",
    "\n",
    "This environment uses a utility-based reward function instead of the standard mean-variance approach. The reward is based on **quadratic utility**:\n",
    "\n",
    "$$U(x) = x - \\frac{\\lambda}{2} x^2$$\n",
    "\n",
    "where $x_t = r_t^p - c_t$ (net portfolio return after transaction costs).\n",
    "\n",
    "The complete reward function becomes:\n",
    "\n",
    "$$R_t = (r_t^p - c_t) - \\frac{\\lambda}{2}(r_t^p - c_t)^2$$\n",
    "\n",
    "This penalizes both negative returns and excessively high returns (reducing risk-taking), with the risk aversion parameter $\\lambda$ controlling the strength of the penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a928ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioWeightsEnvUtility(gym.Env):\n",
    "    \"\"\"\n",
    "    Statistical Arbitrage Environment with quadratic utility-based reward function.\n",
    "    \n",
    "    Uses a utility-based reward instead of mean-variance:\n",
    "    R_t = (r_t^p - c_t) - (lambda/2) * (r_t^p - c_t)^2\n",
    "    \n",
    "    where:\n",
    "    - r_t^p = portfolio log return\n",
    "    - c_t = transaction costs\n",
    "    - lambda = risk aversion parameter (lambda_utility from config)\n",
    "    \n",
    "    Action space and mechanics are identical to PortfolioWeightsEnvReturn:\n",
    "    - Action value ranges from -1 to 1\n",
    "    - Action = -1: Maximum short asset1, long asset2\n",
    "    - Action = 0: Close all positions (100% cash)\n",
    "    - Action = 1: Maximum long asset1, short asset2\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\": []}\n",
    "\n",
    "    def __init__(self, X, R, VOL, tickers, lookback, cfg_env):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.R = R\n",
    "        self.VOL = VOL\n",
    "        self.tickers = tickers\n",
    "        self.lookback = lookback\n",
    "        self.cfg = cfg_env\n",
    "\n",
    "        self.active_pair_idx = 0\n",
    "        self.n_assets = 2\n",
    "        self.include_cash = cfg_env[\"include_cash\"]\n",
    "        self.shorting = cfg_env[\"shorting\"]\n",
    "\n",
    "        # Calculate observation space dimension\n",
    "        n_features = X.shape[2]\n",
    "        market_obs_dim = n_features * lookback\n",
    "        position_obs_dim = 3\n",
    "        obs_dim = market_obs_dim + position_obs_dim\n",
    "        \n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-5, \n",
    "            high=5, \n",
    "            shape=(obs_dim,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.action_space = spaces.Box(\n",
    "            low=-1.0,\n",
    "            high=1.0,\n",
    "            shape=(1,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.commission = cfg_env[\"transaction_costs\"][\"commission_bps\"] / 1e4\n",
    "        self.slippage = cfg_env[\"transaction_costs\"][\"slippage_bps\"] / 1e4\n",
    "        \n",
    "        # Quadratic utility parameter (lambda)\n",
    "        self.lambda_utility = cfg_env[\"reward\"].get(\"lambda_utility\", 20.0)\n",
    "\n",
    "        self.reset(seed=cfg_env.get(\"seed\", 42))\n",
    "\n",
    "    def _continuous_to_weights(self, action: float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert continuous action to portfolio weights [asset1, asset2, cash]\n",
    "        Identical to PortfolioWeightsEnv implementation.\n",
    "        \"\"\"\n",
    "        action = np.clip(action, -1.0, 1.0)\n",
    "        position_size = action * 0.5\n",
    "        asset1_weight = position_size\n",
    "        asset2_weight = -position_size\n",
    "        cash_weight = 1.0 - abs(asset1_weight) - abs(asset2_weight)\n",
    "        return np.array([asset1_weight, asset2_weight, cash_weight])\n",
    "    \n",
    "    def _to_obs(self, t):\n",
    "        \"\"\"Get observation at time t.\"\"\"\n",
    "        market_features = self.X[t, self.active_pair_idx, :, :].reshape(-1).astype(np.float32)\n",
    "        position_features = self.w.astype(np.float32)\n",
    "        obs = np.concatenate([market_features, position_features])\n",
    "        return np.clip(obs, -5.0, 5.0)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.t = 0\n",
    "        self.portfolio_value = 1.0\n",
    "        self.w = np.array([0.0, 0.0, 1.0])  # Start with 100% cash\n",
    "        self.last_action = 0.0\n",
    "        obs = self._to_obs(self.t)\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # Extract scalar action\n",
    "        if isinstance(action, np.ndarray):\n",
    "            action = float(action[0])\n",
    "        else:\n",
    "            action = float(action)\n",
    "        \n",
    "        # Convert action to weights\n",
    "        w_target = self._continuous_to_weights(action)\n",
    "        \n",
    "        # Calculate turnover and trading costs\n",
    "        turnover = np.sum(np.abs(w_target[:2] - self.w[:2]))\n",
    "        trading_cost = (self.commission + self.slippage) * turnover\n",
    "\n",
    "        # Update weights\n",
    "        self.w = w_target\n",
    "\n",
    "        # Get returns for the active pair\n",
    "        asset1_ret = self.R[self.t, 0]\n",
    "        asset2_ret = self.R[self.t, 1]\n",
    "        \n",
    "        # Clip returns to prevent numerical instability\n",
    "        asset1_ret = np.clip(asset1_ret, -0.1, 0.1)\n",
    "        asset2_ret = np.clip(asset2_ret, -0.1, 0.1)\n",
    "        \n",
    "        # Calculate portfolio log return\n",
    "        portfolio_log_ret = self.w[0] * asset1_ret + self.w[1] * asset2_ret\n",
    "        \n",
    "        # Net return after transaction costs\n",
    "        net_return = portfolio_log_ret - trading_cost\n",
    "        \n",
    "        # QUADRATIC UTILITY REWARD FUNCTION\n",
    "        # R_t = x_t - (lambda/2) * x_t^2\n",
    "        # where x_t = net_return = r_t^p - c_t\n",
    "        reward = net_return - (self.lambda_utility / 2.0) * (net_return ** 2)\n",
    "\n",
    "        # Update portfolio value using exponential of log return\n",
    "        self.portfolio_value *= math.exp(net_return)\n",
    "        self.last_action = action\n",
    "        \n",
    "        self.t += 1\n",
    "        terminated = (self.t >= len(self.R)-1)\n",
    "        truncated = False\n",
    "\n",
    "        obs = self._to_obs(self.t) if not terminated else self._to_obs(self.t-1)\n",
    "        \n",
    "        # Calculate leverage\n",
    "        leverage = np.sum(np.abs(self.w[:2]))\n",
    "        \n",
    "        info = {\n",
    "            \"portfolio_value\": self.portfolio_value,\n",
    "            \"turnover\": turnover,\n",
    "            \"total_leverage\": leverage,\n",
    "            \"portfolio_log_ret\": portfolio_log_ret,\n",
    "            \"net_return\": net_return,\n",
    "            \"utility_reward\": reward,\n",
    "            \"action_taken\": action\n",
    "        }\n",
    "        \n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34099461",
   "metadata": {},
   "source": [
    "### Test the Utility Environment\n",
    "\n",
    "Quick test to verify the quadratic utility environment works correctly and compare reward computation with the standard environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba3f503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Compare standard vs utility environment reward calculation\n",
    "# This cell demonstrates the difference between the two reward functions\n",
    "\n",
    "# Note: This test requires X_train, R_train, VOL_train to be created first\n",
    "# Run the data preprocessing cells before running this test\n",
    "\n",
    "try:\n",
    "    # Create both environments with train data\n",
    "    print(\"Creating standard environment...\")\n",
    "    env_standard = PortfolioWeightsEnvReturn(\n",
    "        X,\n",
    "        R, \n",
    "        VOL,\n",
    "        tickers=assets,\n",
    "        lookback=CONFIG[\"ENV\"][\"lookback_window\"],\n",
    "        cfg_env=CONFIG[\"ENV\"]\n",
    "    )\n",
    "    \n",
    "    print(\"\\nCreating utility-based environment...\")\n",
    "    env_utility = PortfolioWeightsEnvUtility(\n",
    "        X,\n",
    "        R,\n",
    "        VOL, \n",
    "        tickers=assets,\n",
    "        lookback=CONFIG[\"ENV\"][\"lookback_window\"],\n",
    "        cfg_env=CONFIG[\"ENV\"]\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"REWARD COMPARISON TEST\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Reset both environments\n",
    "    obs_std, _ = env_standard.reset(seed=42)\n",
    "    obs_util, _ = env_utility.reset(seed=42)\n",
    "    \n",
    "    # Test various actions\n",
    "    test_actions = [-1.0, -0.5, 0.0, 0.5, 1.0]\n",
    "    \n",
    "    print(f\"\\nLambda (risk penalty, standard): {env_standard.risk_lambda}\")\n",
    "    print(f\"Lambda (utility risk aversion): {env_utility.lambda_utility}\")\n",
    "    print()\n",
    "    \n",
    "    for action in test_actions:\n",
    "        # Reset both environments to same state\n",
    "        env_standard.reset(seed=42)\n",
    "        env_utility.reset(seed=42)\n",
    "        \n",
    "        # Take same action in both\n",
    "        obs1, reward_std, done1, trunc1, info_std = env_standard.step([action])\n",
    "        obs2, reward_util, done2, trunc2, info_util = env_utility.step([action])\n",
    "        \n",
    "        print(f\"Action: {action:+.2f}\")\n",
    "        print(f\"  Standard Reward:  {reward_std:+.6f}\")\n",
    "        print(f\"  Utility Reward:   {reward_util:+.6f}\")\n",
    "        print(f\"  Difference:       {reward_util - reward_std:+.6f}\")\n",
    "        print(f\"  Portfolio Return: {info_std['portfolio_log_ret']:+.6f}\")\n",
    "        print(f\"  Net Return:       {info_util['net_return']:+.6f}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"✓ Both environments created successfully!\")\n",
    "    print(\"✓ You can now train models using either environment class\")\n",
    "    \n",
    "except NameError as e:\n",
    "    print(f\"⚠ Data not yet loaded: {e}\")\n",
    "    print(\"Run the data preprocessing cells first to create X_train, R_train, VOL_train\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7281e10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_by_mask(X, R, VOL, timestamps, mask: np.ndarray):\n",
    "    \"\"\"\n",
    "    Slice tensors by time mask using actual sample timestamps.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X, R, VOL : np.ndarray\n",
    "        Tensor data to slice\n",
    "    timestamps : pd.DatetimeIndex\n",
    "        Timestamp for each sample in the tensors\n",
    "    mask : np.ndarray\n",
    "        Boolean mask based on TIME_INDEX from panel\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Sliced tensors matching the time period specified by mask\n",
    "    \"\"\"\n",
    "    # Ensure timestamps is timezone-aware\n",
    "    if timestamps.tz is None:\n",
    "        timestamps = timestamps.tz_localize('UTC')\n",
    "    elif timestamps.tz != pytz.UTC:\n",
    "        timestamps = timestamps.tz_convert('UTC')\n",
    "    \n",
    "    # Get the date range from the mask\n",
    "    mask_times = TIME_INDEX[mask]\n",
    "    if mask_times.tz is None:\n",
    "        mask_times = mask_times.tz_localize('UTC')\n",
    "    elif mask_times.tz != pytz.UTC:\n",
    "        mask_times = mask_times.tz_convert('UTC')\n",
    "    \n",
    "    start_time = mask_times.min()\n",
    "    end_time = mask_times.max()\n",
    "    \n",
    "    # Create a mask for samples that fall within the time range\n",
    "    sample_mask = (timestamps >= start_time) & (timestamps <= end_time)\n",
    "    idx = np.where(sample_mask)[0]\n",
    "\n",
    "    # If we have no valid indices, raise a ValueError\n",
    "    if len(idx) == 0:\n",
    "        raise ValueError(f\"No data points available in the selected time period {start_time} to {end_time}\")\n",
    "    \n",
    "    print(f\"Slicing: {len(idx)} samples out of {len(timestamps)} fall in range {start_time} to {end_time}\")\n",
    "    \n",
    "    return X[idx], R[idx], VOL[idx]\n",
    "\n",
    "def make_env_from_mask(mask, name=\"env\"):\n",
    "    try:\n",
    "        X_s, R_s, V_s = slice_by_mask(X_all, R_all, VOL_all, SAMPLE_TIMESTAMPS, mask)\n",
    "\n",
    "        env = PortfolioWeightsEnvUtility(X_s, R_s, V_s, TICKER_ORDER, CONFIG[\"ENV\"][\"lookback_window\"], CONFIG[\"ENV\"])\n",
    "        env = Monitor(env, filename=None)\n",
    "        return env\n",
    "    except ValueError as e:\n",
    "        print(f\"Error creating environment '{name}': {str(e)}\")\n",
    "        print(f\"Mask summary: {np.sum(mask)} True values out of {len(mask)} total\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadc07b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annualize_factor(sampling: str):\n",
    "    return ANNUALIZATION.get(sampling, 365*24)\n",
    "\n",
    "def compute_metrics(equity_curve: pd.Series, sampling: str, turnover_series: pd.Series = None):\n",
    "    ret = equity_curve.pct_change().dropna()\n",
    "    ann = annualize_factor(sampling)\n",
    "    mu = ret.mean() * ann\n",
    "    sigma = ret.std() * math.sqrt(ann)\n",
    "    sharpe = mu / (sigma + 1e-12)\n",
    "    downside = ret[ret < 0].std() * math.sqrt(ann)\n",
    "    sortino = mu / (downside + 1e-12)\n",
    "    if len(equity_curve) > 1:\n",
    "        # Calculate years based on number of samples and sampling frequency\n",
    "        if isinstance(equity_curve.index, pd.DatetimeIndex):\n",
    "            dt_years = (equity_curve.index[-1] - equity_curve.index[0]).total_seconds() / (365 * 24 * 3600)\n",
    "        else:\n",
    "            # If using RangeIndex, calculate based on sampling frequency\n",
    "            samples = len(equity_curve)\n",
    "            samples_per_year = annualize_factor(sampling)\n",
    "            dt_years = samples / samples_per_year\n",
    "        dt_years = float(dt_years) if float(dt_years) != 0 else 1e-12\n",
    "        cagr = (equity_curve.iloc[-1] / equity_curve.iloc[0]) ** (1/dt_years) - 1\n",
    "    else:\n",
    "        cagr = 0.0\n",
    "    cummax = equity_curve.cummax()\n",
    "    dd = (equity_curve / cummax - 1).min()\n",
    "    maxdd = float(dd)\n",
    "    calmar = mu / (abs(maxdd) + 1e-12)\n",
    "    hit_ratio = (ret > 0).mean()\n",
    "    turnover = turnover_series.mean() if turnover_series is not None and len(turnover_series)>0 else np.nan\n",
    "    return {\"CAGR\": cagr, \"Sharpe\": sharpe, \"Sortino\": sortino, \"MaxDrawdown\": maxdd, \"Calmar\": calmar, \"Volatility\": sigma, \"Turnover\": turnover, \"HitRatio\": hit_ratio}\n",
    "\n",
    "def plot_series(series: pd.Series, title: str):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(series.index, series.values)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_splits(time_index: pd.DatetimeIndex, train_mask: np.ndarray, val_mask: np.ndarray, test_mask: np.ndarray = None):\n",
    "    \"\"\"Plot timeline showing training, validation, and test periods.\"\"\"\n",
    "    plt.figure(figsize=(15,3))\n",
    "    \n",
    "    # Convert time index to numeric for plotting\n",
    "    x = np.arange(len(time_index))\n",
    "    \n",
    "    # Plot splits\n",
    "    plt.plot(x[train_mask], [1]*train_mask.sum(), 'b.', label='Train', alpha=0.3)\n",
    "    plt.plot(x[val_mask], [1]*val_mask.sum(), 'g.', label='Validation', alpha=0.3)\n",
    "    if test_mask is not None:\n",
    "        plt.plot(x[test_mask], [1]*test_mask.sum(), 'r.', label='Test', alpha=0.3)\n",
    "    \n",
    "    # Set x-axis ticks to show dates\n",
    "    tick_locations = np.linspace(0, len(time_index)-1, 5, dtype=int)\n",
    "    plt.xticks(tick_locations, time_index[tick_locations].strftime('%Y-%m-%d'), rotation=45)\n",
    "    \n",
    "    plt.title(\"Data Split Timeline\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def backtest_env(env: gym.Env, model=None, include_leverage: bool = False, include_returns: bool = False):\n",
    "    # Get the unwrapped environment\n",
    "    unwrapped = env.unwrapped if hasattr(env, 'unwrapped') else env\n",
    "    \n",
    "    obs, _ = env.reset()\n",
    "    pv, turns, levs, actions, rets = [], [], [], [], []  # Initialize lists for portfolio value, turnover, leverage, actions, and returns\n",
    "    \n",
    "    for _ in range(len(unwrapped.R)-1):\n",
    "        if model is None:\n",
    "            action = np.array([0.0])  # Default to cash position (action=0)\n",
    "        else:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, _, done, _, info = env.step(action)\n",
    "        pv.append(info[\"portfolio_value\"])\n",
    "        turns.append(info[\"turnover\"])\n",
    "        levs.append(info[\"total_leverage\"])  # Track leverage from info dict\n",
    "        actions.append(info.get(\"action_taken\", action[0] if isinstance(action, np.ndarray) else action))  # Track action taken\n",
    "        rets.append(info.get(\"portfolio_log_ret\", 0.0))  # Track log returns from info dict\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    idx = pd.RangeIndex(start=0, stop=len(pv), step=1)\n",
    "    ec = pd.Series(pv, index=idx)\n",
    "    to = pd.Series(turns, index=idx)\n",
    "    lev = pd.Series(levs, index=idx)  # Create leverage Series\n",
    "    act = pd.Series(actions, index=idx)  # Create actions Series\n",
    "    ret = pd.Series(rets, index=idx)  # Create returns Series\n",
    "    \n",
    "    if include_returns:\n",
    "        return ec, to, lev, act, ret\n",
    "    if include_leverage:\n",
    "        return ec, to, lev, act\n",
    "    return ec, to\n",
    "\n",
    "print(\"Visualizing data splits timeline...\")\n",
    "plot_splits(TIME_INDEX, train_mask, val_mask, test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1902bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the actual variables\n",
    "print(f\"all_X length: {len(all_X)}\")\n",
    "print(f\"all_R length: {len(all_R)}\")\n",
    "print(f\"all_VOL length: {len(all_VOL)}\")\n",
    "print(f\"all_pairs length: {len(all_pairs)}\")\n",
    "print(f\"all_sample_timestamps length: {len(all_sample_timestamps)}\")\n",
    "print(f\"\\ntimestamps variable (from last iteration): type={type(timestamps)}, length={len(timestamps)}\")\n",
    "if len(timestamps) > 0:\n",
    "    print(f\"First 5 timestamps: {timestamps[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc41478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_dir(CONFIG[\"IO\"][\"models_dir\"])\n",
    "ensure_dir(CONFIG[\"EVAL\"][\"reports_dir\"])\n",
    "\n",
    "# Check if required tensors exist\n",
    "if 'X_all' not in globals() or 'R_all' not in globals() or 'VOL_all' not in globals() or 'SAMPLE_TIMESTAMPS' not in globals():\n",
    "    raise NameError(\n",
    "        \"Required tensors (X_all, R_all, VOL_all, SAMPLE_TIMESTAMPS) not found!\\n\"\n",
    "        \"Please run the tensor construction cell (cell 29) before running this cell.\\n\"\n",
    "        \"The cell starts with: def build_state_tensor_for_interval...\"\n",
    "    )\n",
    "\n",
    "RESULTS = []\n",
    "\n",
    "for split in SPLITS:\n",
    "    train_env = make_env_from_mask(split[\"train\"], name=f\"{split['name']}_train\")\n",
    "    val_env = make_env_from_mask(split[\"val\"], name=f\"{split['name']}_val\")\n",
    "\n",
    "    vec_train = DummyVecEnv([lambda env=train_env: env])\n",
    "    vec_val = DummyVecEnv([lambda env=val_env: env])\n",
    "\n",
    "    # PPO without custom LR schedule / entropy decay\n",
    "    model = PPO(\n",
    "        policy=CONFIG[\"RL\"][\"policy\"],\n",
    "        env=vec_train,\n",
    "        gamma=CONFIG[\"RL\"][\"gamma\"],\n",
    "        gae_lambda=CONFIG[\"RL\"][\"gae_lambda\"],\n",
    "        clip_range=CONFIG[\"RL\"][\"clip_range\"],\n",
    "        n_steps=CONFIG[\"RL\"][\"n_steps\"],\n",
    "        batch_size=CONFIG[\"RL\"][\"batch_size\"],\n",
    "        learning_rate=CONFIG[\"RL\"][\"learning_rate\"],  # fixed\n",
    "        ent_coef=CONFIG[\"RL\"][\"ent_coef\"],            # fixed\n",
    "        vf_coef=CONFIG[\"RL\"][\"vf_coef\"],\n",
    "        max_grad_norm=CONFIG[\"RL\"][\"max_grad_norm\"],\n",
    "        tensorboard_log=CONFIG[\"IO\"][\"tb_logdir\"],\n",
    "        device=\"cpu\",\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    eval_callback = EvalCallback(\n",
    "        vec_val,\n",
    "        best_model_save_path=CONFIG[\"IO\"][\"models_dir\"],\n",
    "        log_path=CONFIG[\"IO\"][\"models_dir\"],\n",
    "        eval_freq=CONFIG[\"EVAL\"][\"frequency\"],\n",
    "        deterministic=True,\n",
    "        render=False\n",
    "    )\n",
    "\n",
    "    model.learn(total_timesteps=CONFIG[\"RL\"][\"timesteps\"], callback=eval_callback)\n",
    "\n",
    "    # Save the final trained model\n",
    "    model_path = os.path.join(CONFIG[\"IO\"][\"models_dir\"], \"final_model.zip\")\n",
    "    model.save(model_path)\n",
    "\n",
    "    # Run on test set (unseen data)\n",
    "    test_env = make_env_from_mask(split[\"test\"], name=f\"{split['name']}_test\")\n",
    "    ec, to, lev, act = backtest_env(test_env, model=model, include_leverage=True)\n",
    "\n",
    "    # Compare against equal weight strategy on test set\n",
    "    base_env = test_env.unwrapped if hasattr(test_env, \"unwrapped\") else test_env\n",
    "    raw_returns = np.asarray(base_env.R)\n",
    "    if raw_returns.ndim == 1:\n",
    "        raw_returns = raw_returns.reshape(-1, 1)\n",
    "\n",
    "    steps = min(len(ec.index) - 1, raw_returns.shape[0])\n",
    "    if raw_returns.shape[1] == 0 or steps <= 0:\n",
    "        ec_bench = pd.Series(np.ones(len(ec.index)), index=ec.index)\n",
    "    else:\n",
    "        ew = np.ones(raw_returns.shape[1]) / raw_returns.shape[1]\n",
    "        ec_bench = [1.0]\n",
    "        for i in range(steps):\n",
    "            step_ret = float(np.dot(ew, raw_returns[i].reshape(-1)))\n",
    "            ec_bench.append(ec_bench[-1] * math.exp(step_ret))\n",
    "        if len(ec_bench) < len(ec.index):\n",
    "            ec_bench.extend([ec_bench[-1]] * (len(ec.index) - len(ec_bench)))\n",
    "        ec_bench = pd.Series(ec_bench[:len(ec.index)], index=ec.index)\n",
    "\n",
    "    # Calculate metrics\n",
    "    m_model = compute_metrics(ec, CONFIG[\"DATA\"][\"sampling\"], to)\n",
    "    m_ew = compute_metrics(ec_bench, CONFIG[\"DATA\"][\"sampling\"])\n",
    "\n",
    "    # Add average leverage to model metrics\n",
    "    m_model[\"Avg_Leverage\"] = lev.mean()\n",
    "    m_model[\"Max_Leverage\"] = lev.max()\n",
    "\n",
    "    RESULTS.append({\n",
    "        \"split\": split[\"name\"],\n",
    "        \"model\": m_model,\n",
    "        \"equal_weight\": m_ew\n",
    "    })\n",
    "\n",
    "    if CONFIG[\"EVAL\"][\"plots\"]:\n",
    "        plt.figure(figsize=(15, 8))\n",
    "\n",
    "        # Plot equity curves - Model vs Equal Weight Benchmark\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(ec.index, ec.values, label='PPO Model', color='blue', linewidth=2)\n",
    "        plt.plot(ec_bench.index, ec_bench.values, label='Equal Weight Benchmark', color='orange', linewidth=2, linestyle='--', alpha=0.7)\n",
    "        plt.axhline(y=1.0, color='gray', linestyle=':', alpha=0.3, label='Starting Value')\n",
    "        plt.title(f\"Test Set Equity Curve: Model vs Benchmark ({split['name']})\")\n",
    "        plt.xlabel(\"Steps\")\n",
    "        plt.ylabel(\"Portfolio Value\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Plot actions\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(act.index, act.values, label='Action', drawstyle='steps-post', color='purple')\n",
    "        plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5, label='Neutral (Cash)')\n",
    "        plt.axhline(y=-1, color='red', linestyle=':', alpha=0.3, label='Max Short/Long')\n",
    "        plt.axhline(y=1, color='green', linestyle=':', alpha=0.3, label='Max Long/Short')\n",
    "        plt.ylim(-1.1, 1.1)\n",
    "        plt.title(f\"Test Set Trading Actions ({split['name']}) - Continuous\")\n",
    "        plt.xlabel(\"Steps\")\n",
    "        plt.ylabel(\"Action Value\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eec7971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with all results\n",
    "rows = []\n",
    "for res in RESULTS:\n",
    "    row = {\"split\": res[\"split\"]}\n",
    "    for k, metrics in res.items():\n",
    "        if k == \"split\":\n",
    "            continue\n",
    "        for mname, mval in metrics.items():\n",
    "            row[f\"{k}_{mname}\"] = mval\n",
    "    rows.append(row)\n",
    "\n",
    "df_results = pd.DataFrame(rows)\n",
    "\n",
    "# Reorder columns to group related metrics together\n",
    "column_order = ['split']\n",
    "for model_type in ['model', 'equal_weight']:\n",
    "    for metric in ['CAGR', 'Sharpe', 'Sortino', 'Calmar', 'MaxDrawdown', 'Volatility', 'Turnover', 'HitRatio']:\n",
    "        col = f\"{model_type}_{metric}\"\n",
    "        if col in df_results.columns:\n",
    "            column_order.append(col)\n",
    "    # Add leverage metrics for model only\n",
    "    if model_type == 'model':\n",
    "        for metric in ['Avg_Leverage', 'Max_Leverage']:\n",
    "            col = f\"{model_type}_{metric}\"\n",
    "            if col in df_results.columns:\n",
    "                column_order.append(col)\n",
    "\n",
    "df_results = df_results[column_order]\n",
    "\n",
    "# Format only numeric columns, leave 'split' column as-is\n",
    "format_dict = {col: \"{:.4f}\" for col in df_results.columns if col != 'split'}\n",
    "display(df_results.style.format(format_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f15943",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "ensure_dir(CONFIG[\"EVAL\"][\"reports_dir\"])\n",
    "out_json = os.path.join(CONFIG[\"EVAL\"][\"reports_dir\"], f\"metrics_{ts}.json\")\n",
    "out_csv  = os.path.join(CONFIG[\"EVAL\"][\"reports_dir\"], f\"metrics_{ts}.csv\")\n",
    "df_results.to_csv(out_csv, index=False)\n",
    "with open(out_json, \"w\") as f:\n",
    "    json.dump(RESULTS, f, indent=2)\n",
    "print(\"Saved:\", out_json, \"and\", out_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33105fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best trained model\n",
    "best_split = SPLITS[0]\n",
    "model_path = os.path.join(CONFIG[\"IO\"][\"models_dir\"], \"best_model.zip\")\n",
    "\n",
    "# Check if best model exists, otherwise use the split model\n",
    "if not os.path.exists(model_path):\n",
    "    #model_path = os.path.join(CONFIG[\"IO\"][\"models_dir\"], f\"ppo_{best_split['name']}.zip\")\n",
    "    model_path = os.path.join(CONFIG[\"IO\"][\"models_dir\"], \"final_model.zip\")\n",
    "\n",
    "print(f\"Loading model from: {model_path}\")\n",
    "model = PPO.load(model_path)\n",
    "\n",
    "# Create test environment\n",
    "test_env = make_env_from_mask(best_split[\"test\"], name=\"test_backtest\")\n",
    "\n",
    "# Run backtest\n",
    "print(\"\\nRunning backtest on test set...\")\n",
    "ec, to, lev, act = backtest_env(test_env, model=model, include_leverage=True)\n",
    "\n",
    "# Calculate equal weight benchmark for comparison\n",
    "base_env = test_env.unwrapped if hasattr(test_env, \"unwrapped\") else test_env\n",
    "raw_returns = np.asarray(base_env.R)\n",
    "if raw_returns.ndim == 1:\n",
    "    raw_returns = raw_returns.reshape(-1, 1)\n",
    "\n",
    "steps = min(len(ec.index) - 1, raw_returns.shape[0])\n",
    "if raw_returns.shape[1] == 0 or steps <= 0:\n",
    "    ec_bench = pd.Series(np.ones(len(ec.index)), index=ec.index)\n",
    "else:\n",
    "    ew = np.ones(raw_returns.shape[1]) / raw_returns.shape[1]\n",
    "    ec_bench = [1.0]\n",
    "    for i in range(steps):\n",
    "        step_ret = float(np.dot(ew, raw_returns[i].reshape(-1)))\n",
    "        ec_bench.append(ec_bench[-1] * math.exp(step_ret))\n",
    "    if len(ec_bench) < len(ec.index):\n",
    "        ec_bench.extend([ec_bench[-1]] * (len(ec.index) - len(ec_bench)))\n",
    "    ec_bench = pd.Series(ec_bench[:len(ec.index)], index=ec.index)\n",
    "\n",
    "# Get test period dates\n",
    "test_period_start = CONFIG[\"SPLITS\"][\"test\"][0]\n",
    "test_period_end = CONFIG[\"SPLITS\"][\"test\"][1]\n",
    "\n",
    "print(\"\\nBacktest completed!\")\n",
    "print(f\"Test period: {test_period_start} to {test_period_end}\")\n",
    "print(f\"Total steps: {len(ec)}\")\n",
    "print(f\"Model final value: {ec.iloc[-1]:.4f} ({(ec.iloc[-1] - 1) * 100:.2f}%)\")\n",
    "print(f\"Benchmark final value: {ec_bench.iloc[-1]:.4f} ({(ec_bench.iloc[-1] - 1) * 100:.2f}%)\")\n",
    "print(f\"Average leverage: {lev.mean():.4f}\")\n",
    "print(f\"Average turnover: {to.mean():.4f}\")\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Portfolio Value - Model vs Benchmark\n",
    "axes[0].plot(ec.index, ec.values, label='PPO Model', color='blue', linewidth=2)\n",
    "axes[0].plot(ec_bench.index, ec_bench.values, label='Equal Weight Benchmark', color='orange', linewidth=2, linestyle='--', alpha=0.7)\n",
    "axes[0].axhline(y=1.0, color='gray', linestyle=':', alpha=0.3, label='Starting Value')\n",
    "axes[0].set_title(f'Test Set Portfolio Performance: Model vs Benchmark ({test_period_start} to {test_period_end})', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Steps', fontsize=12)\n",
    "axes[0].set_ylabel('Portfolio Value', fontsize=12)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add performance metrics as text\n",
    "final_value = ec.iloc[-1]\n",
    "total_return = (final_value - 1) * 100\n",
    "max_drawdown = (ec / ec.cummax() - 1).min()\n",
    "bench_return = (ec_bench.iloc[-1] - 1) * 100\n",
    "metrics_text = f'Model:\\n  Return: {total_return:.2f}%\\n  Max DD: {abs(max_drawdown) * 100:.2f}%\\n\\nBenchmark:\\n  Return: {bench_return:.2f}%'\n",
    "axes[0].text(0.02, 0.98, metrics_text, transform=axes[0].transAxes, \n",
    "             fontsize=10, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Plot 2: Trading Actions\n",
    "axes[1].plot(act.index, act.values, label='Action', drawstyle='steps-post', color='purple', linewidth=1.5)\n",
    "axes[1].axhline(y=0, color='gray', linestyle='--', alpha=0.5, label='Neutral (Cash)')\n",
    "axes[1].axhline(y=-1, color='red', linestyle=':', alpha=0.3, label='Max Short/Long')\n",
    "axes[1].axhline(y=1, color='green', linestyle=':', alpha=0.3, label='Max Long/Short')\n",
    "axes[1].fill_between(act.index, act.values, 0, alpha=0.3, color='purple')\n",
    "axes[1].set_ylim(-1.1, 1.1)\n",
    "axes[1].set_title('Trading Actions Over Time', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Steps', fontsize=12)\n",
    "axes[1].set_ylabel('Action Value', fontsize=12)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add action statistics as text\n",
    "action_stats = f'Mean: {act.mean():.3f}\\nStd: {act.std():.3f}\\nMin: {act.min():.3f}\\nMax: {act.max():.3f}'\n",
    "axes[1].text(0.02, 0.98, action_stats, transform=axes[1].transAxes, \n",
    "             fontsize=10, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "# Print detailed statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED BACKTEST STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nPortfolio Metrics:\")\n",
    "print(\"  Starting Value:        1.0000\")\n",
    "print(f\"  Ending Value:          {ec.iloc[-1]:.4f}\")\n",
    "print(f\"  Total Return:          {(ec.iloc[-1] - 1) * 100:.2f}%\")\n",
    "print(f\"  Max Drawdown:          {abs(max_drawdown) * 100:.2f}%\")\n",
    "print(f\"  Volatility (Returns):  {ec.pct_change().std():.6f}\")\n",
    "\n",
    "print(\"\\nTrading Statistics:\")\n",
    "print(f\"  Average Action:        {act.mean():.4f}\")\n",
    "print(f\"  Action Std Dev:        {act.std():.4f}\")\n",
    "print(f\"  Action Range:          [{act.min():.4f}, {act.max():.4f}]\")\n",
    "print(f\"  Avg Turnover:          {to.mean():.4f}\")\n",
    "print(f\"  Avg Leverage:          {lev.mean():.4f}\")\n",
    "print(f\"  Max Leverage:          {lev.max():.4f}\")\n",
    "\n",
    "# Action distribution\n",
    "neutral_pct = ((act.abs() < 0.1).sum() / len(act)) * 100\n",
    "long_pct = ((act > 0.1).sum() / len(act)) * 100\n",
    "short_pct = ((act < -0.1).sum() / len(act)) * 100\n",
    "\n",
    "print(\"\\nAction Distribution:\")\n",
    "print(f\"  Neutral (|action| < 0.1): {neutral_pct:.2f}%\")\n",
    "print(f\"  Long (action > 0.1):      {long_pct:.2f}%\")\n",
    "print(f\"  Short (action < -0.1):    {short_pct:.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acd8d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot showing returns on each timestep\n",
    "print(\"\\nGenerating returns visualization...\")\n",
    "\n",
    "# Get returns from test environment backtest\n",
    "test_env_returns = make_env_from_mask(best_split[\"test\"], name=\"test_returns\")\n",
    "ec_ret, to_ret, lev_ret, act_ret, log_ret = backtest_env(test_env_returns, model=model, include_returns=True)\n",
    "\n",
    "# Convert log returns to simple returns (stable): use expm1 for precision\n",
    "ret = np.expm1(log_ret)\n",
    "\n",
    "# Create comprehensive returns visualization with actions\n",
    "fig, axes = plt.subplots(4, 1, figsize=(15, 16))\n",
    "\n",
    "# Plot 1: Step Returns (Simple Returns as %)\n",
    "ret_pct = ret * 100  # Convert to percentage\n",
    "axes[0].bar(ret.index, ret_pct.values, color=['green' if r > 0 else 'red' for r in ret_pct.values], alpha=0.7, width=1.0)\n",
    "axes[0].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "axes[0].set_title('Step Returns (Simple Returns)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Steps', fontsize=12)\n",
    "axes[0].set_ylabel('Return (%)', fontsize=12)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add statistics\n",
    "ret_mean = ret.mean() * 100\n",
    "ret_std = ret.std() * 100\n",
    "ret_pos = (ret > 0).sum()\n",
    "ret_neg = (ret < 0).sum()\n",
    "ret_stats = f'Mean: {ret_mean:.4f}%\\nStd: {ret_std:.4f}%\\nPositive: {ret_pos} ({ret_pos/len(ret)*100:.1f}%)\\nNegative: {ret_neg} ({ret_neg/len(ret)*100:.1f}%)'\n",
    "axes[0].text(0.02, 0.98, ret_stats, transform=axes[0].transAxes, \n",
    "             fontsize=10, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.7))\n",
    "\n",
    "# Plot 2: Trading Actions\n",
    "axes[1].plot(act_ret.index, act_ret.values, label='Action', drawstyle='steps-post', color='purple', linewidth=1.5)\n",
    "axes[1].axhline(y=0, color='gray', linestyle='--', alpha=0.5, label='Neutral (Cash)')\n",
    "axes[1].axhline(y=-1, color='red', linestyle=':', alpha=0.3, label='Max Short/Long')\n",
    "axes[1].axhline(y=1, color='green', linestyle=':', alpha=0.3, label='Max Long/Short')\n",
    "axes[1].fill_between(act_ret.index, act_ret.values, 0, alpha=0.3, color='purple')\n",
    "axes[1].set_ylim(-1.1, 1.1)\n",
    "axes[1].set_title('Trading Actions Over Time', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Steps', fontsize=12)\n",
    "axes[1].set_ylabel('Action Value', fontsize=12)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add action statistics\n",
    "action_stats = f'Mean: {act_ret.mean():.3f}\\nStd: {act_ret.std():.3f}\\nMin: {act_ret.min():.3f}\\nMax: {act_ret.max():.3f}'\n",
    "axes[1].text(0.02, 0.98, action_stats, transform=axes[1].transAxes, \n",
    "             fontsize=10, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
    "\n",
    "# Plot 3: Cumulative Returns (%)\n",
    "cumulative_ret = ((1 + ret).cumprod() - 1) * 100\n",
    "axes[2].plot(cumulative_ret.index, cumulative_ret.values, color='blue', linewidth=2, label='Cumulative Return')\n",
    "axes[2].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[2].fill_between(cumulative_ret.index, cumulative_ret.values, 0, alpha=0.3, color='blue')\n",
    "axes[2].set_title('Cumulative Returns Over Time', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Steps', fontsize=12)\n",
    "axes[2].set_ylabel('Cumulative Return (%)', fontsize=12)\n",
    "axes[2].legend(fontsize=10)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Rolling Statistics (20-period window)\n",
    "window_raw = len(ret) // 4  # Base window from data length\n",
    "window = max(1, min(20, window_raw))  # Ensure valid window size\n",
    "rolling_mean = (ret * 100).rolling(window=window).mean()\n",
    "rolling_std = (ret * 100).rolling(window=window).std()\n",
    "\n",
    "axes[3].plot(rolling_mean.index, rolling_mean.values, color='blue', linewidth=2, label=f'{window}-Period Rolling Mean')\n",
    "axes[3].fill_between(rolling_mean.index, \n",
    "                      rolling_mean - rolling_std, \n",
    "                      rolling_mean + rolling_std, \n",
    "                      alpha=0.3, color='blue', label='±1 Std Dev')\n",
    "axes[3].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[3].set_title(f'Rolling Return Statistics ({window}-Period Window)', fontsize=14, fontweight='bold')\n",
    "axes[3].set_xlabel('Steps', fontsize=12)\n",
    "axes[3].set_ylabel('Rolling Mean Return (%)', fontsize=12)\n",
    "axes[3].legend(fontsize=10)\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "# Print return statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RETURN STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nStep Returns (Simple Returns):\")\n",
    "print(f\"  Mean:              {ret.mean() * 100:.4f}%\")\n",
    "print(f\"  Median:            {ret.median() * 100:.4f}%\")\n",
    "print(f\"  Std Dev:           {ret.std() * 100:.4f}%\")\n",
    "print(f\"  Min:               {ret.min() * 100:.4f}%\")\n",
    "print(f\"  Max:               {ret.max() * 100:.4f}%\")\n",
    "print(f\"  Skewness:          {ret.skew():.4f}\")\n",
    "print(f\"  Kurtosis:          {ret.kurtosis():.4f}\")\n",
    "print(\"\\nReturn Distribution:\")\n",
    "print(f\"  Positive steps:    {(ret > 0).sum()} ({(ret > 0).sum()/len(ret)*100:.1f}%)\")\n",
    "print(f\"  Negative steps:    {(ret < 0).sum()} ({(ret < 0).sum()/len(ret)*100:.1f}%)\")\n",
    "print(f\"  Zero steps:        {(ret == 0).sum()} ({(ret == 0).sum()/len(ret)*100:.1f}%)\")\n",
    "print(\"\\nCumulative:\")\n",
    "print(f\"  Total Return:      {((1 + ret).prod() - 1) * 100:.2f}%\")\n",
    "print(f\"  Arithmetic Mean:   {ret.mean() * 100:.4f}% per step\")\n",
    "print(f\"  Geometric Mean:    {(((1 + ret).prod()) ** (1/len(ret)) - 1) * 100:.4f}% per step\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clean up\n",
    "test_env_returns.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a041d0",
   "metadata": {},
   "source": [
    "## Inference: Generate Trading Signals\n",
    "\n",
    "This cell contains a standalone inference function that can be copied to a separate Python script.\n",
    "The function takes a single observation (features for one timestep) and outputs a trading signal in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7a3293",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STANDALONE INFERENCE FUNCTION\n",
    "==============================\n",
    "This function can be copied to a separate Python script for production use.\n",
    "It takes features for a single timestep and generates a trading signal.\n",
    "\n",
    "Required imports for standalone use:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    from stable_baselines3 import PPO\n",
    "\n",
    "Feature structure:\n",
    "------------------\n",
    "Single asset features (per asset): 12 features\n",
    "    ['close', 'closeBias', 'closeEwmaVol', 'closeKalman', 'closeLowerShadow', \n",
    "     'closeMacd', 'closeRsi', 'closeSigns', 'closeStochRsi', 'closeUpperShadow', \n",
    "     'funding', 'fundingMinutesLeft']\n",
    "\n",
    "Pair features (for both assets combined): 8 features\n",
    "    ['alpha', 'beta', 'corr', 'pval', 'spreadNorm', 'spreadNormKalman', \n",
    "     'spreadNormMa', 'spreadNormVol']\n",
    "\"\"\"\n",
    "\n",
    "def continuous_action_to_weights(action: float) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Convert continuous action [-1, 1] to portfolio weights.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    action : float\n",
    "        Action value from -1 (short asset1/long asset2) to 1 (long asset1/short asset2)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple[float, float]\n",
    "        (asset1_weight, asset2_weight)\n",
    "    \"\"\"\n",
    "    action = np.clip(action, -1.0, 1.0)\n",
    "    position_size = action * 0.5\n",
    "    asset1_weight = position_size\n",
    "    asset2_weight = -position_size\n",
    "    return asset1_weight, asset2_weight\n",
    "\n",
    "\n",
    "def generate_signal_from_features(\n",
    "    model_path: str,\n",
    "    asset1: str,\n",
    "    asset2: str,\n",
    "    asset1_features: dict,\n",
    "    asset2_features: dict,\n",
    "    pair_features: dict,\n",
    "    current_portfolio_weights: tuple[float, float, float] = (0.0, 0.0, 1.0),\n",
    "    lookback_window: int = 24,\n",
    "    timestamp: str = None,\n",
    "    notional_usd: float = 100,\n",
    "    bar_timeframe: str = \"1h\",\n",
    "    exchange: str = \"binance_perpetual\",\n",
    "    fresh_for_seconds: int = 3600,\n",
    "    version: int = 1\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Generate a trading signal from features for a single timestep.\n",
    "    \n",
    "    This is a STANDALONE function that can be copied to a separate Python script.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_path : str\n",
    "        Path to the trained PPO model (.zip file)\n",
    "    asset1 : str\n",
    "        First asset symbol (e.g., \"ETH\")\n",
    "    asset2 : str\n",
    "        Second asset symbol (e.g., \"BTC\")\n",
    "    asset1_features : dict\n",
    "        Features for asset1 with keys:\n",
    "        ['close', 'closeBias', 'closeEwmaVol', 'closeKalman', 'closeLowerShadow',\n",
    "         'closeMacd', 'closeRsi', 'closeSigns', 'closeStochRsi', 'closeUpperShadow',\n",
    "         'funding', 'fundingMinutesLeft']\n",
    "        Each value should be a list/array of length lookback_window\n",
    "    asset2_features : dict\n",
    "        Features for asset2 (same structure as asset1_features)\n",
    "    pair_features : dict\n",
    "        Combined pair features with keys:\n",
    "        ['alpha', 'beta', 'corr', 'pval', 'spreadNorm', 'spreadNormKalman',\n",
    "         'spreadNormMa', 'spreadNormVol']\n",
    "        Each value should be a list/array of length lookback_window\n",
    "    current_portfolio_weights : tuple[float, float, float]\n",
    "        Current portfolio weights (asset1_weight, asset2_weight, cash_weight)\n",
    "        Default: (0.0, 0.0, 1.0) = 100% cash position\n",
    "    lookback_window : int\n",
    "        Number of historical timesteps (default: 24)\n",
    "    timestamp : str, optional\n",
    "        ISO format timestamp. If None, uses current time.\n",
    "    notional_usd : float\n",
    "        Notional value in USD\n",
    "    bar_timeframe : str\n",
    "        Timeframe of bars (e.g., \"1h\", \"1m\")\n",
    "    exchange : str\n",
    "        Exchange identifier\n",
    "    fresh_for_seconds : int\n",
    "        Signal validity duration in seconds\n",
    "    version : int\n",
    "        Signal format version\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Trading signal in JSON format\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> signal = generate_signal_from_features(\n",
    "    ...     model_path=\"./models/ppo_model.zip\",\n",
    "    ...     asset1=\"ETH\",\n",
    "    ...     asset2=\"BTC\",\n",
    "    ...     asset1_features={\n",
    "    ...         'close': [0.1, 0.2, ...],  # 24 values\n",
    "    ...         'closeBias': [0.05, -0.03, ...],\n",
    "    ...         # ... other 10 features\n",
    "    ...     },\n",
    "    ...     asset2_features={\n",
    "    ...         'close': [0.15, 0.18, ...],\n",
    "    ...         # ... other 11 features\n",
    "    ...     },\n",
    "    ...     pair_features={\n",
    "    ...         'alpha': [0.01, 0.02, ...],  # 24 values\n",
    "    ...         'beta': [0.98, 0.99, ...],\n",
    "    ...         # ... other 6 features\n",
    "    ...     },\n",
    "    ...     current_portfolio_weights=(0.3, -0.3, 0.4)  # Current position\n",
    "    ... )\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define feature order (must match training order)\n",
    "    SINGLE_ASSET_FEATURES = [\n",
    "        'close', 'closeBias', 'closeEwmaVol', 'closeKalman', 'closeLowerShadow',\n",
    "        'closeMacd', 'closeRsi', 'closeSigns', 'closeStochRsi', 'closeUpperShadow',\n",
    "        'funding', 'fundingMinutesLeft'\n",
    "    ]\n",
    "    \n",
    "    PAIR_FEATURES = [\n",
    "        'alpha', 'beta', 'corr', 'pval', 'spreadNorm', 'spreadNormKalman',\n",
    "        'spreadNormMa', 'spreadNormVol'\n",
    "    ]\n",
    "    \n",
    "    # Validate inputs\n",
    "    for feat in SINGLE_ASSET_FEATURES:\n",
    "        if feat not in asset1_features:\n",
    "            raise ValueError(f\"Missing feature '{feat}' in asset1_features\")\n",
    "        if feat not in asset2_features:\n",
    "            raise ValueError(f\"Missing feature '{feat}' in asset2_features\")\n",
    "        if len(asset1_features[feat]) != lookback_window:\n",
    "            raise ValueError(f\"asset1_features['{feat}'] must have length {lookback_window}\")\n",
    "        if len(asset2_features[feat]) != lookback_window:\n",
    "            raise ValueError(f\"asset2_features['{feat}'] must have length {lookback_window}\")\n",
    "    \n",
    "    # Validate pair features\n",
    "    for feat in PAIR_FEATURES:\n",
    "        if feat not in pair_features:\n",
    "            raise ValueError(f\"Missing feature '{feat}' in pair_features\")\n",
    "        if len(pair_features[feat]) != lookback_window:\n",
    "            raise ValueError(f\"pair_features['{feat}'] must have length {lookback_window}\")\n",
    "    \n",
    "    # Validate portfolio weights\n",
    "    if len(current_portfolio_weights) != 3:\n",
    "        raise ValueError(f\"current_portfolio_weights must have length 3, got {len(current_portfolio_weights)}\")\n",
    "    \n",
    "    # Build observation tensor\n",
    "    # Shape: (n_features * lookback_window + 3,)\n",
    "    # - Market features: (8 pair + 12 asset1 + 12 asset2) * lookback_window = 32 * 24 = 768 dimensions\n",
    "    # - Position features: 3 portfolio weights (asset1, asset2, cash) = 3 dimensions\n",
    "    # Total observation shape: (771,) with default lookback=24\n",
    "    \n",
    "    n_pair_features = len(PAIR_FEATURES)\n",
    "    n_asset_features = len(SINGLE_ASSET_FEATURES)\n",
    "    n_total_features = n_pair_features + 2 * n_asset_features  # pair + asset1 + asset2\n",
    "    \n",
    "    market_obs = np.zeros((n_total_features, lookback_window), dtype=np.float32)\n",
    "    \n",
    "    # Fill features in order: pair features, then asset1 features, then asset2 features\n",
    "    idx = 0\n",
    "    \n",
    "    # 1. Fill pair features\n",
    "    for feat in PAIR_FEATURES:\n",
    "        market_obs[idx, :] = pair_features[feat]\n",
    "        idx += 1\n",
    "    \n",
    "    # 2. Fill asset1 features\n",
    "    for feat in SINGLE_ASSET_FEATURES:\n",
    "        market_obs[idx, :] = asset1_features[feat]\n",
    "        idx += 1\n",
    "    \n",
    "    # 3. Fill asset2 features\n",
    "    for feat in SINGLE_ASSET_FEATURES:\n",
    "        market_obs[idx, :] = asset2_features[feat]\n",
    "        idx += 1\n",
    "    \n",
    "    # Flatten market features to (n_features * lookback_window,)\n",
    "    market_obs = market_obs.reshape(-1).astype(np.float32)\n",
    "    \n",
    "    # Add portfolio weights to observation\n",
    "    position_obs = np.array(current_portfolio_weights, dtype=np.float32)\n",
    "    \n",
    "    # Concatenate market features and position features\n",
    "    obs = np.concatenate([market_obs, position_obs])\n",
    "    \n",
    "    # Clip observation values (same as training)\n",
    "    obs = np.clip(obs, -5.0, 5.0)\n",
    "    \n",
    "    # Load model and predict\n",
    "    model = PPO.load(model_path)\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    \n",
    "    # Convert action to weights\n",
    "    action_value = float(action[0]) if isinstance(action, np.ndarray) else float(action)\n",
    "    asset1_weight, asset2_weight = continuous_action_to_weights(action_value)\n",
    "    \n",
    "    # Get or generate timestamp\n",
    "    if timestamp is None:\n",
    "        timestamp = pd.Timestamp.now(tz='UTC').strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    elif isinstance(timestamp, pd.Timestamp):\n",
    "        timestamp = timestamp.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    \n",
    "    # Create signal JSON\n",
    "    signal = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"bar_timeframe\": bar_timeframe,\n",
    "        \"pair\": {\n",
    "            \"asset1\": asset1,\n",
    "            \"asset2\": asset2\n",
    "        },\n",
    "        \"markets\": {\n",
    "            \"exchange\": exchange,\n",
    "            \"asset1\": f\"{asset1}-USDT\",\n",
    "            \"asset2\": f\"{asset2}-USDT\"\n",
    "        },\n",
    "        \"weights\": {\n",
    "            \"asset1\": float(asset1_weight),\n",
    "            \"asset2\": float(asset2_weight)\n",
    "        },\n",
    "        \"notional_usd\": notional_usd,\n",
    "        \"fresh_for_seconds\": fresh_for_seconds,\n",
    "        \"version\": version\n",
    "    }\n",
    "    \n",
    "    return signal\n",
    "print(\"  - Features for a single timestep:\")\n",
    "print(\"    * pair_features: 8 features x lookback_window values\")\n",
    "print(\"    * current_portfolio_weights: (asset1, asset2, cash) tuple\")\n",
    "print(\"  - Note: asset1_features and asset2_features parameters exist for\")\n",
    "print(\"    API compatibility but are not currently used by the model\")\n",
    "print(\"\\nExample below demonstrates usage with TEST data (unseen data)...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nExample below demonstrates usage with TEST data (unseen data)...\")\n",
    "print(\"    * pair_features: 8 features x lookback_window values\")\n",
    "print(\"    * asset2_features: 12 features x lookback_window values\")\n",
    "print(\"    * asset1_features: 12 features x lookback_window values\")\n",
    "print(\"  - Features for a single timestep:\")\n",
    "print(\"    * asset1_features: 12 features x lookback_window values\")\n",
    "print(\"    * asset2_features: 12 features x lookback_window values\")\n",
    "print(\"    * pair_features: 8 features x lookback_window values\")\n",
    "print(\"\\nExample below demonstrates usage with TEST data (unseen data)...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b3adf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Test the standalone function with test data\n",
    "print(\"\\nRunning demo with test data...\\n\")\n",
    "\n",
    "# Load the trained model\n",
    "best_split = SPLITS[0]\n",
    "#model_path = os.path.join(CONFIG[\"IO\"][\"models_dir\"], f\"ppo_{best_split['name']}.zip\")\n",
    "model_path = os.path.join(CONFIG[\"IO\"][\"models_dir\"], \"final_model.zip\")\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    try:\n",
    "        # Get a sample from test data\n",
    "        test_env = make_env_from_mask(best_split[\"test\"], name=\"test_inference\")\n",
    "        unwrapped_env = test_env.unwrapped if hasattr(test_env, 'unwrapped') else test_env\n",
    "        \n",
    "        # Get the first pair\n",
    "        sample_pair = all_pairs[0]\n",
    "        asset1, asset2 = sample_pair\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error setting up test environment: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    # Extract features from the first timestep in test set\n",
    "    # Shape of X: (samples, 1, n_features, lookback)\n",
    "    sample_idx = 0\n",
    "    sample_features = unwrapped_env.X[sample_idx, 0, :, :]  # (n_features, lookback)\n",
    "    \n",
    "    # For demo, we'll construct the feature dictionaries\n",
    "    # Note: In production, you'd receive these features from your data pipeline\n",
    "    lookback = CONFIG[\"ENV\"][\"lookback_window\"]\n",
    "    \n",
    "    # Define feature names (must match training order)\n",
    "    PAIR_FEATURES = [\n",
    "        'alpha', 'beta', 'corr', 'pval', 'spreadNorm', 'spreadNormKalman',\n",
    "        'spreadNormMa', 'spreadNormVol'\n",
    "    ]\n",
    "    \n",
    "    SINGLE_ASSET_FEATURES = [\n",
    "        'close', 'closeBias', 'closeEwmaVol', 'closeKalman', 'closeLowerShadow',\n",
    "        'closeMacd', 'closeRsi', 'closeSigns', 'closeStochRsi', 'closeUpperShadow',\n",
    "        'funding', 'fundingMinutesLeft'\n",
    "    ]\n",
    "    \n",
    "    # Extract features from sample in the correct order: pair, asset1, asset2\n",
    "    n_pair_features = len(PAIR_FEATURES)\n",
    "    n_asset_features = len(SINGLE_ASSET_FEATURES)\n",
    "    \n",
    "    # Build pair_features dict (first 8 features)\n",
    "    pair_features = {}\n",
    "    for i, feat_name in enumerate(PAIR_FEATURES):\n",
    "        if i < sample_features.shape[0]:\n",
    "            pair_features[feat_name] = sample_features[i, :].tolist()\n",
    "    \n",
    "    # Build asset1_features dict (next 12 features)\n",
    "    asset1_features = {}\n",
    "    for i, feat_name in enumerate(SINGLE_ASSET_FEATURES):\n",
    "        feat_idx = n_pair_features + i\n",
    "        if feat_idx < sample_features.shape[0]:\n",
    "            asset1_features[feat_name] = sample_features[feat_idx, :].tolist()\n",
    "        else:\n",
    "            asset1_features[feat_name] = [0.0] * lookback\n",
    "    \n",
    "    # Build asset2_features dict (last 12 features)\n",
    "    asset2_features = {}\n",
    "    for i, feat_name in enumerate(SINGLE_ASSET_FEATURES):\n",
    "        feat_idx = n_pair_features + n_asset_features + i\n",
    "        if feat_idx < sample_features.shape[0]:\n",
    "            asset2_features[feat_name] = sample_features[feat_idx, :].tolist()\n",
    "        else:\n",
    "            asset2_features[feat_name] = [0.0] * lookback\n",
    "    \n",
    "    # Get timestamp from test set\n",
    "    test_timestamps = SAMPLE_TIMESTAMPS[SAMPLE_TIMESTAMPS >= pd.Timestamp(CONFIG[\"SPLITS\"][\"test\"][0], tz='UTC')]\n",
    "    if len(test_timestamps) > 0:\n",
    "        sample_timestamp = test_timestamps[0]\n",
    "    else:\n",
    "        sample_timestamp = None\n",
    "    \n",
    "    # Generate signal using standalone function\n",
    "    # Start with a neutral position (100% cash) for the first prediction\n",
    "    current_weights = (0.0, 0.0, 1.0)  # (asset1, asset2, cash)\n",
    "    \n",
    "    try:\n",
    "        signal = generate_signal_from_features(\n",
    "            model_path=model_path,\n",
    "            asset1=asset1,\n",
    "            asset2=asset2,\n",
    "            asset1_features=asset1_features,\n",
    "            asset2_features=asset2_features,\n",
    "            pair_features=pair_features,\n",
    "            current_portfolio_weights=current_weights,\n",
    "            lookback_window=lookback,\n",
    "            timestamp=sample_timestamp,\n",
    "            notional_usd=100,\n",
    "            bar_timeframe=\"1h\",\n",
    "            exchange=\"binance_perpetual\",\n",
    "            fresh_for_seconds=3600,\n",
    "            version=1\n",
    "        )\n",
    "        \n",
    "        print(\"✓ Successfully generated signal using standalone function!\")\n",
    "        print(\"✓ Inference performed on TEST SET (unseen data)\\n\")\n",
    "        print(\"Sample output:\")\n",
    "        print(json.dumps(signal, indent=2))\n",
    "        \n",
    "        # Save as example\n",
    "        example_file = os.path.join(CONFIG[\"EVAL\"][\"reports_dir\"], \"signal_example_test.json\")\n",
    "        with open(example_file, \"w\") as f:\n",
    "            json.dump(signal, f, indent=2)\n",
    "        print(f\"\\n✓ Saved example signal to: {example_file}\")\n",
    "        print(f\"✓ Test period: {CONFIG['SPLITS']['test'][0]} to {CONFIG['SPLITS']['test'][1]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error generating signal: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        # Clean up test environment\n",
    "        if 'test_env' in locals():\n",
    "            test_env.close()\n",
    "    \n",
    "else:\n",
    "    print(f\"✗ Model not found at {model_path}\")\n",
    "    print(\"Please run the training cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e43690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostics: verify action sign mapping vs realized returns\n",
    "print(\"\\nVerifying action sign mapping...\")\n",
    "try:\n",
    "    # If available from earlier cell, reuse log_ret and act_ret; else recompute quickly\n",
    "    if 'log_ret' in globals() and isinstance(log_ret, pd.Series) and 'act_ret' in globals():\n",
    "        ret_diag = np.expm1(log_ret).astype(float)\n",
    "        act_diag = act_ret.astype(float)\n",
    "    else:\n",
    "        _env_diag = make_env_from_mask(best_split[\"test\"], name=\"diag_returns\")\n",
    "        _, _, _, act_diag, log_ret_diag = backtest_env(_env_diag, model=model, include_returns=True)\n",
    "        ret_diag = np.expm1(log_ret_diag).astype(float)\n",
    "        _env_diag.close()\n",
    "    # Align indexes\n",
    "    idx = ret_diag.index.intersection(act_diag.index)\n",
    "    ret_diag = ret_diag.loc[idx]\n",
    "    act_diag = act_diag.loc[idx]\n",
    "    corr = act_diag.corr(ret_diag)\n",
    "    print(f\"Correlation(action, realized return): {corr:.4f}\")\n",
    "    if np.isnan(corr):\n",
    "        print(\"Could not compute correlation (NaN). Check data alignment.\")\n",
    "    elif corr < 0:\n",
    "        print(\"Potential sign mismatch: positive action correlates with negative returns.\")\n",
    "        print(\"Consider flipping the action sign in mapping (long-short convention) or pair ordering.\")\n",
    "    else:\n",
    "        print(\"Sign mapping looks consistent: positive action aligns with positive returns.\")\n",
    "except Exception as e:\n",
    "    print(\"Sign verification failed:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
