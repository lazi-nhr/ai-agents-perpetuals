{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1bb7b54",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Agent for Statistical Arbitrage\n",
    "\n",
    "This notebook demonstrates how to configure and train a reinforcement learning agent for statistical arbitrage using a snapshot model. The agent will learn to identify and exploit statistical arbitrage opportunities in financial markets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e79612d",
   "metadata": {},
   "source": [
    "## Install necessary libraries\n",
    "\n",
    "The following libraries are required for this notebook. If you haven't installed them yet, you can do so using running the cell below or by using pip install in your terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b434b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install -U numpy pandas pyarrow gdown gymnasium stable-baselines3 torch matplotlib tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4664e80a",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "The configuration section sets up the parameters for the reinforcement learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f34fbc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"DATA\": {\n",
    "        \"drive_folder_id\": \"1uXEBUyySypdsW_ZqL-RZ3d1bWdIZisij\", # google drive folder ID (can be found in the URL)\n",
    "        \"structure\": {\"1d\":\"ohlcv_1d\", # subfolder structure inside the drive folder\n",
    "                      \"1h\":\"ohlcv_1h\",\n",
    "                      \"15m\":\"ohlcv_15m\",\n",
    "                      \"5m\":\"ohlcv_5m\",\n",
    "                      \"1m\":\"ohlcv_1m\"},\n",
    "        \"file_pattern\": \"{TICKER}_{FREQ}.parquet\", # file naming pattern\n",
    "        \"tickers\": [\"BTC\",\"ETH\"], # pairs of assets to trade\n",
    "        \"sampling\": \"1h\", # data sampling frequency (1m, 5m, 15m, 1h, 1d)\n",
    "        \"price_point\": \"close\", # what price point to use for returns calculation (open, high, low, close, etc)\n",
    "        \"forward_fill\": True, # forward fill missing data\n",
    "        \"drop_na_after_ffill\": True, # drop rows with NA values after forward filling\n",
    "        \"cache_dir\": \"./data_cache\", # local cache directory to store downloaded files\n",
    "        \"file_ids\": {\"BTC_1h\": \"1-sBNQpEFGEpVO3GDFCkSZiV3Iaqp2vB_\", # map and define files to download (all data will be downloaded if left empty)\n",
    "                     \"ETH_1h\": \"1kj8G1scpFuEYTTXKEUzF9pwgGI2WFFL9\"\n",
    "                     }\n",
    "    },\n",
    "    \"COINTEGRATION\": {\n",
    "        \"drive_folder_id\": \"1V55Wx2MT-g6rQEo-ZEJIOSA-65zxLJYw\", # google drive folder for cointegration files\n",
    "        \"file_pattern\": \"{ASSET1}_{ASSET2}_window_cointegration.csv\", # cointegration file pattern\n",
    "        \"timestamp_format\": \"%Y-%m-%d %H:%M:%S\", # format of timestamps in cointegration files\n",
    "        \"training_window_days\": 2, # number of days to use for training after cointegration period\n",
    "        \"cache_dir\": \"./cointegration_cache\" # cache directory for cointegration files\n",
    "    },\n",
    "    \"ENV\": {\n",
    "        \"include_cash\": True, # include cash as a third asset\n",
    "        \"shorting\": True, # allow shorting\n",
    "        \"lookback_window\": 64, # lookback for feature slicing, meaning that the observation for each step will include data from the last 'lookback_window' time steps\n",
    "        \"features\": {\n",
    "            \"vol_window\": 64, # rolling volatility window\n",
    "            \"rsi_period\": 14, # RSI lookback period\n",
    "            \"volume_change\": True, # include volume change as a feature\n",
    "            \"normalize\": True # normalize features using rolling z-score\n",
    "        },\n",
    "        \"transaction_costs\": { # transaction costs settings for exchange (e.g. Hyperliquid)\n",
    "            \"commission_bps\": 5.0, # commission in basis points (bps)\n",
    "            \"slippage_bps\": 5.0, # slippage in basis points (bps)\n",
    "        },\n",
    "        \"reward\": {\n",
    "            \"risk_lambda\":0.001 # risk penalty coefficient (lambda), a.k.a. risk aversion factor\n",
    "        },\n",
    "        \"leverage\": {\n",
    "            \"long_cap\": 2.0,  # maximum leverage for long positions\n",
    "            \"short_cap\": 2.0, # maximum leverage for short positions\n",
    "            \"use_asymmetric\": True, # whether to allow different caps for long and short positions\n",
    "        },\n",
    "        \"constraints\": {\n",
    "            \"min_weight\": -2.0, # minimum weight for each asset (for shorting)\n",
    "            \"max_weight\": 2.0,  # maximum weight for each asset (for leverage)\n",
    "            \"sum_to_one\": False # whether weights must sum to one (need to be False for leverage)\n",
    "        },\n",
    "        \"seed\": 42 # random seed for reproducibility\n",
    "    },\n",
    "    \"SPLITS\": { # date splits for training, validation, and testing\n",
    "        \"data_start\":\"2024-09-02\", # start date of the entire dataset\n",
    "        \"data_end\":\"2025-09-02\", # end date of the entire dataset\n",
    "        \"train\":[\"2024-09-02\",\"2025-06-15\"], # training period\n",
    "        \"val\":[\"2025-06-16\",\"2025-07-15\"], # validation period\n",
    "        \"test\":[\"2025-07-16\",\"2025-09-02\"], # testing period\n",
    "        \"walk_forward\": True, # whether to use walk-forward splits (e.g. sliding window)\n",
    "        \"wf_train_span_days\": 180, # training window span in days for walk-forward\n",
    "        \"wf_test_span_days\": 30, # testing window span in days for walk-forward\n",
    "        \"wf_step_days\": 30 # step size in days to move the window forward\n",
    "    },\n",
    "    \"RL\": {\n",
    "        \"timesteps\":10,\n",
    "        \"policy\":\"MlpPolicy\",\n",
    "        \"gamma\":0.99,\n",
    "        \"gae_lambda\":0.95,\n",
    "        \"clip_range\":0.2,\n",
    "        \"n_steps\":1024,\n",
    "        \"batch_size\":256,\n",
    "        \"learning_rate\":3e-4,\n",
    "        \"ent_coef\":0.0,\n",
    "        \"vf_coef\":0.5,\n",
    "        \"max_grad_norm\":0.5\n",
    "    },\n",
    "    \"EVAL\": {\n",
    "        \"plots\":True,\n",
    "        \"reports_dir\":\"./reports\"\n",
    "    },\n",
    "    \"IO\": {\n",
    "        \"models_dir\":\"./models\",\n",
    "        \"tb_logdir\":\"./tb\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568820c4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4143bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gdown\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import pytz\n",
    "\n",
    "import time\n",
    "\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reinforcement Learning\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# Deep Learning\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f12ee23",
   "metadata": {},
   "source": [
    "## Set Computation Device\n",
    "\n",
    "This section sets the computation device for training the model. It checks if a GPU is available and sets it as the device; otherwise, it defaults to CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50bc4ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available. Using Apple Silicon GPU.\n"
     ]
    }
   ],
   "source": [
    "# run on cuda GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"CUDA is available. Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# run on Apple Silicon\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS is available. Using Apple Silicon GPU.\")\n",
    "\n",
    "# run on CPU (slow)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA and MPS are not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63981a61",
   "metadata": {},
   "source": [
    "## Set seeds for reproducibility\n",
    "\n",
    "This section sets the random seeds for various libraries to ensure that the results are reproducible.\n",
    "\n",
    "Note: It is good practice to type (set data types) for function and method parameters for better code maintainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "775695a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed: int = 42):\n",
    "    random.seed(seed) # seed for random module\n",
    "    np.random.seed(seed) # seed for numpy module\n",
    "    try:\n",
    "        torch.manual_seed(seed) # seed for torch module\n",
    "        if torch.cuda.is_available(): # seed for CUDA device\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        elif torch.backends.mps.is_available(): # seed for Apple Silicon device\n",
    "            torch.backends.mps.manual_seed_all(seed)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "set_all_seeds(CONFIG[\"ENV\"][\"seed\"]) # set all seeds for reproducibility\n",
    "\n",
    "# set annualization factors for different timeframes\n",
    "ANNUALIZATION = {\"1m\":365*24*60,\n",
    "                 \"5m\":365*24*12,\n",
    "                 \"15m\":365*24*4,\n",
    "                 \"1h\":365*24,\n",
    "                 \"1d\":365}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b556fc3",
   "metadata": {},
   "source": [
    "## Fetch Data from Google Drive\n",
    "\n",
    "This section handles downloading data from Google Drive. It supports two methods: downloading an entire folder or downloading specific files by their IDs. The data will be cached locally for faster subsequent loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1e7db74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File BTC_1h already exists in cache. Skipping download.\n",
      "File ETH_1h already exists in cache. Skipping download.\n",
      "Download step complete.\n"
     ]
    }
   ],
   "source": [
    "ROOT_ID = CONFIG[\"DATA\"][\"drive_folder_id\"] # suffix for Google Drive download URL\n",
    "CACHE_DIR = CONFIG[\"DATA\"][\"cache_dir\"] # data will be cached here for faster subsequent loading\n",
    "FILE_IDS = CONFIG[\"DATA\"][\"file_ids\"] # mapping of data to its URL suffix\n",
    "\n",
    "download_all = True if FILE_IDS == {} else False # this determines whether to download entire folder or specific files by their IDs (s. CONFIG)\n",
    "\n",
    "# create directory if it doesn't already exist\n",
    "def ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# download entire folder from Google Drive with retry logic\n",
    "def download_drive_folder(root_id: str, out_dir: str, max_retries: int = 3):\n",
    "    print(\"Mirroring Google Drive folder locally...\")\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            gdown.download_folder(\n",
    "                id=root_id, \n",
    "                output=out_dir, \n",
    "                quiet=False, \n",
    "                use_cookies=False,\n",
    "                remaining_ok=True  # Continue even if some files fail\n",
    "            )\n",
    "            print(\"Folder mirroring complete.\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Download attempt {attempt + 1} failed. Retrying... Error: {str(e)}\")\n",
    "                time.sleep(5)  # Wait 5 seconds before retrying\n",
    "            else:\n",
    "                print(f\"All download attempts failed for folder {root_id}. Error: {str(e)}\")\n",
    "                return False\n",
    "\n",
    "# download specific files from Google Drive by their file IDs with retry logic\n",
    "def targeted_download_by_ids(file_id_map: Dict[str, str], out_dir: str, max_retries: int = 3):\n",
    "    ensure_dir(out_dir)\n",
    "    \n",
    "    for name, fid in file_id_map.items():\n",
    "        # check if file already exists in local cache folder\n",
    "        if os.path.exists(os.path.join(out_dir, name)) or os.path.exists(os.path.join(out_dir, f\"{name}.parquet\")):\n",
    "            print(f\"File {name} already exists in cache. Skipping download.\")\n",
    "            continue\n",
    "        \n",
    "        suffix = name if name.endswith(\".parquet\") else f\"{name}.parquet\" # ensure .parquet suffix\n",
    "        out_path = os.path.join(out_dir, suffix) # full output path\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                print(f\"Downloading {name} -> {out_path}\")\n",
    "                url = f\"https://drive.google.com/uc?id={fid}\"\n",
    "                success = gdown.download(url, out_path, quiet=False, use_cookies=False, verify=False)\n",
    "                if success:\n",
    "                    break\n",
    "                else:\n",
    "                    raise Exception(\"Download failed\")\n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"Download attempt {attempt + 1} failed for {name}. Retrying... Error: {str(e)}\")\n",
    "                    time.sleep(5)  # Wait 5 seconds before retrying\n",
    "                else:\n",
    "                    print(f\"All download attempts failed for {name}. Error: {str(e)}\")\n",
    "\n",
    "# check if cache directory exists, if not create it\n",
    "ensure_dir(CACHE_DIR)\n",
    "\n",
    "if download_all: # download entire folder\n",
    "    download_drive_folder(ROOT_ID, CACHE_DIR)\n",
    "else: # download specific files by their IDs\n",
    "    targeted_download_by_ids(FILE_IDS, CACHE_DIR)\n",
    "\n",
    "print(\"Download step complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b151dd54",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Once the data is downloaded, this section loads the data into a pandas DataFrame for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d0762b",
   "metadata": {},
   "source": [
    "## Load Cointegration Data\n",
    "\n",
    "This section loads the cointegration test results from CSV files and processes them to identify valid training periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0d50a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading cointegration data...\n",
      "Mirroring Google Drive folder locally...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1YZJXQ2fZNyP5xO9U2NTkyBnEexLB9odN ADA_DOGE_window_cointegration.csv\n",
      "Processing file 136YCiC1IscZpZn6hoOf3US7tGet4p1Aj ADA_HBAR_window_cointegration.csv\n",
      "Processing file 18iktTkGXlTNkCaZ_-C3kAVzPTjQv-quE ADA_LINK_window_cointegration.csv\n",
      "Processing file 1H2b8ZIBK5jjFUgQ8KT0yPBfsQoRRuF8p ADA_LTC_window_cointegration.csv\n",
      "Processing file 1UGyabqR_vBbVZ0PcSBmGy8dftKdMQBVt ADA_SUI_window_cointegration.csv\n",
      "Processing file 1DZtQzzXeIkxlCNc4_QLuIDXeth3cGpxi ADA_XLM_window_cointegration.csv\n",
      "Processing file 11urUO0yJzjDxPYAd0YpYiCugHlQA70Ft ADA_XRP_window_cointegration.csv\n",
      "Processing file 1OurOhnonlKXtpDSi1keAuARoeXy9vEXw APT_ARB_window_cointegration.csv\n",
      "Processing file 1EHHIl_07HpbzMYkYMMeVzl9R5xBR-usN APT_AVAX_window_cointegration.csv\n",
      "Processing file 1BWe7E8-IPROlxsuQCqxsgisQv0O5zIhF APT_NEAR_window_cointegration.csv\n",
      "Processing file 1JMjHuSVv7sHzi5cbgZHbkRL_0gKOlnQ- APT_TON_window_cointegration.csv\n",
      "Processing file 1def47NyM3EZJHavN1ip7NrF2JVxTDPJs APT_WLD_window_cointegration.csv\n",
      "Processing file 1fSQohRy1xGlz9fk4KITweKTToaqvdFu3 ARB_AVAX_window_cointegration.csv\n",
      "Processing file 1tCjlHG7rwGBQtqm9bC8_-DlH4BMfGRVb ARB_DOT_window_cointegration.csv\n",
      "Processing file 1OvkkFsp-ruq1uCsAVF2faO2XxfF15-5q ARB_ETC_window_cointegration.csv\n",
      "Processing file 1EVCCHRbjqvO9RzrUjIpqJ1kTSEI58mWa ARB_NEAR_window_cointegration.csv\n",
      "Processing file 1TgWDAjDX7_vg3BOTDqFFG4nGyO1Hfsz0 ARB_TON_window_cointegration.csv\n",
      "Processing file 1OSZiGMxHuzU_Ezw92FaWwwOPsmMFpTTo ARB_WLD_window_cointegration.csv\n",
      "Processing file 1onpgBna92Tbv8hAEwJnUbfYL7OIV-CxB ATOM_AVAX_window_cointegration.csv\n",
      "Processing file 1Rz7lh0AFoH8uLfQNFM6a7leCb1JRAMM0 ATOM_DOT_window_cointegration.csv\n",
      "Processing file 1pewNxlkYZvFDYfP2vUMtTUbSQ9V_SuG- ATOM_ETC_window_cointegration.csv\n",
      "Processing file 1rYyP860K-bbgBH_5pbZeEUgkw9UtnQsK AVAX_DOT_window_cointegration.csv\n",
      "Processing file 1Qak64F49G9sOWAzZoQN05W4RX-7Iww_G AVAX_ETC_window_cointegration.csv\n",
      "Processing file 1uUvu5mhosYMasYJL1XjwyRWKQOIs67hN AVAX_NEAR_window_cointegration.csv\n",
      "Processing file 14O86VW7rybJThienFM6NGrJMRj4W_2nL AVAX_WLD_window_cointegration.csv\n",
      "Processing file 1WEuUGL5FaMRUtOf_dyC-X08pPujtUVcz BTC_SUI_window_cointegration.csv\n",
      "Processing file 1ACB7qa9A6mN1ZKpxs1h0O1YHdIrFHQLN BTC_TRX_window_cointegration.csv\n",
      "Processing file 1WsoE1hbKopFiMsILoq-ixROlnsHvHsjW BTC_XLM_window_cointegration.csv\n",
      "Processing file 1_8gbXlI0tNlQr4_WZDldO28XIJ76iXBA BTC_XRP_window_cointegration.csv\n",
      "Processing file 1nGTkBS19T-HlgREKBMU6iS4bJ6lR94Yi DOGE_ENA_window_cointegration.csv\n",
      "Processing file 1Xf-y_ItRmJR06XcszTryCAwpSi72snBB DOT_ETC_window_cointegration.csv\n",
      "Processing file 1WeRdaqnim5rWKMcdv7OHp0PSuEpxv0Kc ENA_ETC_window_cointegration.csv\n",
      "Processing file 1pG6CbD6Eto1tBZtoik1J8bcqnBZD1dTi ENA_LINK_window_cointegration.csv\n",
      "Processing file 1KScE0GK_cJMQD7Y-ekgexuf8kFZvTWSi ENA_UNI_window_cointegration.csv\n",
      "Processing file 19oNSISYqitYVpmix9fHJljP8Ob_kFK3C ETC_SOL_window_cointegration.csv\n",
      "Processing file 1Ynx4DcDwyxjTkFbyXddIUiNA54FxwXER ETC_UNI_window_cointegration.csv\n",
      "Processing file 1Tw-FL25wKvTUDPoLdNiGBWIh41-42kun HBAR_LTC_window_cointegration.csv\n",
      "Processing file 1c9zaYGFE3IWTetHXcl4vrYsOEyuC14H7 HBAR_XLM_window_cointegration.csv\n",
      "Processing file 1QUnPeCmKVbG8SsZc0wuQAc441WCFGVpn HBAR_XRP_window_cointegration.csv\n",
      "Processing file 1AWXfCp2egL4d9D1lntAG6cKlzJJ-62c4 historical_pairs_with_spreads.csv\n",
      "Processing file 1a7N87WZvssER23TMvA9ki1812nagpLl9 LINK_LTC_window_cointegration.csv\n",
      "Processing file 1spxkVxFyMi6yLAz7DIYVJnI0uQxk4uga LTC_XLM_window_cointegration.csv\n",
      "Processing file 19yJsB-jMLQvL7Vbd86flQauOq7aQEc1T NEAR_TON_window_cointegration.csv\n",
      "Processing file 1wMeomTAJyXKJeAGQhXUrzjecCdDF8IrB NEAR_WLD_window_cointegration.csv\n",
      "Processing file 1tz4_faFuIQH1XMoU_GawDGo3R08DBhFF TON_WLD_window_cointegration.csv\n",
      "Processing file 14hHs3gH-XbVggsiN9tubt2gQN-tnNpRW TRX_XRP_window_cointegration.csv\n",
      "Processing file 17ssL2NhY_q50TVrCqrWqrBWu2KHQdIMn XLM_XRP_window_cointegration.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download attempt 1 failed. Retrying... Error: Failed to retrieve file url:\n",
      "\n",
      "\tCannot retrieve the public link of the file. You may need to change\n",
      "\tthe permission to 'Anyone with the link', or have had many accesses.\n",
      "\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n",
      "\n",
      "You may still be able to access the file from the browser:\n",
      "\n",
      "\thttps://drive.google.com/uc?id=1YZJXQ2fZNyP5xO9U2NTkyBnEexLB9odN\n",
      "\n",
      "but Gdown can't. Please check connections and permissions.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileURLRetrievalError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/gdown/download.py:267\u001b[39m, in \u001b[36mdownload\u001b[39m\u001b[34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format, user_agent, log_messages)\u001b[39m\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m     url = \u001b[43mget_url_from_gdrive_confirmation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m FileURLRetrievalError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/gdown/download.py:55\u001b[39m, in \u001b[36mget_url_from_gdrive_confirmation\u001b[39m\u001b[34m(contents)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m url:\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m FileURLRetrievalError(\n\u001b[32m     56\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot retrieve the public link of the file. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     57\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou may need to change the permission to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     58\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mAnyone with the link\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, or have had many accesses. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     59\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     60\u001b[39m     )\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m url\n",
      "\u001b[31mFileURLRetrievalError\u001b[39m: Cannot retrieve the public link of the file. You may need to change the permission to 'Anyone with the link', or have had many accesses. Check FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mFileURLRetrievalError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mdownload_drive_folder\u001b[39m\u001b[34m(root_id, out_dir, max_retries)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[43mgdown\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_folder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mroot_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cookies\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mremaining_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Continue even if some files fail\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFolder mirroring complete.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/gdown/download_folder.py:325\u001b[39m, in \u001b[36mdownload_folder\u001b[39m\u001b[34m(url, id, output, quiet, proxy, speed, use_cookies, remaining_ok, verify, user_agent, skip_download, resume)\u001b[39m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m local_path = \u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://drive.google.com/uc?id=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspeed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cookies\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m local_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/gdown/download.py:278\u001b[39m, in \u001b[36mdownload\u001b[39m\u001b[34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format, user_agent, log_messages)\u001b[39m\n\u001b[32m    269\u001b[39m         message = (\n\u001b[32m    270\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFailed to retrieve file url:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    271\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou may still be able to access the file from the browser:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    276\u001b[39m             url_origin,\n\u001b[32m    277\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m FileURLRetrievalError(message)\n\u001b[32m    280\u001b[39m filename_from_url = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mFileURLRetrievalError\u001b[39m: Failed to retrieve file url:\n\n\tCannot retrieve the public link of the file. You may need to change\n\tthe permission to 'Anyone with the link', or have had many accesses.\n\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n\nYou may still be able to access the file from the browser:\n\n\thttps://drive.google.com/uc?id=1YZJXQ2fZNyP5xO9U2NTkyBnEexLB9odN\n\nbut Gdown can't. Please check connections and permissions.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Download cointegration folder with improved error handling\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDownloading cointegration data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m success = \u001b[43mdownload_drive_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCOINT_ROOT_ID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCOINT_CACHE_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m success:\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mWarning: Some cointegration files may be missing. Continuing with available data.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mdownload_drive_folder\u001b[39m\u001b[34m(root_id, out_dir, max_retries)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attempt < max_retries - \u001b[32m1\u001b[39m:\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownload attempt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattempt\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m failed. Retrying... Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[43mtime\u001b[49m.sleep(\u001b[32m5\u001b[39m)  \u001b[38;5;66;03m# Wait 5 seconds before retrying\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAll download attempts failed for folder \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroot_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "# Download cointegration data\n",
    "COINT_ROOT_ID = CONFIG[\"COINTEGRATION\"][\"drive_folder_id\"]\n",
    "COINT_CACHE_DIR = CONFIG[\"COINTEGRATION\"][\"cache_dir\"]\n",
    "ensure_dir(COINT_CACHE_DIR)\n",
    "\n",
    "# Download cointegration folder with improved error handling\n",
    "print(\"Downloading cointegration data...\")\n",
    "success = download_drive_folder(COINT_ROOT_ID, COINT_CACHE_DIR)\n",
    "\n",
    "if not success:\n",
    "    print(\"Warning: Some cointegration files may be missing. Continuing with available data.\")\n",
    "\n",
    "def get_all_assets_from_cointegration():\n",
    "    \"\"\"Scan cointegration files to get all unique assets.\"\"\"\n",
    "    assets = set()\n",
    "    pattern = os.path.join(COINT_CACHE_DIR, \"**\", \"*_window_cointegration.csv\")\n",
    "    coint_files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    for file_path in coint_files:\n",
    "        base_name = os.path.basename(file_path)\n",
    "        pair_assets = base_name.replace(\"_window_cointegration.csv\", \"\").split(\"_\")[:2]\n",
    "        assets.update(pair_assets)\n",
    "    \n",
    "    return sorted(list(assets))\n",
    "\n",
    "def load_cointegration_data():\n",
    "    \"\"\"Load all cointegration test results and return a DataFrame with training windows.\"\"\"\n",
    "    all_periods = []\n",
    "    \n",
    "    # Find all cointegration CSV files\n",
    "    pattern = os.path.join(COINT_CACHE_DIR, \"**\", \"*_window_cointegration.csv\")\n",
    "    coint_files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    if not coint_files:\n",
    "        print(\"Warning: No cointegration files found in cache directory.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    for file_path in coint_files:\n",
    "        try:\n",
    "            # Extract asset names from filename\n",
    "            base_name = os.path.basename(file_path)\n",
    "            assets = base_name.replace(\"_window_cointegration.csv\", \"\").split(\"_\")[:2]\n",
    "            \n",
    "            # Read cointegration results\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['start'] = pd.to_datetime(df['start'], format=CONFIG[\"COINTEGRATION\"][\"timestamp_format\"])\n",
    "            df['end'] = pd.to_datetime(df['end'], format=CONFIG[\"COINTEGRATION\"][\"timestamp_format\"])\n",
    "            \n",
    "            # Filter for cointegrated periods\n",
    "            coint_periods = df[df['cointegrated']]\n",
    "            \n",
    "            for _, row in coint_periods.iterrows():\n",
    "                # Calculate training period (2 days after cointegration period)\n",
    "                train_start = row['end']\n",
    "                train_end = train_start + pd.Timedelta(days=CONFIG[\"COINTEGRATION\"][\"training_window_days\"])\n",
    "                \n",
    "                all_periods.append({\n",
    "                    'asset1': assets[0],\n",
    "                    'asset2': assets[1],\n",
    "                    'coint_start': row['start'],\n",
    "                    'coint_end': row['end'],\n",
    "                    'train_start': train_start,\n",
    "                    'train_end': train_end,\n",
    "                    'alpha': row['alpha'],\n",
    "                    'beta': row['beta'],\n",
    "                    'adf_p': row['adf_p'],\n",
    "                    'correlation': row['correlation']\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(all_periods)\n",
    "\n",
    "# Get all unique assets from cointegration files\n",
    "all_assets = get_all_assets_from_cointegration()\n",
    "print(f\"Found {len(all_assets)} unique assets in cointegration files: {all_assets}\")\n",
    "\n",
    "# Update CONFIG with discovered assets\n",
    "CONFIG[\"DATA\"][\"tickers\"] = all_assets\n",
    "\n",
    "# Load cointegration data\n",
    "coint_df = load_cointegration_data()\n",
    "print(f\"Loaded {len(coint_df)} cointegration periods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1c9541d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BTC': (8785, 5), 'ETH': (8785, 5)}\n"
     ]
    }
   ],
   "source": [
    "sampling = CONFIG[\"DATA\"][\"sampling\"] # data sampling frequency (1m, 5m, 15m, 1h, 1d)\n",
    "subfolder = CONFIG[\"DATA\"][\"structure\"][sampling] # subfolder name inside the drive folder is determined by the sampling frequency\n",
    "pattern_fmt = CONFIG[\"DATA\"][\"file_pattern\"] # file naming pattern (s. CONFIG)\n",
    "tickers = CONFIG[\"DATA\"][\"tickers\"] # list of tickers to load\n",
    "forward_fill = CONFIG[\"DATA\"][\"forward_fill\"] # whether to forward fill missing data\n",
    "drop_na_after_ffill = CONFIG[\"DATA\"][\"drop_na_after_ffill\"] # whether to drop NA values after forward filling\n",
    "\n",
    "# function to find parquet file path\n",
    "def find_parquet_path(ticker: str, sampling: str) -> str:\n",
    "    fname = pattern_fmt.format(TICKER=ticker, FREQ=sampling)\n",
    "    # try subfolder first\n",
    "    candidates = glob.glob(os.path.join(CACHE_DIR, \"**\", subfolder, fname), recursive=True)\n",
    "    # if not found, try flat cache\n",
    "    if not candidates:\n",
    "        candidates = glob.glob(os.path.join(CACHE_DIR, \"**\", fname), recursive=True)\n",
    "    # if still not found, try direct file in cache (for file_ids downloads)\n",
    "    if not candidates:\n",
    "        direct_path = os.path.join(CACHE_DIR, fname)\n",
    "        if os.path.exists(direct_path):\n",
    "            candidates = [direct_path]\n",
    "    # if nothing is found at all, raise an error\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f\"Could not find {fname} under {CACHE_DIR}.\")\n",
    "    return candidates[0]\n",
    "\n",
    "# function to localize timestamps and align dataframes\n",
    "def localize_and_align(df: pd.DataFrame, tz_in: str = None, tz_out: str = None) -> pd.DataFrame:\n",
    "    # convert millisecond timestamps to datetime\n",
    "    if 'datetime' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['datetime'], unit='ms', utc=True)\n",
    "        df = df.set_index('timestamp')\n",
    "    # make column names lowercase for consistency\n",
    "    cols = {c: c.lower() for c in df.columns}\n",
    "    df = df.rename(columns=cols)\n",
    "    return df.sort_index()\n",
    "\n",
    "dfs = {}\n",
    "for t in tickers:\n",
    "    pth = find_parquet_path(t, sampling) # find file path\n",
    "    tmp = pd.read_parquet(pth) # read parquet file\n",
    "    tmp = localize_and_align(tmp) # standardize timestamps and column names\n",
    "    if forward_fill: # forward fill missing data if specified\n",
    "        tmp = tmp.ffill()\n",
    "    if drop_na_after_ffill: # drop rows with NA values after forward filling\n",
    "        tmp = tmp.dropna()\n",
    "    dfs[t] = tmp # store in dictionary\n",
    "\n",
    "common_index = None\n",
    "# find common index across all dataframes\n",
    "for t, df in dfs.items():\n",
    "    common_index = df.index if common_index is None else common_index.intersection(df.index)\n",
    "# reindex all dataframes to the common index and drop any remaining NA values\n",
    "for t in tickers:\n",
    "    dfs[t] = dfs[t].reindex(common_index).dropna()\n",
    "\n",
    "# print the shape of each dataframe to ensure alignment\n",
    "print({t: dfs[t].shape for t in tickers})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a77e6",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "This section performs feature engineering on the loaded data. It includes creating new features, normalizing data, and preparing the dataset for training the reinforcement learning agent. To ensure statistical arbitrage strategy is imitated, create relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b326bc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ticker                    BTC                                      ETH  \\\n",
      "feature                   ret       vol        rsi    volchg       ret   \n",
      "datetime                                                                 \n",
      "2025-09-02 18:00:00 -0.002415  0.004553  56.046963 -0.326068 -0.005253   \n",
      "2025-09-02 19:00:00  0.001441  0.004556  57.347919 -0.123407 -0.005208   \n",
      "2025-09-02 20:00:00  0.005512  0.004575  61.998583  0.267870  0.009015   \n",
      "2025-09-02 21:00:00 -0.002596  0.004589  58.744666 -0.496680  0.003603   \n",
      "2025-09-02 22:00:00  0.000574  0.004588  59.252994  0.016223 -0.000814   \n",
      "\n",
      "ticker                                              \n",
      "feature                   vol        rsi    volchg  \n",
      "datetime                                            \n",
      "2025-09-02 18:00:00  0.006955  43.078511  0.493765  \n",
      "2025-09-02 19:00:00  0.006879  40.698143 -0.197759  \n",
      "2025-09-02 20:00:00  0.006964  46.245797 -0.063885  \n",
      "2025-09-02 21:00:00  0.006982  48.339198 -0.467361  \n",
      "2025-09-02 22:00:00  0.006952  47.885173  0.147730  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>ticker</th>\n",
       "      <th colspan=\"4\" halign=\"left\">BTC</th>\n",
       "      <th colspan=\"4\" halign=\"left\">ETH</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th>ret</th>\n",
       "      <th>vol</th>\n",
       "      <th>rsi</th>\n",
       "      <th>volchg</th>\n",
       "      <th>ret</th>\n",
       "      <th>vol</th>\n",
       "      <th>rsi</th>\n",
       "      <th>volchg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8784.000000</td>\n",
       "      <td>8784.000000</td>\n",
       "      <td>8784.000000</td>\n",
       "      <td>8784.000000</td>\n",
       "      <td>8784.000000</td>\n",
       "      <td>8784.000000</td>\n",
       "      <td>8784.000000</td>\n",
       "      <td>8784.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.004585</td>\n",
       "      <td>51.232259</td>\n",
       "      <td>-0.000192</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.007127</td>\n",
       "      <td>51.206699</td>\n",
       "      <td>-0.000106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.004991</td>\n",
       "      <td>0.001950</td>\n",
       "      <td>11.619616</td>\n",
       "      <td>0.528632</td>\n",
       "      <td>0.007677</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>12.326999</td>\n",
       "      <td>0.546888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.050195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.149282</td>\n",
       "      <td>-0.124892</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.828906</td>\n",
       "      <td>-2.068495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.002048</td>\n",
       "      <td>0.003331</td>\n",
       "      <td>43.736511</td>\n",
       "      <td>-0.347504</td>\n",
       "      <td>-0.003253</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>43.299516</td>\n",
       "      <td>-0.360170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.004109</td>\n",
       "      <td>51.268990</td>\n",
       "      <td>-0.024810</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.006657</td>\n",
       "      <td>51.018342</td>\n",
       "      <td>-0.043595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.002256</td>\n",
       "      <td>0.005470</td>\n",
       "      <td>58.885158</td>\n",
       "      <td>0.296147</td>\n",
       "      <td>0.003631</td>\n",
       "      <td>0.008315</td>\n",
       "      <td>58.761127</td>\n",
       "      <td>0.317103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.049047</td>\n",
       "      <td>0.013790</td>\n",
       "      <td>90.153680</td>\n",
       "      <td>2.690468</td>\n",
       "      <td>0.091261</td>\n",
       "      <td>0.022406</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.696904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ticker           BTC                                                 ETH  \\\n",
       "feature          ret          vol          rsi       volchg          ret   \n",
       "count    8784.000000  8784.000000  8784.000000  8784.000000  8784.000000   \n",
       "mean        0.000075     0.004585    51.232259    -0.000192     0.000067   \n",
       "std         0.004991     0.001950    11.619616     0.528632     0.007677   \n",
       "min        -0.050195     0.000000     0.000000    -2.149282    -0.124892   \n",
       "25%        -0.002048     0.003331    43.736511    -0.347504    -0.003253   \n",
       "50%         0.000064     0.004109    51.268990    -0.024810     0.000131   \n",
       "75%         0.002256     0.005470    58.885158     0.296147     0.003631   \n",
       "max         0.049047     0.013790    90.153680     2.690468     0.091261   \n",
       "\n",
       "ticker                                          \n",
       "feature          vol          rsi       volchg  \n",
       "count    8784.000000  8784.000000  8784.000000  \n",
       "mean        0.007127    51.206699    -0.000106  \n",
       "std         0.002781    12.326999     0.546888  \n",
       "min         0.000000     6.828906    -2.068495  \n",
       "25%         0.005369    43.299516    -0.360170  \n",
       "50%         0.006657    51.018342    -0.043595  \n",
       "75%         0.008315    58.761127     0.317103  \n",
       "max         0.022406   100.000000     2.696904  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_cfg = CONFIG[\"ENV\"][\"features\"]\n",
    "price_col = CONFIG[\"DATA\"][\"price_point\"]\n",
    "\n",
    "# function to compute relative strength index (momentum indicator)\n",
    "def compute_rsi(series: pd.Series, period: int = 14) -> pd.Series:\n",
    "    delta = series.diff() # compute price changes\n",
    "    up = (delta.clip(lower=0)).ewm(alpha=1/period, adjust=False).mean() # average of upward price changes\n",
    "    down = (-delta.clip(upper=0)).ewm(alpha=1/period, adjust=False).mean() # average of downward price changes\n",
    "    rs = up / (down + 1e-12) # relative strength ratio indicates if upward or downward momentum is stronger\n",
    "    rsi = 100 - (100 / (1 + rs)) # converts RS to RSI bounded between 0 and 100\n",
    "    return rsi\n",
    "\n",
    "# function to create features dataframe\n",
    "def make_features(df: pd.DataFrame, price_col: str, vol_window: int, rsi_period: int, volume_change: bool):\n",
    "    out = pd.DataFrame(index=df.index) # initialize output dataframe\n",
    "    out[\"ret\"] = np.log(df[price_col]).diff(1) # log returns\n",
    "    out[\"vol\"] = out[\"ret\"].rolling(vol_window).std().fillna(0.0) # rolling volatility\n",
    "    out[\"rsi\"] = compute_rsi(df[price_col], rsi_period).fillna(50.0) # relative strength index\n",
    "    if \"volume\" in df.columns and volume_change: # log volume change if volume data is available\n",
    "        out[\"volchg\"] = np.log(df[\"volume\"].replace(0, np.nan)).diff().fillna(0.0) # log volume change\n",
    "    else:\n",
    "        out[\"volchg\"] = 0.0 # if no volume data, set to zero\n",
    "    return out\n",
    "\n",
    "features_by_ticker = {}\n",
    "for t in tickers: # create features for each ticker\n",
    "    fdf = make_features(dfs[t], price_col, feat_cfg[\"vol_window\"], feat_cfg[\"rsi_period\"], feat_cfg[\"volume_change\"])\n",
    "    features_by_ticker[t] = fdf # store features in dictionary\n",
    "\n",
    "panel_cols = []\n",
    "for t in tickers: # create multi-index columns\n",
    "    for col in [\"ret\",\"vol\",\"rsi\",\"volchg\"]:\n",
    "        panel_cols.append((t, col))\n",
    "panel = pd.concat([features_by_ticker[t][[\"ret\",\"vol\",\"rsi\",\"volchg\"]] for t in tickers], axis=1) # combine features horizontally\n",
    "panel.columns = pd.MultiIndex.from_tuples(panel_cols, names=[\"ticker\",\"feature\"]) # set multi-index columns\n",
    "panel = panel.dropna() # remove rows with missing values\n",
    "\n",
    "print(panel.tail())\n",
    "panel.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da1a418",
   "metadata": {},
   "source": [
    "## Feature scaling and state tensor construction\n",
    "\n",
    "This section normalizes the features and constructs the state tensors required for training the reinforcement learning agent. State tensors are multi-dimensional arrays that represent the current state of the environment that the RL agent uses to make decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f681542d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State tensor: (8719, 2, 4, 64) Returns: (8719, 2) InstVol: (8719, 2)\n"
     ]
    }
   ],
   "source": [
    "lookback = CONFIG[\"ENV\"][\"lookback_window\"] # lookback window \n",
    "normalize = CONFIG[\"ENV\"][\"features\"][\"normalize\"] # normalization method\n",
    "\n",
    "# rolling z-score normalization for features\n",
    "def rolling_zscore(df: pd.DataFrame, window: int = 256) -> pd.DataFrame:\n",
    "    mu = df.rolling(window).mean() # rolling mean\n",
    "    sigma = df.rolling(window).std().replace(0, np.nan) # rolling std dev, replace 0 with NaN to avoid division by zero\n",
    "    z = (df - mu) / (sigma + 1e-12) # z-score normalization\n",
    "    return z.fillna(0.0) # fill NaNs with 0.0 and return z-scored dataframe\n",
    "\n",
    "# build state tensor\n",
    "def build_state_tensor(panel: pd.DataFrame, lookback: int, normalize: bool = False):\n",
    "    # normalization step for each feature\n",
    "    if normalize: # group by feature and apply z-score while preserving MultiIndex\n",
    "        normalized = pd.DataFrame(index=panel.index) # empty dataframe with same index as features dataframes to hold scaled features\n",
    "        for feature in panel.columns.unique(level=1): # iterate over features\n",
    "            feature_data = panel.xs(feature, level=1, axis=1)\n",
    "            z_scored = rolling_zscore(feature_data, window=max(lookback*2, 256)) # take double the lookback value for rolling\n",
    "            for ticker in z_scored.columns: # reconstruct MultiIndex columns\n",
    "                normalized[(ticker, feature)] = z_scored[ticker]\n",
    "        normalized.columns = pd.MultiIndex.from_tuples(normalized.columns, names=[\"ticker\", \"feature\"]) # set MultiIndex columns\n",
    "    else: # no normalization, use raw features\n",
    "        normalized = panel.copy()\n",
    "\n",
    "    # organize data by tickers, features, and time\n",
    "    tickers = sorted(panel.columns.unique(level=0)) # ensure consistent order\n",
    "    features = sorted(panel.columns.unique(level=1)) # feature order\n",
    "    times = normalized.index # time index\n",
    "\n",
    "    # create sliding windows of lookback length\n",
    "    # extract feature data for the window and stack into a 3D tensor (tickers x features x lookback)\n",
    "    X, y_ret, inst_vol = [], [], [] # lists to hold state tensors, next returns, and current volatilities\n",
    "    for i in range(lookback, len(times)-1): # iterate over time index in lookback window to have a sliding window with each time point as a step\n",
    "        window_slice = normalized.iloc[i-lookback:i]\n",
    "        frames = []\n",
    "        for t in tickers: # save the windows of each ticker\n",
    "            frames.append(window_slice[t].T.values)\n",
    "        tensor = np.stack(frames, axis=0) # stack into 3D tensor\n",
    "        X.append(tensor) # append tensor to list\n",
    "        nxt = panel.iloc[i+1] # next period returns\n",
    "        y_ret.append(np.array([nxt[(t, \"ret\")] for t in tickers], dtype=float)) # save to list\n",
    "        cur = panel.iloc[i] # current period volatilities\n",
    "        inst_vol.append(np.array([cur[(t, \"vol\")] for t in tickers], dtype=float)) # save to list\n",
    "\n",
    "    X = np.array(X, dtype=np.float32) # convert list of state tensors of tensors to 3D numpy array\n",
    "    y_ret = np.array(y_ret, dtype=np.float32) # convert list of next returns to 2D numpy array\n",
    "    inst_vol = np.array(inst_vol, dtype=np.float32) # convert list of volatilities to 2D numpy array\n",
    "    return X, y_ret, inst_vol, tickers, features, times[lookback+1:]\n",
    "\n",
    "X_all, R_all, VOL_all, TICKER_ORDER, FEAT_ORDER, TIME_INDEX = build_state_tensor(\n",
    "    panel, lookback=lookback, normalize=normalize\n",
    ")\n",
    "\n",
    "print(\"State tensor:\", X_all.shape, \"Returns:\", R_all.shape, \"InstVol:\", VOL_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745f60af",
   "metadata": {},
   "source": [
    "## Define Splits and Adjust Timezones\n",
    "\n",
    "This section defines the training and validation splits for the dataset. It ensures that the data is divided appropriately to train the model, validate its performance during training, and test its final performance on unseen data. Also, it adjusts the timezones of the datetime indices to ensure consistency across the dataset. This is necessary in case the data comes from multiple sources with different timezone settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a8a71a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'coint_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m final_mask\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Create training masks based on cointegration periods\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m training_mask = create_cointegration_splits(\u001b[43mcoint_df\u001b[49m, TIME_INDEX)\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_mask.sum()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m timesteps in training windows\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Split the data into training and testing periods\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'coint_df' is not defined"
     ]
    }
   ],
   "source": [
    "# function to create boolean mask for date slicing\n",
    "def date_slice_mask(times: pd.DatetimeIndex, start: str, end: str):\n",
    "    # convert input dates to UTC timestamps\n",
    "    start_ts = pd.Timestamp(start).tz_localize('UTC')\n",
    "    end_ts = pd.Timestamp(end).tz_localize('UTC')\n",
    "    \n",
    "    # ensure time index is in UTC\n",
    "    if times.tz is None: # if naive, localize to UTC\n",
    "        times = times.tz_localize('UTC')\n",
    "    elif times.tz != pytz.UTC: # if timezone-aware but not UTC, convert to UTC\n",
    "        times = times.tz_convert('UTC')\n",
    "        \n",
    "    return (times >= start_ts) & (times <= end_ts) # return boolean mask for splits later on\n",
    "\n",
    "def create_cointegration_splits(coint_df: pd.DataFrame, price_data_index: pd.DatetimeIndex):\n",
    "    \"\"\"Create training windows based on cointegration periods.\"\"\"\n",
    "    train_masks = []\n",
    "    val_masks = []\n",
    "    test_masks = []\n",
    "    \n",
    "    # Ensure indices are timezone-aware\n",
    "    if price_data_index.tz is None:\n",
    "        price_data_index = price_data_index.tz_localize('UTC')\n",
    "    \n",
    "    # Sort periods chronologically\n",
    "    coint_df = coint_df.sort_values('train_start')\n",
    "    \n",
    "    # Split cointegration periods into train/val/test\n",
    "    split_point1 = int(0.7 * len(coint_df))  # 70% for training\n",
    "    split_point2 = int(0.85 * len(coint_df))  # 15% for validation\n",
    "    \n",
    "    train_periods = coint_df.iloc[:split_point1]\n",
    "    val_periods = coint_df.iloc[split_point1:split_point2]\n",
    "    test_periods = coint_df.iloc[split_point2:]\n",
    "    \n",
    "    # Create masks for each period type\n",
    "    for periods, masks in [(train_periods, train_masks), \n",
    "                          (val_periods, val_masks), \n",
    "                          (test_periods, test_masks)]:\n",
    "        for _, row in periods.iterrows():\n",
    "            mask = (price_data_index >= row['train_start']) & (price_data_index <= row['train_end'])\n",
    "            masks.append(mask)\n",
    "    \n",
    "    # Combine masks for each split with OR operation\n",
    "    train_mask = np.logical_or.reduce(train_masks) if train_masks else np.zeros(len(price_data_index), dtype=bool)\n",
    "    val_mask = np.logical_or.reduce(val_masks) if val_masks else np.zeros(len(price_data_index), dtype=bool)\n",
    "    test_mask = np.logical_or.reduce(test_masks) if test_masks else np.zeros(len(price_data_index), dtype=bool)\n",
    "    \n",
    "    print(f\"Training windows: {train_mask.sum()} timesteps\")\n",
    "    print(f\"Validation windows: {val_mask.sum()} timesteps\")\n",
    "    print(f\"Testing windows: {test_mask.sum()} timesteps\")\n",
    "    \n",
    "    return train_mask, val_mask, test_mask\n",
    "\n",
    "# Create training masks based on cointegration periods\n",
    "train_mask, val_mask, test_mask = create_cointegration_splits(coint_df, TIME_INDEX)\n",
    "\n",
    "# Create splits for training\n",
    "SPLITS = [{\n",
    "    \"name\": \"CointegrationSplit\",\n",
    "    \"train\": train_mask,\n",
    "    \"val\": val_mask,\n",
    "    \"test\": test_mask\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be710c8e",
   "metadata": {},
   "source": [
    "## Environment Configuration\n",
    "\n",
    "This is a custom Gymnasium environment for portfolio optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aab18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioWeightsEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": []}\n",
    "\n",
    "    def __init__(self, X, R, VOL, tickers, lookback, cfg_env):\n",
    "        super().__init__()\n",
    "        self.X = X # state tensor\n",
    "        self.R = R # next period returns\n",
    "        self.VOL = VOL # current period volatilities\n",
    "        self.tickers = tickers # list of assets\n",
    "        self.lookback = lookback # lookback window\n",
    "        self.cfg = cfg_env # environment configuration taken from CONFIG\n",
    "\n",
    "        self.n_assets = len(tickers) # number of assets\n",
    "        self.include_cash = cfg_env[\"include_cash\"] # whether to include cash as an asset\n",
    "        self.shorting = cfg_env[\"shorting\"] # whether shorting is allowed\n",
    "        self.dim_action = self.n_assets + (1 if self.include_cash else 0) # action space dimension: one weight per asset (+1 for cash if included)\n",
    "\n",
    "        obs_dim = self.n_assets * self.X.shape[2] * self.lookback # observation space dimension: tickers x features x lookback\n",
    "        self.observation_space = spaces.Box(low=-5, high=5, shape=(obs_dim,), dtype=np.float32) # needed for Gym to define feature bounds (±5 for z-scored features)\n",
    "        \n",
    "        # Update action space to allow for leveraged positions\n",
    "        if self.shorting:\n",
    "            self.action_space = spaces.Box(\n",
    "                low=-cfg_env[\"leverage\"][\"short_cap\"],\n",
    "                high=cfg_env[\"leverage\"][\"long_cap\"],\n",
    "                shape=(self.dim_action,),\n",
    "                dtype=np.float32\n",
    "            )\n",
    "        else:\n",
    "            self.action_space = spaces.Box(\n",
    "                low=0.0,\n",
    "                high=cfg_env[\"leverage\"][\"long_cap\"],\n",
    "                shape=(self.dim_action,),\n",
    "                dtype=np.float32\n",
    "            )\n",
    "\n",
    "        self.commission = cfg_env[\"transaction_costs\"][\"commission_bps\"] / 1e4 # commission in decimal form\n",
    "        self.slippage = cfg_env[\"transaction_costs\"][\"slippage_bps\"] / 1e4 # slippage in decimal form\n",
    "        self.risk_lambda = cfg_env[\"reward\"][\"risk_lambda\"] # defines risk aversion in reward function\n",
    "\n",
    "        # Leverage settings\n",
    "        self.long_cap = cfg_env[\"leverage\"][\"long_cap\"]\n",
    "        self.short_cap = cfg_env[\"leverage\"][\"short_cap\"]\n",
    "        self.use_asymmetric = cfg_env[\"leverage\"][\"use_asymmetric\"]\n",
    "\n",
    "        self.reset(seed=cfg_env.get(\"seed\", 42))\n",
    "\n",
    "    def _to_obs(self, t):\n",
    "        # Get the current window of observations\n",
    "        arr = self.X[t].reshape(-1).astype(np.float32)\n",
    "        return arr\n",
    "    \n",
    "    # project raw actions to asset weights with leverage\n",
    "    def _project_weights(self, a):\n",
    "        if self.use_asymmetric:\n",
    "            # Apply asymmetric leverage caps\n",
    "            w = np.clip(a, -self.short_cap if self.shorting else 0.0, self.long_cap)\n",
    "        else:\n",
    "            # Use symmetric leverage cap\n",
    "            max_leverage = max(self.long_cap, self.short_cap)\n",
    "            w = np.clip(a, -max_leverage if self.shorting else 0.0, max_leverage)\n",
    "        return w\n",
    "\n",
    "    # reset environment to initial state\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.t = 0 # reset time index\n",
    "        self.portfolio_value = 1.0 # reset portfolio value\n",
    "        self.w = np.zeros(self.dim_action) # initialize with zero weights\n",
    "        if self.include_cash:\n",
    "            self.w[-1] = 1.0  # start with all cash\n",
    "        obs = self._to_obs(self.t)\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        w_target = self._project_weights(action) # get new target weights from raw actions\n",
    "        turnover = np.sum(np.abs(w_target - self.w)) # calculate turnover as sum of absolute weight changes\n",
    "        trading_cost = (self.commission + self.slippage) * turnover # total trading cost based on turnover\n",
    "\n",
    "        asset_w_prev = self.w[:self.n_assets] # previous weights excluding cash\n",
    "        asset_ret = np.dot(asset_w_prev, self.R[self.t]) # calculate asset return based on previous weights and next period returns\n",
    "        inst_vol = np.dot(asset_w_prev, self.VOL[self.t]) # calculate instantaneous volatility based on previous weights and current volatilities\n",
    "\n",
    "        # reward function: asset return minus trading costs and risk penalty\n",
    "        reward = asset_ret - trading_cost - self.risk_lambda * inst_vol\n",
    "\n",
    "        self.portfolio_value *= math.exp(asset_ret - trading_cost) # update portfolio value using log returns\n",
    "\n",
    "        self.w = w_target # update calculated weights for next step\n",
    "        self.t += 1 # move to next time step\n",
    "        terminated = (self.t >= len(self.R)-1) # episode ends if we reach the end of the data\n",
    "        truncated = False\n",
    "\n",
    "        obs = self._to_obs(self.t) if not terminated else self._to_obs(self.t-1)\n",
    "        obs = np.clip(obs, -5.0, 5.0) # clip observations to previously defined bounds (±5 for z-scored features)\n",
    "        \n",
    "        # Additional info for monitoring leveraged positions\n",
    "        leverage = np.sum(np.abs(self.w[:self.n_assets]))\n",
    "        info = {\n",
    "            \"portfolio_value\": self.portfolio_value, \n",
    "            \"turnover\": turnover, \n",
    "            \"inst_vol\": inst_vol, \n",
    "            \"asset_ret\": asset_ret,\n",
    "            \"total_leverage\": leverage\n",
    "        }\n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7281e10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_by_mask(X, R, VOL, mask: np.ndarray):\n",
    "    idx = np.where(mask)[0] # get all indices where mask is True\n",
    "    \n",
    "    # If we have no valid indices, return empty arrays with correct shapes\n",
    "    if len(idx) == 0:\n",
    "        empty_shape_x = list(X.shape)\n",
    "        empty_shape_x[0] = 0\n",
    "        empty_shape_r = list(R.shape)\n",
    "        empty_shape_r[0] = 0\n",
    "        empty_shape_v = list(VOL.shape)\n",
    "        empty_shape_v[0] = 0\n",
    "        return np.zeros(empty_shape_x), np.zeros(empty_shape_r), np.zeros(empty_shape_v)\n",
    "    \n",
    "    return X[idx], R[idx], VOL[idx] # return only the selected slices of data\n",
    "\n",
    "def make_env_from_mask(mask, name=\"env\"):\n",
    "    X_s, R_s, V_s = slice_by_mask(X_all, R_all, VOL_all, mask)\n",
    "    env = PortfolioWeightsEnv(X_s, R_s, V_s, TICKER_ORDER, CONFIG[\"ENV\"][\"lookback_window\"], CONFIG[\"ENV\"])\n",
    "    env = Monitor(env, filename=None)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadc07b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annualize_factor(sampling: str):\n",
    "    return ANNUALIZATION.get(sampling, 365*24)\n",
    "\n",
    "def compute_metrics(equity_curve: pd.Series, sampling: str, turnover_series: pd.Series = None):\n",
    "    ret = equity_curve.pct_change().dropna()\n",
    "    ann = annualize_factor(sampling)\n",
    "    mu = ret.mean() * ann\n",
    "    sigma = ret.std() * math.sqrt(ann)\n",
    "    sharpe = mu / (sigma + 1e-12)\n",
    "    downside = ret[ret < 0].std() * math.sqrt(ann)\n",
    "    sortino = mu / (downside + 1e-12)\n",
    "    if len(equity_curve) > 1:\n",
    "        # Calculate years based on number of samples and sampling frequency\n",
    "        if isinstance(equity_curve.index, pd.DatetimeIndex):\n",
    "            dt_years = (equity_curve.index[-1] - equity_curve.index[0]).total_seconds() / (365 * 24 * 3600)\n",
    "        else:\n",
    "            # If using RangeIndex, calculate based on sampling frequency\n",
    "            samples = len(equity_curve)\n",
    "            samples_per_year = annualize_factor(sampling)\n",
    "            dt_years = samples / samples_per_year\n",
    "        dt_years = float(dt_years) if float(dt_years) != 0 else 1e-12\n",
    "        cagr = (equity_curve.iloc[-1] / equity_curve.iloc[0]) ** (1/dt_years) - 1\n",
    "    else:\n",
    "        cagr = 0.0\n",
    "    cummax = equity_curve.cummax()\n",
    "    dd = (equity_curve / cummax - 1).min()\n",
    "    maxdd = float(dd)\n",
    "    calmar = mu / (abs(maxdd) + 1e-12)\n",
    "    hit_ratio = (ret > 0).mean()\n",
    "    turnover = turnover_series.mean() if turnover_series is not None and len(turnover_series)>0 else np.nan\n",
    "    return {\"CAGR\": cagr, \"Sharpe\": sharpe, \"Sortino\": sortino, \"MaxDrawdown\": maxdd, \"Calmar\": calmar, \"Volatility\": sigma, \"Turnover\": turnover, \"HitRatio\": hit_ratio}\n",
    "\n",
    "def plot_series(series: pd.Series, title: str):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(series.index, series.values)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_training_periods(coint_df: pd.DataFrame, price_index: pd.DatetimeIndex):\n",
    "    \"\"\"Plot timeline showing training periods and their overlap.\"\"\"\n",
    "    plt.figure(figsize=(15,5))\n",
    "    y = 0\n",
    "    for _, row in coint_df.iterrows():\n",
    "        plt.hlines(y, row['train_start'], row['train_end'], 'blue', alpha=0.3)\n",
    "        y += 1\n",
    "    plt.title(\"Training Periods Timeline\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Period Index\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def backtest_env(env: gym.Env, model=None):\n",
    "    # Get the unwrapped environment\n",
    "    unwrapped = env.unwrapped if hasattr(env, 'unwrapped') else env\n",
    "    \n",
    "    obs, _ = env.reset()\n",
    "    pv, turns = [], []\n",
    "    leverage = []  # Track leverage over time\n",
    "    \n",
    "    for t in range(len(unwrapped.R)-1):\n",
    "        if model is None:\n",
    "            action = np.ones(unwrapped.dim_action)/unwrapped.dim_action\n",
    "        else:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, trunc, info = env.step(action)\n",
    "        pv.append(info[\"portfolio_value\"])\n",
    "        turns.append(info[\"turnover\"])\n",
    "        leverage.append(info.get(\"total_leverage\", 0))\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    idx = pd.RangeIndex(start=0, stop=len(pv), step=1)\n",
    "    ec = pd.Series(pv, index=idx)\n",
    "    to = pd.Series(turns, index=idx)\n",
    "    lev = pd.Series(leverage, index=idx)\n",
    "    \n",
    "    return ec, to, lev\n",
    "\n",
    "# Plot cointegration training periods\n",
    "print(\"Visualizing training periods timeline...\")\n",
    "plot_training_periods(coint_df, TIME_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc41478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_dir(CONFIG[\"IO\"][\"models_dir\"])\n",
    "ensure_dir(CONFIG[\"EVAL\"][\"reports_dir\"])\n",
    "\n",
    "RESULTS = []\n",
    "\n",
    "for split in SPLITS:\n",
    "    print(f\"\\n=== Training on split: {split['name']} ===\")\n",
    "    train_env = make_env_from_mask(split[\"train\"], name=f\"{split['name']}_train\")\n",
    "    eval_env  = make_env_from_mask(split[\"test\"], name=f\"{split['name']}_test\")\n",
    "\n",
    "    vec_train = DummyVecEnv([lambda: train_env])\n",
    "    vec_eval  = DummyVecEnv([lambda: eval_env])\n",
    "\n",
    "    model = PPO(\n",
    "        policy=CONFIG[\"RL\"][\"policy\"],\n",
    "        env=vec_train,\n",
    "        gamma=CONFIG[\"RL\"][\"gamma\"],\n",
    "        gae_lambda=CONFIG[\"RL\"][\"gae_lambda\"],\n",
    "        clip_range=CONFIG[\"RL\"][\"clip_range\"],\n",
    "        n_steps=CONFIG[\"RL\"][\"n_steps\"],\n",
    "        batch_size=CONFIG[\"RL\"][\"batch_size\"],\n",
    "        learning_rate=CONFIG[\"RL\"][\"learning_rate\"],\n",
    "        ent_coef=CONFIG[\"RL\"][\"ent_coef\"],\n",
    "        vf_coef=CONFIG[\"RL\"][\"vf_coef\"],\n",
    "        max_grad_norm=CONFIG[\"RL\"][\"max_grad_norm\"],\n",
    "        tensorboard_log=CONFIG[\"IO\"][\"tb_logdir\"],\n",
    "        device=device,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    eval_callback = EvalCallback(\n",
    "        vec_eval, \n",
    "        best_model_save_path=CONFIG[\"IO\"][\"models_dir\"],\n",
    "        log_path=CONFIG[\"IO\"][\"models_dir\"], \n",
    "        eval_freq=10000,\n",
    "        deterministic=True, \n",
    "        render=False\n",
    "    )\n",
    "    \n",
    "    model.learn(total_timesteps=CONFIG[\"RL\"][\"timesteps\"], callback=eval_callback)\n",
    "    model_path = os.path.join(CONFIG[\"IO\"][\"models_dir\"], f\"ppo_{split['name']}.zip\")\n",
    "    model.save(model_path)\n",
    "    print(\"Saved model:\", model_path)\n",
    "\n",
    "    test_env = make_env_from_mask(split[\"test\"], name=f\"{split['name']}_test\")\n",
    "    ec, to = backtest_env(test_env, model=model)\n",
    "\n",
    "    idx = np.where(split[\"test\"])[0]\n",
    "    R_test = R_all[idx]\n",
    "    ew = np.ones(len(TICKER_ORDER))/len(TICKER_ORDER)\n",
    "    ec_bench = [1.0]\n",
    "    # Only iterate through the same number of returns as we have in ec.index\n",
    "    for i in range(len(ec.index)-1):\n",
    "        ec_bench.append(ec_bench[-1]*math.exp(np.dot(ew, R_test[i])))\n",
    "    ec_bench = pd.Series(ec_bench, index=ec.index)\n",
    "\n",
    "    bh_btc, bh_eth = [1.0], [1.0]\n",
    "    # Only iterate through the same number of returns as we have in ec.index\n",
    "    for i in range(len(ec.index)-1):\n",
    "        bh_btc.append(bh_btc[-1]*math.exp(R_test[i][0]))\n",
    "        bh_eth.append(bh_eth[-1]*math.exp(R_test[i][1]))\n",
    "    bh_btc = pd.Series(bh_btc, index=ec.index)\n",
    "    bh_eth = pd.Series(bh_eth, index=ec.index)\n",
    "\n",
    "    m_model = compute_metrics(ec, CONFIG[\"DATA\"][\"sampling\"], to)\n",
    "    m_ew    = compute_metrics(ec_bench, CONFIG[\"DATA\"][\"sampling\"])\n",
    "    m_btc   = compute_metrics(bh_btc, CONFIG[\"DATA\"][\"sampling\"])\n",
    "    m_eth   = compute_metrics(bh_eth, CONFIG[\"DATA\"][\"sampling\"])\n",
    "\n",
    "    RESULTS.append({\"split\": split[\"name\"], \"model\": m_model, \"equal_weight\": m_ew, \"buy_and_hold_BTC\": m_btc, \"buy_and_hold_ETH\": m_eth})\n",
    "\n",
    "    if CONFIG[\"EVAL\"][\"plots\"]:\n",
    "        plot_series(ec, f\"Equity Curve — PPO ({split['name']})\")\n",
    "        plot_series((ec / ec.cummax()) - 1.0, f\"Drawdown — PPO ({split['name']})\")\n",
    "        plot_series(ec_bench, f\"Equity Curve — Equal-Weight Hold ({split['name']})\")\n",
    "\n",
    "print(\"Done. RESULTS collected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eec7971",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rows = []\n",
    "for res in RESULTS:\n",
    "    row = {\"split\": res[\"split\"]}\n",
    "    for k, metrics in res.items():\n",
    "        if k == \"split\":\n",
    "            continue\n",
    "        for mname, mval in metrics.items():\n",
    "            row[f\"{k}_{mname}\"] = mval\n",
    "    rows.append(row)\n",
    "\n",
    "df_results = pd.DataFrame(rows)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f15943",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "ensure_dir(CONFIG[\"EVAL\"][\"reports_dir\"])\n",
    "out_json = os.path.join(CONFIG[\"EVAL\"][\"reports_dir\"], f\"metrics_{ts}.json\")\n",
    "out_csv  = os.path.join(CONFIG[\"EVAL\"][\"reports_dir\"], f\"metrics_{ts}.csv\")\n",
    "df_results.to_csv(out_csv, index=False)\n",
    "with open(out_json, \"w\") as f:\n",
    "    json.dump(RESULTS, f, indent=2)\n",
    "print(\"Saved:\", out_json, \"and\", out_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
