{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1bb7b54",
   "metadata": {},
   "source": [
    "# Conservative Q-Learning for Statistical Arbitrage\n",
    "\n",
    "This notebook demonstrates how to configure and train a Conservative Q-Learning (CQL) agent for statistical arbitrage using observations and actions from a simple rule-based strategy. The CQL agent learns to optimize expected returns using offline reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e79612d",
   "metadata": {},
   "source": [
    "## Install Packages\n",
    "\n",
    "The following libraries are required for this notebook. If you haven't installed them yet, you can do so using running the cell below or by using pip install in your terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67b434b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages including d3rlpy\n",
    "# %pip -q install -U numpy pandas pyarrow gdown gymnasium torch matplotlib tensorboard d3rlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568820c4",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4143bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from gdown import download\n",
    "import random\n",
    "\n",
    "from typing import Tuple, Set, Optional, Match\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Reinforcement Learning - d3rlpy\n",
    "import gymnasium as gym\n",
    "import d3rlpy\n",
    "\n",
    "# Deep Learning\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4664e80a",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "The configuration section sets up the parameters for the reinforcement learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f34fbc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"DATA\": {\n",
    "        \"forward_fill\": True,\n",
    "        \"drop_na_after_ffill\": True,\n",
    "        \"cache_dir\": \"./data_cache\",\n",
    "        \"timestamp_format\": \"%Y-%m-%d %H:%M:%S\",\n",
    "        \"asset_price_format\": \"{ASSET}_{FEATURE}\",\n",
    "        \"pair_feature_format\": \"{ASSET1}_{ASSET2}_{FEATURE}\",\n",
    "        \"timestamp_col\": \"timestamp\",\n",
    "        \"sampling\": \"1m\",\n",
    "        \"features\": {\n",
    "            \"file_id\": \"1OCqEkOWV73Z8e-67fpqVL3r3ugVcfml8\",\n",
    "            \"file_name\": \"bin_futures_full_features\",\n",
    "            \"type\": \"csv\",\n",
    "            \"seperator\": \",\",\n",
    "            \"index\": \"datetime\",\n",
    "            \"start\": \"2024-05-01 00:00:00\",\n",
    "            \"end\": \"2025-05-01 00:00:00\",\n",
    "            \"individual_identifier\": \"close\",\n",
    "            \"pair_identifier\": \"beta\",\n",
    "        },\n",
    "    },\n",
    "    \"Strategy\": {\n",
    "        \"z-score\": \"spreadNorm\",\n",
    "        \"entry\": 1.5,\n",
    "        \"exit\": 0.5,\n",
    "    },\n",
    "    \"BC\": {\n",
    "        \"batch_size\": 256,\n",
    "        \"n_epochs\": 50,\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"gamma\": 0.99,\n",
    "        \"tau\": 0.005,\n",
    "        \"n_critics\": 2,\n",
    "        \"use_gpu\": True,\n",
    "    },\n",
    "    \"CQL\": {\n",
    "        \"batch_size\": 256,\n",
    "        \"n_steps\": 10000,\n",
    "        \"n_steps_per_epoch\": 1000,\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"gamma\": 0.99,\n",
    "        \"tau\": 0.005,\n",
    "        \"alpha\": 5.0,\n",
    "        \"conservative_weight\": 5.0,\n",
    "        \"use_gpu\": True,\n",
    "        \"encoder_hidden_sizes\": [256, 256],\n",
    "        \"q_func_hidden_sizes\": [256, 256],\n",
    "    },\n",
    "    \"CQL_PnL\": {\n",
    "        \"batch_size\": 256,\n",
    "        \"n_steps\": 10000,\n",
    "        \"n_steps_per_epoch\": 1000,\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"gamma\": 0.99,\n",
    "        \"tau\": 0.005,\n",
    "        \"alpha\": 5.0,\n",
    "        \"use_gpu\": True,\n",
    "        \"encoder_hidden_sizes\": [256, 256],\n",
    "        \"q_func_hidden_sizes\": [256, 256],\n",
    "        \"transaction_cost_bps\": 3.5,  # 0.035% = 3.5 basis points\n",
    "    },\n",
    "    \"ENV\": {\n",
    "        \"seed\": 42,\n",
    "        \"trading_window_days\": \"2D\",\n",
    "        \"sliding_window_step\": \"1D\",\n",
    "    },\n",
    "    \"SPLITS\": {\n",
    "        \"data_start\": \"2024-05-01\",\n",
    "        \"data_end\": \"2025-04-30\",\n",
    "        \"train\": [\"2024-05-01\", \"2024-12-31\"],    # ~8 months (70%)\n",
    "        \"val\":   [\"2025-01-01\", \"2025-02-28\"],    # ~2 months (15%)\n",
    "        \"test\":  [\"2025-03-01\", \"2025-04-30\"],    # ~2 months (15%)\n",
    "    },\n",
    "    \"EVAL\": {\n",
    "        \"plots\": True,\n",
    "        \"reports_dir\": \"./reports\"\n",
    "    },\n",
    "    \"IO\": {\n",
    "        \"models_dir\": \"./models\",\n",
    "        \"tb_logdir\": \"./tb_logs\",\n",
    "        \"dataset_dir\": \"./offline_datasets\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c6388",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69053e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set annualization factors for different timeframes\n",
    "ANNUALIZATION = {\"1m\":365*24*60,\n",
    "                 \"5m\":365*24*12,\n",
    "                 \"15m\":365*24*4,\n",
    "                 \"1h\":365*24,\n",
    "                 \"1d\":365}\n",
    "\n",
    "# create directory if it doesn't already exist\n",
    "def ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f12ee23",
   "metadata": {},
   "source": [
    "## Set Computation Device\n",
    "\n",
    "This section sets the computation device for training the model. It checks if a GPU is available and sets it as the device; otherwise, it defaults to CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50bc4ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available. Using Apple Silicon GPU.\n"
     ]
    }
   ],
   "source": [
    "# run on cuda GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"CUDA is available. Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# run on Apple Silicon GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS is available. Using Apple Silicon GPU.\")\n",
    "\n",
    "# run on CPU (slow)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA and MPS are not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63981a61",
   "metadata": {},
   "source": [
    "## Set Seeds\n",
    "\n",
    "This section sets the random seeds for various libraries to ensure that the results are reproducible.\n",
    "\n",
    "Note: It is good practice to type (set data types) for function and method parameters for better code maintainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "775695a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed: int = 42):\n",
    "    random.seed(seed) # seed for random module\n",
    "    np.random.seed(seed) # seed for numpy module\n",
    "    try:\n",
    "        torch.manual_seed(seed) # seed for torch module\n",
    "        if torch.cuda.is_available(): # seed for CUDA device\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        # Note: MPS uses the same manual_seed as regular torch\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "set_all_seeds(CONFIG[\"ENV\"][\"seed\"]) # set all seeds for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e449a3db",
   "metadata": {},
   "source": [
    "## Download Feature Data\n",
    "\n",
    "In this section, we retrieve the .csv file created during feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aee5ec06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping download. File bin_futures_full_features already exists in cache.\n"
     ]
    }
   ],
   "source": [
    "def download_file(file_name: str, file_id: str, out_dir: str):\n",
    "    ensure_dir(out_dir)\n",
    "\n",
    "    out_path = os.path.join(out_dir, f\"{file_name}.csv\")  # full output path\n",
    "\n",
    "    if os.path.exists(out_path):\n",
    "        print(f\"Skipping download. File {file_name} already exists in cache.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        print(f\"Downloading {file_name} -> {out_path}\")\n",
    "        url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "        success = download(url, out_path, quiet=False, use_cookies=False, verify=True)\n",
    "        print(\"Download complete.\")\n",
    "        return success\n",
    "    except Exception as e:\n",
    "        print(f\"Download attempt failed for {file_name}. Error: {str(e)}\")\n",
    "    \n",
    "# donwload features\n",
    "file_name = CONFIG[\"DATA\"][\"features\"][\"file_name\"]\n",
    "file_id = CONFIG[\"DATA\"][\"features\"][\"file_id\"]\n",
    "cache_dir = CONFIG[\"DATA\"][\"cache_dir\"]\n",
    "\n",
    "success = download_file(file_name, file_id, cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b151dd54",
   "metadata": {},
   "source": [
    "## Load Feature Data\n",
    "\n",
    "Once the data is downloaded, this section loads the data into a pandas DataFrame for later processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65cc3f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 525600 entries, 2024-05-01 00:00:00 to 2025-04-30 23:59:00\n",
      "Columns: 789 entries, AAVE_close to XRP_fundingMinutesLeft\n",
      "dtypes: float64(763), int64(25), object(1)\n",
      "memory usage: 3.1+ GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def load_csv_to_df(\n",
    "    path: str,\n",
    "    sep: str = \",\",\n",
    "    timestamp_index_col: str | None = \"datetime\",\n",
    "    encoding: str = \"utf-8-sig\",\n",
    "    **read_csv_kwargs,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a CSV into a pandas DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Filesystem path to the CSV.\n",
    "    parse_timestamp_col : str | None\n",
    "        If provided and present in the CSV, this column will be parsed to datetime.\n",
    "        Set to None to skip datetime parsing.\n",
    "    **read_csv_kwargs :\n",
    "        Extra arguments passed to `pd.read_csv` (e.g., sep, dtype, usecols).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse header-only to check for timestamp col presence\n",
    "    head = pd.read_csv(path, sep=sep, encoding=encoding, nrows=0)\n",
    "    if timestamp_index_col and timestamp_index_col in head.columns:\n",
    "        read_csv_kwargs = {\n",
    "            **read_csv_kwargs,\n",
    "            \"parse_dates\": [timestamp_index_col],\n",
    "        }\n",
    "\n",
    "    df = pd.read_csv(path, sep=sep, encoding=encoding, engine=\"pyarrow\", **read_csv_kwargs)\n",
    "\n",
    "    df = df.set_index(\"datetime\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# load features\n",
    "file_name = CONFIG[\"DATA\"][\"features\"][\"file_name\"]\n",
    "cache_dir = CONFIG[\"DATA\"][\"cache_dir\"]\n",
    "index = CONFIG[\"DATA\"][\"features\"][\"index\"]\n",
    "sep = CONFIG[\"DATA\"][\"features\"].get(\"seperator\", \",\")\n",
    "file_path = os.path.join(cache_dir, f\"{file_name}.csv\")\n",
    "features_df = load_csv_to_df(file_path, sep, timestamp_index_col=index)\n",
    "\n",
    "# print dataframe info\n",
    "print(\"Features DataFrame Info:\")\n",
    "print(features_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916f09ef",
   "metadata": {},
   "source": [
    "## Identify Feature Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30901f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 25 assets: ['AAVE', 'ADA', 'APT', 'ARB', 'ATOM', 'AVAX', 'BCH', 'BNB', 'BTC', 'DOGE', 'DOT', 'ENA', 'ETC', 'ETH', 'HBAR', 'LINK', 'LTC', 'NEAR', 'SUI', 'TON', 'TRX', 'UNI', 'WLD', 'XLM', 'XRP']\n",
      "Identified 61 asset pairs: [('AAVE', 'SUI'), ('AAVE', 'TRX'), ('ADA', 'BTC'), ('ADA', 'DOGE'), ('ADA', 'HBAR'), ('ADA', 'LTC'), ('ADA', 'SUI'), ('ADA', 'XLM'), ('ADA', 'XRP'), ('APT', 'AVAX'), ('ARB', 'ATOM'), ('ARB', 'AVAX'), ('ARB', 'DOT'), ('ARB', 'ETC'), ('ARB', 'ETH'), ('ARB', 'NEAR'), ('ARB', 'WLD'), ('ATOM', 'BCH'), ('ATOM', 'DOT'), ('ATOM', 'ENA'), ('ATOM', 'ETC'), ('AVAX', 'BCH'), ('AVAX', 'DOT'), ('AVAX', 'ETC'), ('AVAX', 'UNI'), ('BCH', 'DOT'), ('BCH', 'ENA'), ('BCH', 'ETC'), ('BNB', 'LINK'), ('BTC', 'DOGE'), ('BTC', 'HBAR'), ('BTC', 'LTC'), ('BTC', 'SUI'), ('BTC', 'TRX'), ('BTC', 'XLM'), ('BTC', 'XRP'), ('DOGE', 'LINK'), ('DOGE', 'LTC'), ('DOGE', 'SUI'), ('DOGE', 'XLM'), ('DOT', 'ENA'), ('DOT', 'ETC'), ('DOT', 'ETH'), ('ENA', 'ETC'), ('ENA', 'UNI'), ('ETC', 'ETH'), ('ETC', 'NEAR'), ('ETH', 'NEAR'), ('HBAR', 'LTC'), ('HBAR', 'XLM'), ('HBAR', 'XRP'), ('LINK', 'LTC'), ('LINK', 'UNI'), ('LTC', 'XLM'), ('NEAR', 'TON'), ('NEAR', 'WLD'), ('SUI', 'TRX'), ('SUI', 'XLM'), ('TRX', 'XLM'), ('TRX', 'XRP'), ('XLM', 'XRP')]\n",
      "Identified 12 single-asset features: ['close', 'closeBias', 'closeEwmaVol', 'closeKalman', 'closeLowerShadow', 'closeMacd', 'closeRsi', 'closeSigns', 'closeStochRsi', 'closeUpperShadow', 'funding', 'fundingMinutesLeft']\n",
      "Identified 8 pair features: ['alpha', 'beta', 'corr', 'pval', 'spreadNorm', 'spreadNormKalman', 'spreadNormMa', 'spreadNormVol']\n"
     ]
    }
   ],
   "source": [
    "def identify_assets_features_pairs(\n",
    "    df: pd.DataFrame,\n",
    "    single_asset_format: str,\n",
    "    pair_feature_format: str,\n",
    ") -> Tuple[list[str], list[str], list[str], list[Tuple[str, str]]]:\n",
    "    \"\"\"\n",
    "    Returns distinct\n",
    "      1. assets\n",
    "      2. single-asset feature names (ARB_closeUpperShadow → closeUpperShadow)\n",
    "      3. pair feature names (ARB_ETH_spreadNorm → spreadNorm)\n",
    "      4. unordered asset pairs\n",
    "    \"\"\"\n",
    "\n",
    "    def format_to_regex(fmt: str) -> re.Pattern:\n",
    "        escaped = re.escape(fmt)\n",
    "\n",
    "        def repl(match: Match[str]) -> str:\n",
    "            name = match.group(1)\n",
    "            char_class = r\"[A-Za-z0-9_]+\" if \"FEATURE\" in name.upper() else r\"[A-Za-z0-9]+\"\n",
    "            return f\"(?P<{name}>{char_class})\"\n",
    "\n",
    "        escaped = re.sub(r\"\\\\\\{(\\w+)\\\\\\}\", repl, escaped)\n",
    "        return re.compile(f\"^{escaped}$\")\n",
    "\n",
    "    single_asset_pattern = format_to_regex(single_asset_format)\n",
    "    pair_feature_pattern = format_to_regex(pair_feature_format)\n",
    "    generic_single_pattern = re.compile(r\"^(?P<ASSET>[A-Za-z0-9]+)_(?P<FEATURE>[A-Za-z0-9_]+)$\")\n",
    "\n",
    "    assets: Set[str] = set()\n",
    "    single_features: Set[str] = set()\n",
    "    pair_features: Set[str] = set()\n",
    "    pairs: Set[Tuple[str, str]] = set()\n",
    "\n",
    "    literal_feature = None\n",
    "    if \"{FEATURE}\" not in single_asset_format:\n",
    "        literal_feature = single_asset_format.replace(\"{ASSET}\", \"\").lstrip(\"_\")\n",
    "\n",
    "    skip_cols = {\"timestamp\", \"datetime\", \"date\"}\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col in skip_cols:\n",
    "            continue\n",
    "\n",
    "        match_pair = pair_feature_pattern.match(col)\n",
    "        if match_pair:\n",
    "            a1, a2, feat = match_pair.group(\"ASSET1\"), match_pair.group(\"ASSET2\"), match_pair.group(\"FEATURE\")\n",
    "            assets.update((a1, a2))\n",
    "            pairs.add(tuple(sorted((a1, a2))))\n",
    "            pair_features.add(feat)\n",
    "            continue\n",
    "\n",
    "        match_single = single_asset_pattern.match(col)\n",
    "        if match_single:\n",
    "            asset = match_single.group(\"ASSET\")\n",
    "            assets.add(asset)\n",
    "            feat = match_single.groupdict().get(\"FEATURE\") or literal_feature\n",
    "            if feat:\n",
    "                single_features.add(feat)\n",
    "            continue\n",
    "\n",
    "        match_generic = generic_single_pattern.match(col)\n",
    "        if match_generic:\n",
    "            asset, feat = match_generic.group(\"ASSET\"), match_generic.group(\"FEATURE\")\n",
    "            assets.add(asset)\n",
    "            single_features.add(feat)\n",
    "            continue\n",
    "\n",
    "    return (\n",
    "        sorted(assets),\n",
    "        sorted(single_features),\n",
    "        sorted(pair_features),\n",
    "        sorted(pairs),\n",
    "    )\n",
    "\n",
    "# identify assets, features, and asset pairs\n",
    "single_asset_format = CONFIG[\"DATA\"][\"asset_price_format\"]\n",
    "pair_feature_format = CONFIG[\"DATA\"][\"pair_feature_format\"]\n",
    "assets, single_asset_features, pair_features, asset_pairs = identify_assets_features_pairs(\n",
    "    features_df,\n",
    "    CONFIG[\"DATA\"][\"asset_price_format\"],\n",
    "    CONFIG[\"DATA\"][\"pair_feature_format\"],\n",
    ")\n",
    "\n",
    "print(f\"Identified {len(assets)} assets: {assets}\")\n",
    "print(f\"Identified {len(asset_pairs)} asset pairs: {asset_pairs}\")\n",
    "print(f\"Identified {len(single_asset_features)} single-asset features: {single_asset_features}\")\n",
    "print(f\"Identified {len(pair_features)} pair features: {pair_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bd4a34",
   "metadata": {},
   "source": [
    "## Build Time Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f391580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 363 time intervals with window=2D and step=1D.\n",
      "First 3 intervals:\n",
      "  2024-05-01 00:00:00 to 2024-05-03 00:00:00\n",
      "  2024-05-02 00:00:00 to 2024-05-04 00:00:00\n",
      "  2024-05-03 00:00:00 to 2024-05-05 00:00:00\n"
     ]
    }
   ],
   "source": [
    "def build_time_intervals(\n",
    "    df: pd.DataFrame,\n",
    "    window: pd.Timedelta | str,\n",
    "    step: Optional[pd.Timedelta | str] = None,\n",
    "    timestamp_col: str = \"datetime\",\n",
    "    include_last_partial: bool = False,\n",
    ") -> list[tuple[pd.Timestamp, pd.Timestamp]]:\n",
    "    \"\"\"\n",
    "    Return fixed-length time intervals over the DataFrame's time span.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Must contain a datetime-like 'timestamp' column or have a DatetimeIndex.\n",
    "    window : pd.Timedelta | str\n",
    "        Size of each window, e.g. '2D', '60min', '15T'.\n",
    "    step : pd.Timedelta | str | None\n",
    "        Step between consecutive window starts. Defaults to `window` (non-overlapping).\n",
    "        Use a smaller step than `window` for sliding/overlapping windows.\n",
    "    timestamp_col : str\n",
    "        Name of the timestamp column (ignored if index is a DatetimeIndex).\n",
    "    include_last_partial : bool\n",
    "        If True, include the trailing partial window shorter than `window`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[tuple[pd.Timestamp, pd.Timestamp]]\n",
    "        Half-open intervals [start, end).\n",
    "    \"\"\"\n",
    "    W = pd.Timedelta(window)\n",
    "    S = pd.Timedelta(step) if step is not None else W\n",
    "\n",
    "    # Extract, sanitize, and sort timestamps\n",
    "    if timestamp_col in df.columns:\n",
    "        ts = pd.to_datetime(df[timestamp_col]).dropna().sort_values()\n",
    "    elif isinstance(df.index, pd.DatetimeIndex):\n",
    "        ts = pd.Series(df.index).dropna().sort_values()\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Timestamp column '{timestamp_col}' not found and index is not DatetimeIndex.\"\n",
    "        )\n",
    "\n",
    "    intervals: list[tuple[pd.Timestamp, pd.Timestamp]] = []\n",
    "    if ts.empty:\n",
    "        return intervals\n",
    "\n",
    "    t_min = ts.iloc[0]\n",
    "    t_max = ts.iloc[-1]\n",
    "    cur = t_min\n",
    "\n",
    "    while cur < t_max:\n",
    "        end = cur + W\n",
    "        if end <= t_max:\n",
    "            intervals.append((cur, end))\n",
    "        elif include_last_partial:\n",
    "            intervals.append((cur, t_max))\n",
    "            break\n",
    "        else:\n",
    "            break\n",
    "        cur = cur + S\n",
    "\n",
    "    return intervals\n",
    "\n",
    "# build intervals\n",
    "window = CONFIG[\"ENV\"][\"trading_window_days\"]\n",
    "step = CONFIG[\"ENV\"][\"sliding_window_step\"]\n",
    "timestamp_col = CONFIG[\"DATA\"][\"timestamp_col\"]\n",
    "\n",
    "intervals = build_time_intervals(\n",
    "    features_df,\n",
    "    window,\n",
    "    step,\n",
    "    timestamp_col,\n",
    "    include_last_partial=False\n",
    ")\n",
    "\n",
    "# print interval info\n",
    "print(f\"Built {len(intervals)} time intervals with window={window} and step={step}.\")\n",
    "print(\"First 3 intervals:\")\n",
    "for start, end in intervals[:3]:\n",
    "    print(f\"  {start} to {end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9febee86",
   "metadata": {},
   "source": [
    "## Identify Feature Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3148ff1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair ('AAVE', 'SUI') has 6 valid intervals out of 363 total intervals.\n",
      "Pair ('AAVE', 'TRX') has 2 valid intervals out of 363 total intervals.\n",
      "Pair ('ADA', 'BTC') has 6 valid intervals out of 363 total intervals.\n",
      "Pair ('ADA', 'DOGE') has 15 valid intervals out of 363 total intervals.\n",
      "Pair ('ADA', 'HBAR') has 25 valid intervals out of 363 total intervals.\n",
      "Pair ('ADA', 'LTC') has 7 valid intervals out of 363 total intervals.\n",
      "Pair ('ADA', 'SUI') has 7 valid intervals out of 363 total intervals.\n",
      "Pair ('ADA', 'XLM') has 9 valid intervals out of 363 total intervals.\n",
      "Pair ('ADA', 'XRP') has 11 valid intervals out of 363 total intervals.\n",
      "Pair ('APT', 'AVAX') has 6 valid intervals out of 363 total intervals.\n",
      "Pair ('ARB', 'ATOM') has 11 valid intervals out of 363 total intervals.\n",
      "Pair ('ARB', 'AVAX') has 9 valid intervals out of 363 total intervals.\n",
      "Pair ('ARB', 'DOT') has 12 valid intervals out of 363 total intervals.\n",
      "Pair ('ARB', 'ETC') has 20 valid intervals out of 363 total intervals.\n",
      "Pair ('ARB', 'ETH') has 9 valid intervals out of 363 total intervals.\n",
      "Pair ('ARB', 'NEAR') has 19 valid intervals out of 363 total intervals.\n",
      "Pair ('ARB', 'WLD') has 20 valid intervals out of 363 total intervals.\n",
      "Pair ('ATOM', 'BCH') has 6 valid intervals out of 363 total intervals.\n",
      "Pair ('ATOM', 'DOT') has 11 valid intervals out of 363 total intervals.\n",
      "Pair ('ATOM', 'ENA') has 1 valid intervals out of 363 total intervals.\n",
      "Pair ('ATOM', 'ETC') has 8 valid intervals out of 363 total intervals.\n",
      "Pair ('AVAX', 'BCH') has 6 valid intervals out of 363 total intervals.\n",
      "Pair ('AVAX', 'DOT') has 18 valid intervals out of 363 total intervals.\n",
      "Pair ('AVAX', 'ETC') has 9 valid intervals out of 363 total intervals.\n",
      "Pair ('AVAX', 'UNI') has 6 valid intervals out of 363 total intervals.\n",
      "Pair ('BCH', 'DOT') has 4 valid intervals out of 363 total intervals.\n",
      "Pair ('BCH', 'ENA') has 4 valid intervals out of 363 total intervals.\n",
      "Pair ('BCH', 'ETC') has 16 valid intervals out of 363 total intervals.\n",
      "Pair ('BNB', 'LINK') has 3 valid intervals out of 363 total intervals.\n",
      "Pair ('BTC', 'DOGE') has 18 valid intervals out of 363 total intervals.\n",
      "Pair ('BTC', 'HBAR') has 3 valid intervals out of 363 total intervals.\n",
      "Pair ('BTC', 'LTC') has 1 valid intervals out of 363 total intervals.\n",
      "Pair ('BTC', 'SUI') has 1 valid intervals out of 363 total intervals.\n",
      "Pair ('BTC', 'TRX') has 1 valid intervals out of 363 total intervals.\n",
      "Pair ('BTC', 'XLM') has 3 valid intervals out of 363 total intervals.\n",
      "Pair ('BTC', 'XRP') has 9 valid intervals out of 363 total intervals.\n",
      "Pair ('DOGE', 'LINK') has 9 valid intervals out of 363 total intervals.\n",
      "Pair ('DOGE', 'LTC') has 9 valid intervals out of 363 total intervals.\n",
      "Pair ('DOGE', 'SUI') has 3 valid intervals out of 363 total intervals.\n",
      "Pair ('DOGE', 'XLM') has 4 valid intervals out of 363 total intervals.\n",
      "Pair ('DOT', 'ENA') has 6 valid intervals out of 363 total intervals.\n",
      "Pair ('DOT', 'ETC') has 19 valid intervals out of 363 total intervals.\n",
      "Pair ('DOT', 'ETH') has 4 valid intervals out of 363 total intervals.\n",
      "Pair ('ENA', 'ETC') has 5 valid intervals out of 363 total intervals.\n",
      "Pair ('ENA', 'UNI') has 7 valid intervals out of 363 total intervals.\n",
      "Pair ('ETC', 'ETH') has 15 valid intervals out of 363 total intervals.\n",
      "Pair ('ETC', 'NEAR') has 16 valid intervals out of 363 total intervals.\n",
      "Pair ('ETH', 'NEAR') has 1 valid intervals out of 363 total intervals.\n",
      "Pair ('HBAR', 'LTC') has 2 valid intervals out of 363 total intervals.\n",
      "Pair ('HBAR', 'XLM') has 12 valid intervals out of 363 total intervals.\n",
      "Pair ('HBAR', 'XRP') has 7 valid intervals out of 363 total intervals.\n",
      "Pair ('LINK', 'LTC') has 5 valid intervals out of 363 total intervals.\n",
      "Pair ('LINK', 'UNI') has 4 valid intervals out of 363 total intervals.\n",
      "Pair ('LTC', 'XLM') has 7 valid intervals out of 363 total intervals.\n",
      "Pair ('NEAR', 'TON') has 0 valid intervals out of 363 total intervals.\n",
      "Pair ('NEAR', 'WLD') has 23 valid intervals out of 363 total intervals.\n",
      "Pair ('SUI', 'TRX') has 1 valid intervals out of 363 total intervals.\n",
      "Pair ('SUI', 'XLM') has 4 valid intervals out of 363 total intervals.\n",
      "Pair ('TRX', 'XLM') has 1 valid intervals out of 363 total intervals.\n",
      "Pair ('TRX', 'XRP') has 1 valid intervals out of 363 total intervals.\n",
      "Pair ('XLM', 'XRP') has 9 valid intervals out of 363 total intervals.\n"
     ]
    }
   ],
   "source": [
    "def is_timeframe_valid(\n",
    "    df: pd.DataFrame,\n",
    "    pair: tuple[str, str],\n",
    "    start: pd.Timestamp,\n",
    "    end: pd.Timestamp,\n",
    "    feature_name: str,\n",
    "    pair_feature_format: str = \"{ASSET1}_{ASSET2}_{FEATURE}\",\n",
    "    timestamp_col: str | None = \"datetime\",\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Check if the given time frame has complete data for the specified asset pair.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing the data.\n",
    "    pair : tuple[str, str]\n",
    "        Asset pair (asset1, asset2).\n",
    "    start : pd.Timestamp\n",
    "        Start of the time frame (inclusive).\n",
    "    end : pd.Timestamp\n",
    "        End of the time frame (exclusive).\n",
    "    pair_feature_format : str\n",
    "        Format string for pair-feature columns (e.g., \"{ASSET1}_{ASSET2}_{FEATURE}\").\n",
    "    timestamp_col : str\n",
    "        Name of the timestamp column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the time frame is valid (no missing data), False otherwise.\n",
    "    \"\"\"\n",
    "    feature_col = pair_feature_format.format(\n",
    "        ASSET1=pair[0], ASSET2=pair[1], FEATURE=feature_name\n",
    "    )\n",
    "\n",
    "    if timestamp_col and timestamp_col in df.columns:\n",
    "        ts = df[timestamp_col]\n",
    "    else:\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            raise ValueError(\n",
    "                \"DataFrame neither has the timestamp column nor a DatetimeIndex.\"\n",
    "            )\n",
    "        ts = df.index\n",
    "\n",
    "    mask = (ts >= start) & (ts < end)\n",
    "    if mask.sum() == 0:\n",
    "        return False\n",
    "\n",
    "    data_slice = df.loc[mask, feature_col]\n",
    "    return not data_slice.isna().any()\n",
    "\n",
    "\n",
    "pair_identifier = CONFIG[\"DATA\"][\"features\"][\"pair_identifier\"]\n",
    "\n",
    "valid_intervals_per_pair = {}\n",
    "for pair in asset_pairs:\n",
    "    valid_intervals = []\n",
    "    for start, end in intervals:\n",
    "        if is_timeframe_valid(\n",
    "            features_df,\n",
    "            pair,\n",
    "            start,\n",
    "            end,\n",
    "            feature_name=pair_identifier,\n",
    "            pair_feature_format=pair_feature_format,\n",
    "            timestamp_col=timestamp_col):\n",
    "\n",
    "            valid_intervals.append((start, end))\n",
    "    valid_intervals_per_pair[pair] = valid_intervals\n",
    "    print(f\"Pair {pair} has {len(valid_intervals)} valid intervals out of {len(intervals)} total intervals.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab98fb28",
   "metadata": {},
   "source": [
    "## Dataset Structure Overview\n",
    "\n",
    "**Important**: The dataset has a specific structure:\n",
    "- **Multiple asset pairs**: Each pair trades independently\n",
    "- **2-day trading windows**: Each episode is exactly 2 days long\n",
    "- **Not continuous**: Windows are discrete episodes, not a continuous time series\n",
    "- **Strategy resets**: At the start of each 2-day window, the strategy resets to neutral position\n",
    "\n",
    "This structure allows the model to learn from diverse trading scenarios across different pairs and time periods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dea78b",
   "metadata": {},
   "source": [
    "## Rule-Based Pairs Trading Strategy\n",
    "\n",
    "This section implements a simple rule-based statistical arbitrage strategy:\n",
    "- When z-score crosses above entry threshold: short rich asset, long poor asset\n",
    "- When z-score crosses below -entry threshold: long rich asset, short poor asset\n",
    "- When z-score crosses back under exit threshold (from above or below): close all positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda2f43e",
   "metadata": {},
   "source": [
    "Action -1 (Short Spread): \n",
    "When z_score > 1.5, short first asset, long second asset\n",
    "\n",
    "Action 0 (Neutral):\n",
    "When |z_score| < 0.5, closed positions, previous action is persisted\n",
    "\n",
    "Action +1 (Long Spread):\n",
    "When z_score < -1.5, long first asset, short second asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eb0fe5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing rule-based strategy:\n",
      "Z-score -> Action (Position)\n",
      "    0.0 ->  0\n",
      "    2.0 -> -1\n",
      "    1.8 -> -1\n",
      "    0.8 -> -1\n",
      "    0.3 ->  0\n",
      "    0.7 ->  0\n",
      "   -0.2 ->  0\n",
      "   -2.0 ->  1\n",
      "   -1.5 ->  1\n",
      "    0.4 ->  0\n",
      "    0.1 ->  0\n"
     ]
    }
   ],
   "source": [
    "class RuleBasedPairsStrategy:\n",
    "    \"\"\"\n",
    "    Rule-based pairs trading strategy for generating expert demonstrations.\n",
    "    \n",
    "    Strategy rules:\n",
    "    - When z-score > entry_threshold: short rich asset, long poor asset (action = -1)\n",
    "    - When z-score < -entry_threshold: long rich asset, short poor asset (action = +1)\n",
    "    - When |z-score| < exit_threshold: close all positions (action = 0)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, entry_threshold: float = 1.5, exit_threshold: float = 0.5):\n",
    "        self.entry_threshold = entry_threshold\n",
    "        self.exit_threshold = exit_threshold\n",
    "        self.position = 0  # -1: short spread, 0: neutral, +1: long spread\n",
    "        \n",
    "    def get_action(self, z_score: float) -> int:\n",
    "        \"\"\"\n",
    "        Determine trading action based on z-score.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        z_score : float\n",
    "            Current z-score of the spread\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Action: -1 (short spread), 0 (neutral), +1 (long spread)\n",
    "        \"\"\"\n",
    "        # Entry signals\n",
    "        if z_score > self.entry_threshold:\n",
    "            # Spread is too high -> short the spread (expect mean reversion)\n",
    "            self.position = -1\n",
    "        elif z_score < -self.entry_threshold:\n",
    "            # Spread is too low -> long the spread (expect mean reversion)\n",
    "            self.position = 1\n",
    "        # Exit signals\n",
    "        elif abs(z_score) < self.exit_threshold:\n",
    "            # Spread has reverted to mean -> close position\n",
    "            self.position = 0\n",
    "        # else: maintain current position\n",
    "        \n",
    "        return self.position\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the strategy state.\"\"\"\n",
    "        self.position = 0\n",
    "\n",
    "\n",
    "# Test the strategy\n",
    "strategy = RuleBasedPairsStrategy(\n",
    "    entry_threshold=CONFIG[\"Strategy\"][\"entry\"],\n",
    "    exit_threshold=CONFIG[\"Strategy\"][\"exit\"]\n",
    ")\n",
    "\n",
    "# Test with sample z-scores\n",
    "test_z_scores = [0.0, 2.0, 1.8, 0.8, 0.3, 0.7, -0.2, -2.0, -1.5, 0.4, 0.1]\n",
    "print(\"Testing rule-based strategy:\")\n",
    "print(\"Z-score -> Action (Position)\")\n",
    "for z in test_z_scores:\n",
    "    action = strategy.get_action(z)\n",
    "    print(f\"  {z:5.1f} -> {action:2d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04784125",
   "metadata": {},
   "source": [
    "## Generate Offline Dataset from Rule-Based Strategy\n",
    "\n",
    "This section generates an offline dataset by applying the rule-based strategy to historical data. \n",
    "\n",
    "**Important**: We store data per pair and per interval, NOT as a combined dataset. During training, we'll iterate through pairs and their intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "299c179f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Organizing Data by Pair and Interval\n",
      "============================================================\n",
      "\n",
      "Processing pair ('AAVE', 'SUI') with 6 valid 2-day windows...\n",
      "  ✓ Stored 6 valid intervals for pair ('AAVE', 'SUI')\n",
      "\n",
      "Processing pair ('AAVE', 'TRX') with 2 valid 2-day windows...\n",
      "  ✓ Stored 2 valid intervals for pair ('AAVE', 'TRX')\n",
      "\n",
      "Processing pair ('ADA', 'BTC') with 6 valid 2-day windows...\n",
      "  ✓ Stored 6 valid intervals for pair ('ADA', 'BTC')\n",
      "\n",
      "Processing pair ('ADA', 'DOGE') with 15 valid 2-day windows...\n",
      "  ✓ Stored 15 valid intervals for pair ('ADA', 'DOGE')\n",
      "\n",
      "Processing pair ('ADA', 'HBAR') with 25 valid 2-day windows...\n",
      "  ✓ Stored 25 valid intervals for pair ('ADA', 'HBAR')\n",
      "\n",
      "Processing pair ('ADA', 'LTC') with 7 valid 2-day windows...\n",
      "  ✓ Stored 7 valid intervals for pair ('ADA', 'LTC')\n",
      "\n",
      "Processing pair ('ADA', 'SUI') with 7 valid 2-day windows...\n",
      "  ✓ Stored 7 valid intervals for pair ('ADA', 'SUI')\n",
      "\n",
      "Processing pair ('ADA', 'XLM') with 9 valid 2-day windows...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 419\u001b[39m\n\u001b[32m    416\u001b[39m valid_count = \u001b[32m0\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m interval_idx, interval \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(valid_ints):\n\u001b[32m    418\u001b[39m     \u001b[38;5;66;03m# Generate dataset for this specific interval\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m     interval_data = \u001b[43mgenerate_interval_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeatures_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m        \u001b[49m\u001b[43mz_score_feature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mz_score_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpair_feature_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpair_feature_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m        \u001b[49m\u001b[43msingle_asset_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43msingle_asset_features\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use first 5 features\u001b[39;49;00m\n\u001b[32m    427\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimestamp_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimestamp_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    430\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m interval_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(interval_data[\u001b[33m\"\u001b[39m\u001b[33mobservations\u001b[39m\u001b[33m\"\u001b[39m]) > \u001b[32m0\u001b[39m:\n\u001b[32m    431\u001b[39m         pair_interval_data[pair][interval_idx] = {\n\u001b[32m    432\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m: interval_data,\n\u001b[32m    433\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33minterval\u001b[39m\u001b[33m\"\u001b[39m: interval,\n\u001b[32m    434\u001b[39m         }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mgenerate_interval_dataset\u001b[39m\u001b[34m(df, pair, interval, strategy, z_score_feature, pair_feature_format, single_asset_features, timestamp_col)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Generate state-action pairs for this interval\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m interval_df.iterrows():\n\u001b[32m     67\u001b[39m     \u001b[38;5;66;03m# Get state\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     state = \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate_cols\u001b[49m\u001b[43m]\u001b[49m.values.astype(np.float32)\n\u001b[32m     70\u001b[39m     \u001b[38;5;66;03m# Skip if any NaN in state\u001b[39;00m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m np.any(np.isnan(state)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/series.py:1165\u001b[39m, in \u001b[36mSeries.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1162\u001b[39m     key = np.asarray(key, dtype=\u001b[38;5;28mbool\u001b[39m)\n\u001b[32m   1163\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_rows_with_mask(key)\n\u001b[32m-> \u001b[39m\u001b[32m1165\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/series.py:1206\u001b[39m, in \u001b[36mSeries._get_with\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iloc[key]\n\u001b[32m   1205\u001b[39m \u001b[38;5;66;03m# handle the dup indexing case GH#4246\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1206\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/indexing.py:1192\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1190\u001b[39m maybe_callable = com.apply_if_callable(key, \u001b[38;5;28mself\u001b[39m.obj)\n\u001b[32m   1191\u001b[39m maybe_callable = \u001b[38;5;28mself\u001b[39m._check_deprecated_callable_usage(key, maybe_callable)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/indexing.py:1421\u001b[39m, in \u001b[36m_LocIndexer._getitem_axis\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1418\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[33m\"\u001b[39m\u001b[33mndim\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key.ndim > \u001b[32m1\u001b[39m:\n\u001b[32m   1419\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot index with multidimensional key\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1421\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1423\u001b[39m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[32m   1424\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/indexing.py:1361\u001b[39m, in \u001b[36m_LocIndexer._getitem_iterable\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1358\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_key(key, axis)\n\u001b[32m   1360\u001b[39m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1361\u001b[39m keyarr, indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj._reindex_with_indexers(\n\u001b[32m   1363\u001b[39m     {axis: [keyarr, indexer]}, copy=\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1364\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/indexing.py:1559\u001b[39m, in \u001b[36m_LocIndexer._get_listlike_indexer\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1556\u001b[39m ax = \u001b[38;5;28mself\u001b[39m.obj._get_axis(axis)\n\u001b[32m   1557\u001b[39m axis_name = \u001b[38;5;28mself\u001b[39m.obj._get_axis_name(axis)\n\u001b[32m-> \u001b[39m\u001b[32m1559\u001b[39m keyarr, indexer = \u001b[43max\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1561\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/indexes/base.py:6208\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6206\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._index_as_unique:\n\u001b[32m   6207\u001b[39m     indexer = \u001b[38;5;28mself\u001b[39m.get_indexer_for(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6208\u001b[39m     keyarr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/indexes/base.py:4410\u001b[39m, in \u001b[36mIndex.reindex\u001b[39m\u001b[34m(self, target, method, level, limit, tolerance)\u001b[39m\n\u001b[32m   4408\u001b[39m     target = idx[:\u001b[32m0\u001b[39m]\n\u001b[32m   4409\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4410\u001b[39m     target = \u001b[43mensure_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4412\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m   4413\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCMultiIndex) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target, ABCMultiIndex)\n\u001b[32m   4414\u001b[39m ):\n\u001b[32m   4415\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/indexes/base.py:7715\u001b[39m, in \u001b[36mensure_index\u001b[39m\u001b[34m(index_like, copy)\u001b[39m\n\u001b[32m   7713\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m Index(index_like, copy=copy, tupleize_cols=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   7714\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m7715\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/indexes/base.py:485\u001b[39m, in \u001b[36mIndex.__new__\u001b[39m\u001b[34m(cls, data, dtype, copy, name, tupleize_cols)\u001b[39m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__new__\u001b[39m(\n\u001b[32m    476\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m    477\u001b[39m     data=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    481\u001b[39m     tupleize_cols: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    482\u001b[39m ) -> Self:\n\u001b[32m    483\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrange\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RangeIndex\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m     name = \u001b[43mmaybe_extract_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    487\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    488\u001b[39m         dtype = pandas_dtype(dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/indexes/base.py:7754\u001b[39m, in \u001b[36mmaybe_extract_name\u001b[39m\u001b[34m(name, obj, cls)\u001b[39m\n\u001b[32m   7750\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mright\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minner\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mouter\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   7751\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdo not recognize join method \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m7754\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmaybe_extract_name\u001b[39m(name, obj, \u001b[38;5;28mcls\u001b[39m) -> Hashable:\n\u001b[32m   7755\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   7756\u001b[39m \u001b[33;03m    If no name is passed, then extract it from data, validating hashability.\u001b[39;00m\n\u001b[32m   7757\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   7758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (Index, ABCSeries)):\n\u001b[32m   7759\u001b[39m         \u001b[38;5;66;03m# Note we don't just check for \"name\" attribute since that would\u001b[39;00m\n\u001b[32m   7760\u001b[39m         \u001b[38;5;66;03m#  pick up e.g. dtype.name\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def generate_interval_dataset(\n",
    "    df: pd.DataFrame,\n",
    "    pair: tuple[str, str],\n",
    "    interval: tuple[pd.Timestamp, pd.Timestamp],\n",
    "    strategy: RuleBasedPairsStrategy,\n",
    "    z_score_feature: str = \"spreadNorm\",\n",
    "    pair_feature_format: str = \"{ASSET1}_{ASSET2}_{FEATURE}\",\n",
    "    single_asset_features: list[str] = None,\n",
    "    timestamp_col: str = \"datetime\",\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Generate dataset for a SINGLE interval (2-day window) for a pair.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict with keys:\n",
    "        - observations: np.ndarray of shape (N, obs_dim)\n",
    "        - actions: np.ndarray of shape (N,)\n",
    "        - rewards: np.ndarray of shape (N,)\n",
    "        - timestamps: list of timestamps\n",
    "    \"\"\"\n",
    "    observations = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    timestamps_list = []\n",
    "    \n",
    "    # Construct column names\n",
    "    z_score_col = pair_feature_format.format(\n",
    "        ASSET1=pair[0], ASSET2=pair[1], FEATURE=z_score_feature\n",
    "    )\n",
    "    \n",
    "    # Get single asset feature columns for state representation\n",
    "    state_cols = []\n",
    "    if single_asset_features:\n",
    "        for asset in pair:\n",
    "            for feat in single_asset_features:\n",
    "                col = f\"{asset}_{feat}\"\n",
    "                if col in df.columns:\n",
    "                    state_cols.append(col)\n",
    "    \n",
    "    # Add z-score to state\n",
    "    state_cols.append(z_score_col)\n",
    "    \n",
    "    # Get data for this interval\n",
    "    start, end = interval\n",
    "    if timestamp_col and timestamp_col in df.columns:\n",
    "        ts = df[timestamp_col]\n",
    "    else:\n",
    "        ts = df.index\n",
    "        \n",
    "    mask = (ts >= start) & (ts < end)\n",
    "    interval_df = df[mask].copy()\n",
    "    \n",
    "    if interval_df.empty or z_score_col not in interval_df.columns:\n",
    "        return None\n",
    "        \n",
    "    # Check if we have all required columns\n",
    "    missing_cols = [col for col in state_cols if col not in interval_df.columns]\n",
    "    if missing_cols:\n",
    "        return None\n",
    "    \n",
    "    # Reset strategy for this interval\n",
    "    strategy.reset()\n",
    "    \n",
    "    # Generate state-action pairs for this interval\n",
    "    for idx, row in interval_df.iterrows():\n",
    "        # Get state\n",
    "        state = row[state_cols].values.astype(np.float32)\n",
    "        \n",
    "        # Skip if any NaN in state\n",
    "        if np.any(np.isnan(state)):\n",
    "            continue\n",
    "        \n",
    "        # Get z-score and determine action\n",
    "        z_score = row[z_score_col]\n",
    "        action = strategy.get_action(z_score)\n",
    "        \n",
    "        # Simple reward: negative absolute z-score (encourage mean reversion)\n",
    "        reward = -abs(z_score)\n",
    "        \n",
    "        observations.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        if timestamp_col and timestamp_col in df.columns:\n",
    "            timestamps_list.append(row[timestamp_col])\n",
    "        else:\n",
    "            timestamps_list.append(idx)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    if len(observations) == 0:\n",
    "        return None\n",
    "        \n",
    "    return {\n",
    "        \"observations\": np.array(observations, dtype=np.float32),\n",
    "        \"actions\": np.array(actions, dtype=np.int32),\n",
    "        \"rewards\": np.array(rewards, dtype=np.float32),\n",
    "        \"timestamps\": timestamps_list,\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_offline_dataset(\n",
    "    df: pd.DataFrame,\n",
    "    pair: tuple[str, str],\n",
    "    intervals: list[tuple[pd.Timestamp, pd.Timestamp]],\n",
    "    strategy: RuleBasedPairsStrategy,\n",
    "    z_score_feature: str = \"spreadNorm\",\n",
    "    pair_feature_format: str = \"{ASSET1}_{ASSET2}_{FEATURE}\",\n",
    "    single_asset_features: list[str] = None,\n",
    "    timestamp_col: str = \"datetime\",\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Generate offline dataset using rule-based strategy.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict with keys:\n",
    "        - observations: np.ndarray of shape (N, obs_dim)\n",
    "        - actions: np.ndarray of shape (N,)\n",
    "        - rewards: np.ndarray of shape (N,)\n",
    "        - terminals: np.ndarray of shape (N,)\n",
    "        - timestamps: list of timestamps\n",
    "    \"\"\"\n",
    "    \n",
    "    observations = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    terminals = []\n",
    "    timestamps_list = []\n",
    "    \n",
    "    # Construct column names\n",
    "    z_score_col = pair_feature_format.format(\n",
    "        ASSET1=pair[0], ASSET2=pair[1], FEATURE=z_score_feature\n",
    "    )\n",
    "    \n",
    "    # Get single asset feature columns for state representation\n",
    "    state_cols = []\n",
    "    if single_asset_features:\n",
    "        for asset in pair:\n",
    "            for feat in single_asset_features:\n",
    "                col = f\"{asset}_{feat}\"\n",
    "                if col in df.columns:\n",
    "                    state_cols.append(col)\n",
    "    \n",
    "    # Add z-score to state\n",
    "    state_cols.append(z_score_col)\n",
    "    \n",
    "    print(f\"\\nGenerating dataset for pair {pair}\")\n",
    "    print(f\"State features ({len(state_cols)}): {state_cols[:5]}...\")\n",
    "    \n",
    "    # Process each interval\n",
    "    for start, end in intervals:\n",
    "        # Get data for this interval\n",
    "        if timestamp_col and timestamp_col in df.columns:\n",
    "            ts = df[timestamp_col]\n",
    "        else:\n",
    "            ts = df.index\n",
    "            \n",
    "        mask = (ts >= start) & (ts < end)\n",
    "        interval_df = df[mask].copy()\n",
    "        \n",
    "        if interval_df.empty or z_score_col not in interval_df.columns:\n",
    "            continue\n",
    "            \n",
    "        # Check if we have all required columns\n",
    "        missing_cols = [col for col in state_cols if col not in interval_df.columns]\n",
    "        if missing_cols:\n",
    "            continue\n",
    "        \n",
    "        # Reset strategy for new interval\n",
    "        strategy.reset()\n",
    "        \n",
    "        # Generate state-action pairs\n",
    "        for idx, row in interval_df.iterrows():\n",
    "            # Get state\n",
    "            state = row[state_cols].values.astype(np.float32)\n",
    "            \n",
    "            # Skip if any NaN in state\n",
    "            if np.any(np.isnan(state)):\n",
    "                continue\n",
    "            \n",
    "            # Get z-score and determine action\n",
    "            z_score = row[z_score_col]\n",
    "            action = strategy.get_action(z_score)\n",
    "            \n",
    "            # Simple reward: negative absolute z-score (encourage mean reversion)\n",
    "            reward = -abs(z_score)\n",
    "            \n",
    "            observations.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            terminals.append(False)  # Only last step in interval is terminal\n",
    "            \n",
    "            if timestamp_col and timestamp_col in df.columns:\n",
    "                timestamps_list.append(row[timestamp_col])\n",
    "            else:\n",
    "                timestamps_list.append(idx)\n",
    "        \n",
    "        # Mark last step as terminal\n",
    "        if len(terminals) > 0:\n",
    "            terminals[-1] = True\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    observations = np.array(observations, dtype=np.float32)\n",
    "    actions = np.array(actions, dtype=np.int32)\n",
    "    rewards = np.array(rewards, dtype=np.float32)\n",
    "    terminals = np.array(terminals, dtype=bool)\n",
    "    \n",
    "    print(f\"Generated {len(observations)} transitions\")\n",
    "    print(f\"  Observations shape: {observations.shape}\")\n",
    "    print(f\"  Actions distribution: {np.bincount(actions + 1)}\")  # +1 to handle -1,0,1\n",
    "    print(f\"  Rewards range: [{rewards.min():.3f}, {rewards.max():.3f}]\")\n",
    "    \n",
    "    return {\n",
    "        \"observations\": observations,\n",
    "        \"actions\": actions,\n",
    "        \"rewards\": rewards,\n",
    "        \"terminals\": terminals,\n",
    "        \"timestamps\": timestamps_list,\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_interval_dataset_with_pnl(\n",
    "    df: pd.DataFrame,\n",
    "    pair: tuple[str, str],\n",
    "    interval: tuple[pd.Timestamp, pd.Timestamp],\n",
    "    strategy: RuleBasedPairsStrategy,\n",
    "    transaction_cost_bps: float = 3.5,\n",
    "    z_score_feature: str = \"spreadNorm\",\n",
    "    pair_feature_format: str = \"{ASSET1}_{ASSET2}_{FEATURE}\",\n",
    "    single_asset_features: list[str] = None,\n",
    "    timestamp_col: str = \"datetime\",\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Generate dataset with PnL-based rewards (portfolio returns - transaction costs).\n",
    "    \n",
    "    Reward calculation:\n",
    "    - Observe state_t, take action_t\n",
    "    - Reward_t = portfolio return from t to t+1 based on action_t, minus transaction costs\n",
    "    - Action = -1 (short spread): short asset1, long asset2\n",
    "    - Action = 1 (long spread): long asset1, short asset2  \n",
    "    - Action = 0 (neutral): no position, return = 0\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict with keys:\n",
    "        - observations: np.ndarray of shape (N, obs_dim)\n",
    "        - actions: np.ndarray of shape (N,)\n",
    "        - rewards: np.ndarray of shape (N,)\n",
    "        - timestamps: list of timestamps\n",
    "    \"\"\"\n",
    "    # First pass: collect all states and actions\n",
    "    states_list = []\n",
    "    actions_list = []\n",
    "    prices1_list = []\n",
    "    prices2_list = []\n",
    "    timestamps_list = []\n",
    "    \n",
    "    # Construct column names\n",
    "    z_score_col = pair_feature_format.format(\n",
    "        ASSET1=pair[0], ASSET2=pair[1], FEATURE=z_score_feature\n",
    "    )\n",
    "    \n",
    "    # Get price columns for each asset\n",
    "    asset1_price_col = f\"{pair[0]}_close\"\n",
    "    asset2_price_col = f\"{pair[1]}_close\"\n",
    "    \n",
    "    # Get single asset feature columns for state representation\n",
    "    state_cols = []\n",
    "    if single_asset_features:\n",
    "        for asset in pair:\n",
    "            for feat in single_asset_features:\n",
    "                col = f\"{asset}_{feat}\"\n",
    "                if col in df.columns:\n",
    "                    state_cols.append(col)\n",
    "    \n",
    "    # Add z-score to state\n",
    "    state_cols.append(z_score_col)\n",
    "    \n",
    "    # Get data for this interval\n",
    "    start, end = interval\n",
    "    if timestamp_col and timestamp_col in df.columns:\n",
    "        ts = df[timestamp_col]\n",
    "    else:\n",
    "        ts = df.index\n",
    "        \n",
    "    mask = (ts >= start) & (ts < end)\n",
    "    interval_df = df[mask].copy()\n",
    "    \n",
    "    if interval_df.empty or z_score_col not in interval_df.columns:\n",
    "        return None\n",
    "        \n",
    "    # Check if we have all required columns\n",
    "    missing_cols = [col for col in state_cols if col not in interval_df.columns]\n",
    "    if missing_cols or asset1_price_col not in interval_df.columns or asset2_price_col not in interval_df.columns:\n",
    "        return None\n",
    "    \n",
    "    # Reset strategy for this interval\n",
    "    strategy.reset()\n",
    "    \n",
    "    # First pass: collect states, actions, and prices\n",
    "    for idx, row in interval_df.iterrows():\n",
    "        # Get state\n",
    "        state = row[state_cols].values.astype(np.float32)\n",
    "        \n",
    "        # Skip if any NaN in state\n",
    "        if np.any(np.isnan(state)):\n",
    "            continue\n",
    "        \n",
    "        # Get current prices\n",
    "        price1 = row[asset1_price_col]\n",
    "        price2 = row[asset2_price_col]\n",
    "        \n",
    "        if np.isnan(price1) or np.isnan(price2):\n",
    "            continue\n",
    "        \n",
    "        # Get z-score and determine action\n",
    "        z_score = row[z_score_col]\n",
    "        action = strategy.get_action(z_score)\n",
    "        \n",
    "        states_list.append(state)\n",
    "        actions_list.append(action)\n",
    "        prices1_list.append(price1)\n",
    "        prices2_list.append(price2)\n",
    "        \n",
    "        if timestamp_col and timestamp_col in df.columns:\n",
    "            timestamps_list.append(row[timestamp_col])\n",
    "        else:\n",
    "            timestamps_list.append(idx)\n",
    "    \n",
    "    if len(states_list) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Second pass: calculate rewards\n",
    "    # Reward[t] = return from taking action[t] at time t, observing price change to t+1\n",
    "    rewards_list = []\n",
    "    prev_action = 0  # Track previous action for transaction costs\n",
    "    \n",
    "    for t in range(len(states_list)):\n",
    "        action_t = actions_list[t]\n",
    "        \n",
    "        # Check if we have next prices to calculate return\n",
    "        if t < len(states_list) - 1:\n",
    "            # Calculate returns from t to t+1\n",
    "            price1_t = prices1_list[t]\n",
    "            price2_t = prices2_list[t]\n",
    "            price1_next = prices1_list[t + 1]\n",
    "            price2_next = prices2_list[t + 1]\n",
    "            \n",
    "            ret1 = (price1_next - price1_t) / price1_t if price1_t != 0 else 0\n",
    "            ret2 = (price2_next - price2_t) / price2_t if price2_t != 0 else 0\n",
    "            \n",
    "            # Calculate portfolio return based on action taken at t\n",
    "            if action_t == -1:\n",
    "                # Short spread: short asset1, long asset2\n",
    "                portfolio_return = -ret1 + ret2\n",
    "            elif action_t == 1:\n",
    "                # Long spread: long asset1, short asset2\n",
    "                portfolio_return = ret1 - ret2\n",
    "            else:\n",
    "                # No position\n",
    "                portfolio_return = 0.0\n",
    "            \n",
    "            # Calculate transaction costs (applied when position changes)\n",
    "            transaction_cost = 0.0\n",
    "            if action_t != prev_action:\n",
    "                # Position change: pay transaction cost on both legs\n",
    "                transaction_cost = 2 * (transaction_cost_bps / 10000)\n",
    "            \n",
    "            # Final reward = portfolio return - transaction costs\n",
    "            reward = portfolio_return - transaction_cost\n",
    "        else:\n",
    "            # Last observation: no next price, reward = 0\n",
    "            reward = 0.0\n",
    "        \n",
    "        rewards_list.append(reward)\n",
    "        prev_action = action_t\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    observations = np.array(states_list, dtype=np.float32)\n",
    "    actions = np.array(actions_list, dtype=np.int32)\n",
    "    rewards = np.array(rewards_list, dtype=np.float32)\n",
    "        \n",
    "    return {\n",
    "        \"observations\": observations,\n",
    "        \"actions\": actions,\n",
    "        \"rewards\": rewards,\n",
    "        \"timestamps\": timestamps_list,\n",
    "    }\n",
    "\n",
    "\n",
    "# Store data organized by pair and interval (NOT combined into one dataset)\n",
    "z_score_feature = CONFIG[\"Strategy\"][\"z-score\"]\n",
    "entry_threshold = CONFIG[\"Strategy\"][\"entry\"]\n",
    "exit_threshold = CONFIG[\"Strategy\"][\"exit\"]\n",
    "\n",
    "# Create strategy\n",
    "strategy = RuleBasedPairsStrategy(entry_threshold, exit_threshold)\n",
    "\n",
    "# Store data per pair-interval for iteration during training\n",
    "pair_interval_data = {}  # {pair: {interval_idx: data}}\n",
    "total_intervals = 0\n",
    "pairs_with_data = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Organizing Data by Pair and Interval\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for pair, valid_ints in valid_intervals_per_pair.items():\n",
    "    if len(valid_ints) == 0:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nProcessing pair {pair} with {len(valid_ints)} valid 2-day windows...\")\n",
    "    pair_interval_data[pair] = {}\n",
    "    \n",
    "    valid_count = 0\n",
    "    for interval_idx, interval in enumerate(valid_ints):\n",
    "        # Generate dataset for this specific interval\n",
    "        interval_data = generate_interval_dataset(\n",
    "            features_df,\n",
    "            pair,\n",
    "            interval,\n",
    "            strategy,\n",
    "            z_score_feature=z_score_feature,\n",
    "            pair_feature_format=pair_feature_format,\n",
    "            single_asset_features=single_asset_features[:5],  # Use first 5 features\n",
    "            timestamp_col=timestamp_col,\n",
    "        )\n",
    "        \n",
    "        if interval_data is not None and len(interval_data[\"observations\"]) > 0:\n",
    "            pair_interval_data[pair][interval_idx] = {\n",
    "                \"data\": interval_data,\n",
    "                \"interval\": interval,\n",
    "            }\n",
    "            valid_count += 1\n",
    "    \n",
    "    if valid_count > 0:\n",
    "        pairs_with_data.append(pair)\n",
    "        total_intervals += valid_count\n",
    "        print(f\"  ✓ Stored {valid_count} valid intervals for pair {pair}\")\n",
    "    else:\n",
    "        # Remove pair if no valid data\n",
    "        del pair_interval_data[pair]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Data Organization Summary\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total pairs with data: {len(pairs_with_data)}\")\n",
    "print(f\"Total 2-day intervals: {total_intervals}\")\n",
    "print(f\"Pairs: {pairs_with_data}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66879137",
   "metadata": {},
   "source": [
    "## Dataset Statistics\n",
    "\n",
    "Let's compute some statistics about our organized data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e53dba",
   "metadata": {},
   "source": [
    "## Example Dataset Inspection\n",
    "\n",
    "Let's look at an example dataset to see what the data and actions look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c53d2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(pair_interval_data) > 0:\n",
    "    # Get the first pair and first interval\n",
    "    example_pair = list(pair_interval_data.keys())[0]\n",
    "    example_interval_idx = list(pair_interval_data[example_pair].keys())[0]\n",
    "    example_data = pair_interval_data[example_pair][example_interval_idx][\"data\"]\n",
    "    example_interval = pair_interval_data[example_pair][example_interval_idx][\"interval\"]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Example Dataset from Pair: {example_pair}\")\n",
    "    print(f\"Interval: {example_interval[0]} to {example_interval[1]}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Create a DataFrame for better visualization\n",
    "    example_df = pd.DataFrame({\n",
    "        'timestamp': example_data['timestamps'][:20],\n",
    "        'observation_0': example_data['observations'][:20, 0],\n",
    "        'observation_1': example_data['observations'][:20, 1] if example_data['observations'].shape[1] > 1 else None,\n",
    "        'observation_last': example_data['observations'][:20, -1],  # Last feature (z-score)\n",
    "        'action': example_data['actions'][:20],\n",
    "        'reward': example_data['rewards'][:20],\n",
    "    })\n",
    "    \n",
    "    print(\"First 20 samples from this interval:\")\n",
    "    print(example_df.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Action Encoding:\")\n",
    "    print(\"  -1 = Short Spread (z-score too high, expect mean reversion down)\")\n",
    "    print(\"   0 = Neutral (close positions, z-score near mean)\")\n",
    "    print(\"  +1 = Long Spread (z-score too low, expect mean reversion up)\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Show action counts for this example\n",
    "    action_counts = np.bincount(example_data['actions'] + 1)\n",
    "    print(\"Action distribution in this example interval:\")\n",
    "    print(f\"  Short (-1): {action_counts[0]} samples\")\n",
    "    print(f\"  Neutral (0): {action_counts[1]} samples\")\n",
    "    print(f\"  Long (+1): {action_counts[2]} samples\")\n",
    "    print(f\"  Total: {len(example_data['actions'])} samples\")\n",
    "    \n",
    "else:\n",
    "    print(\"No data available to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d179183f",
   "metadata": {},
   "source": [
    "### Strategy Design Decisions\n",
    "\n",
    "**Current Implementation: Discrete Actions**\n",
    "- Actions: {-1, 0, 1} (Full Short, Neutral, Full Long)\n",
    "- Simple and interpretable\n",
    "- Uses DiscreteCQL algorithm\n",
    "\n",
    "**Alternative: Continuous Actions (Partial Positions)**\n",
    "To allow fractional positions (e.g., 0.5 = half position, -0.3 = 30% short):\n",
    "\n",
    "1. **Modify Strategy to Output Continuous Actions:**\n",
    "   ```python\n",
    "   def get_continuous_action(self, z_score):\n",
    "       \"\"\"Returns action in [-1, 1] based on z-score strength.\"\"\"\n",
    "       if z_score > self.entry_threshold:\n",
    "           # Stronger signal → larger short position\n",
    "           strength = min((z_score - self.entry_threshold) / self.entry_threshold, 1.0)\n",
    "           return -strength  # e.g., -0.5 for moderate signal\n",
    "       elif z_score < -self.entry_threshold:\n",
    "           strength = min((-z_score - self.entry_threshold) / self.entry_threshold, 1.0)\n",
    "           return strength   # e.g., 0.7 for strong signal\n",
    "       else:\n",
    "           return 0.0  # Neutral\n",
    "   ```\n",
    "\n",
    "2. **Switch from DiscreteCQL to Continuous CQL:**\n",
    "   ```python\n",
    "   from d3rlpy.algos import CQLConfig  # Continuous version\n",
    "   \n",
    "   cql = CQLConfig(\n",
    "       learning_rate=3e-4,\n",
    "       batch_size=256,\n",
    "       actor_learning_rate=3e-4,  # Additional: actor network learning rate\n",
    "       alpha=5.0,\n",
    "       # ... other params\n",
    "   ).create(device=device_str)\n",
    "   ```\n",
    "\n",
    "3. **Update Dataset Creation:**\n",
    "   - Actions are already continuous (no need for +1 conversion)\n",
    "   - Rewards scale with position size\n",
    "   \n",
    "4. **Trade-offs:**\n",
    "   - ✅ More flexible position sizing\n",
    "   - ✅ Can express confidence levels\n",
    "   - ✅ Better risk management\n",
    "   - ❌ Harder to learn (larger action space)\n",
    "   - ❌ More hyperparameter tuning needed\n",
    "\n",
    "For this notebook, we use **discrete actions** for simplicity and faster training. For production, continuous actions are recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cd6044",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(pair_interval_data) > 0:\n",
    "    # Compute statistics\n",
    "    total_samples = 0\n",
    "    all_obs_dims = set()\n",
    "    all_actions_list = []\n",
    "    \n",
    "    for pair, intervals in pair_interval_data.items():\n",
    "        for interval_idx, interval_info in intervals.items():\n",
    "            data = interval_info[\"data\"]\n",
    "            total_samples += len(data[\"observations\"])\n",
    "            all_obs_dims.add(data[\"observations\"].shape[1])\n",
    "            all_actions_list.extend(data[\"actions\"])\n",
    "    \n",
    "    obs_dim = list(all_obs_dims)[0] if len(all_obs_dims) == 1 else None\n",
    "    \n",
    "    print(\"\\nDataset Statistics:\")\n",
    "    print(f\"  Total samples across all pair-intervals: {total_samples:,}\")\n",
    "    print(f\"  Observation dimension: {obs_dim}\")\n",
    "    print(f\"  Action distribution: {dict(zip([-1, 0, 1], np.bincount(np.array(all_actions_list) + 1)))}\")\n",
    "    \n",
    "    if obs_dim is None:\n",
    "        print(\"  ⚠ Warning: Inconsistent observation dimensions across pairs!\")\n",
    "else:\n",
    "    print(\"Cannot compute statistics - no data generated\")\n",
    "    obs_dim = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51902fa6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "1. **Rule-Based Strategy**: Implemented a simple statistical arbitrage strategy that:\n",
    "   - Goes short on spread when z-score > entry threshold (1.5)\n",
    "   - Goes long on spread when z-score < -entry threshold (-1.5)\n",
    "   - Closes positions when |z-score| < exit threshold (0.5)\n",
    "\n",
    "2. **Data Organization by Pair and Interval**:\n",
    "   - Data is stored per pair and per 2-day interval (NOT combined into one dataset)\n",
    "   - Each pair has multiple discrete 2-day trading windows\n",
    "   - Strategy resets at the beginning of each interval\n",
    "\n",
    "3. **Training Approach - Iterating over Pairs and Intervals**:\n",
    "   ```python\n",
    "   for epoch in epochs:\n",
    "       for pair, interval in train_pair_intervals:\n",
    "           # Train on this specific pair-interval\n",
    "           data = pair_interval_data[pair][interval]\n",
    "           train_step(model, data)\n",
    "   ```\n",
    "   This approach:\n",
    "   - Treats each pair-interval as a separate training instance\n",
    "   - Model sees diverse examples from different pairs and time periods\n",
    "   - More suitable for the discrete, non-continuous nature of the data\n",
    "\n",
    "4. **Conservative Q-Learning (CQL)**: Train an offline RL agent using observations and actions from our simple strategy:\n",
    "   - Uses d3rlpy's DiscreteCQL implementation\n",
    "   - Learns to optimize expected returns with conservative Q-values\n",
    "   - Trained on diverse 2-day trading scenarios across different pairs\n",
    "\n",
    "The trained CQL model learns a **general policy** applicable across different pairs and market conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f25b70",
   "metadata": {},
   "source": [
    "## Extension: Continuous Action Space (Partial Positions)\n",
    "\n",
    "The current implementation uses discrete actions {-1, 0, 1}. Here's how to extend it to continuous actions for partial position sizing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4540f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Modified Strategy with Continuous Actions\n",
    "# NOTE: This is just an example - it creates a separate strategy instance\n",
    "# and does NOT replace the main 'strategy' variable used elsewhere\n",
    "class ContinuousRuleBasedStrategy:\n",
    "    \"\"\"\n",
    "    Statistical arbitrage strategy with continuous position sizing.\n",
    "    Actions range from -1 (full short) to 1 (full long).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, entry_threshold=1.5, exit_threshold=0.5):\n",
    "        self.entry_threshold = entry_threshold\n",
    "        self.exit_threshold = exit_threshold\n",
    "        self.current_position = 0.0  # Current position size [-1, 1]\n",
    "    \n",
    "    def get_action(self, z_score: float) -> float:\n",
    "        \"\"\"\n",
    "        Returns continuous action based on z-score.\n",
    "        \n",
    "        Action interpretation:\n",
    "        - 1.0: Full long position (100% long the spread)\n",
    "        - 0.5: Half long position (50% long)\n",
    "        - 0.0: Neutral (no position)\n",
    "        - -0.5: Half short position (50% short)\n",
    "        - -1.0: Full short position (100% short)\n",
    "        \"\"\"\n",
    "        # If already in position, check exit condition\n",
    "        if abs(self.current_position) > 0 and abs(z_score) < self.exit_threshold:\n",
    "            self.current_position = 0.0\n",
    "            return 0.0\n",
    "        \n",
    "        # Entry logic with position sizing\n",
    "        if z_score > self.entry_threshold:\n",
    "            # Short the spread - scale by signal strength\n",
    "            # Stronger deviation → larger position\n",
    "            strength = min((z_score - self.entry_threshold) / self.entry_threshold, 1.0)\n",
    "            action = -strength  # Range: [-1, 0]\n",
    "            self.current_position = action\n",
    "            return action\n",
    "            \n",
    "        elif z_score < -self.entry_threshold:\n",
    "            # Long the spread - scale by signal strength\n",
    "            strength = min((-z_score - self.entry_threshold) / self.entry_threshold, 1.0)\n",
    "            action = strength  # Range: [0, 1]\n",
    "            self.current_position = action\n",
    "            return action\n",
    "        \n",
    "        # No strong signal - stay neutral or maintain position\n",
    "        return self.current_position\n",
    "\n",
    "# Example usage (using a separate variable name)\n",
    "print(\"=\"*60)\n",
    "print(\"Continuous Action Strategy Examples\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "continuous_strategy = ContinuousRuleBasedStrategy(entry_threshold=1.5, exit_threshold=0.5)\n",
    "\n",
    "test_cases = [\n",
    "    (2.5, \"Strong mean reversion signal\"),\n",
    "    (1.8, \"Moderate mean reversion signal\"),\n",
    "    (1.5, \"Just at entry threshold\"),\n",
    "    (0.8, \"Weak signal - maintain position\"),\n",
    "    (0.3, \"Exit signal - close position\"),\n",
    "    (-2.0, \"Strong opposite signal\"),\n",
    "]\n",
    "\n",
    "for z_score, description in test_cases:\n",
    "    action = continuous_strategy.get_action(z_score)\n",
    "    position_pct = abs(action) * 100\n",
    "    direction = \"SHORT\" if action < 0 else \"LONG\" if action > 0 else \"NEUTRAL\"\n",
    "    \n",
    "    print(f\"\\nZ-Score: {z_score:+.2f} | {description}\")\n",
    "    print(f\"  → Action: {action:+.3f} ({direction} {position_pct:.1f}%)\")\n",
    "    print(f\"  → Current position: {continuous_strategy.current_position:+.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Key Advantages of Continuous Actions:\")\n",
    "print(\"=\"*60)\n",
    "print(\"✓ Proportional risk-taking (stronger signals → larger positions)\")\n",
    "print(\"✓ Gradual position adjustments\")\n",
    "print(\"✓ Better capital utilization\")\n",
    "print(\"✓ More realistic trading behavior\")\n",
    "print(\"\\nTo use with CQL: Switch from DiscreteCQLConfig to CQLConfig\")\n",
    "print(\"and remove the +1 action conversion (actions already continuous)\")\n",
    "print(\"\\nNote: This example uses 'continuous_strategy' variable,\")\n",
    "print(\"not 'strategy', so it won't interfere with the main workflow.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff3f8ee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Conservative Q-Learning (CQL) with d3rlpy\n",
    "\n",
    "Now we'll train an offline reinforcement learning agent using Conservative Q-Learning (CQL). CQL is designed for offline RL and prevents overestimation of Q-values on out-of-distribution actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84894b0",
   "metadata": {},
   "source": [
    "## Convert Pair-Interval Data to MDPDataset\n",
    "\n",
    "First, we need to convert our pair-interval data structure into d3rlpy's `MDPDataset` format. We'll create one dataset per pair-interval, marking the last transition in each interval as terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7304b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d3rlpy.dataset import MDPDataset\n",
    "\n",
    "def convert_to_mdp_dataset(interval_data: dict) -> MDPDataset:\n",
    "    \"\"\"\n",
    "    Convert interval data to d3rlpy MDPDataset format.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    interval_data : dict\n",
    "        Dictionary with keys: observations, actions, rewards, timestamps\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    MDPDataset\n",
    "        d3rlpy dataset with terminal flags\n",
    "    \"\"\"\n",
    "    observations = interval_data[\"observations\"]\n",
    "    actions = interval_data[\"actions\"]\n",
    "    rewards = interval_data[\"rewards\"]\n",
    "    \n",
    "    # Convert actions from {-1, 0, 1} to {0, 1, 2} for d3rlpy\n",
    "    # d3rlpy expects non-negative action indices\n",
    "    actions_converted = actions + 1  # -1 -> 0, 0 -> 1, 1 -> 2\n",
    "    \n",
    "    # Create terminals array - mark last transition as terminal\n",
    "    terminals = np.zeros(len(observations), dtype=np.float32)\n",
    "    if len(terminals) > 0:\n",
    "        terminals[-1] = 1.0\n",
    "    \n",
    "    # Create MDPDataset\n",
    "    dataset = MDPDataset(\n",
    "        observations=observations,\n",
    "        actions=actions_converted,  # Use converted actions\n",
    "        rewards=rewards,\n",
    "        terminals=terminals,        # flag the end of episode\n",
    "    )\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Convert all pair-interval data to MDPDatasets\n",
    "pair_interval_mdp_datasets = {}  # {pair: {interval_idx: MDPDataset}}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Converting Pair-Interval Data to MDPDatasets\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(pair_interval_data) > 0:\n",
    "    for pair, intervals in pair_interval_data.items():\n",
    "        pair_interval_mdp_datasets[pair] = {}\n",
    "        \n",
    "        for interval_idx, interval_info in intervals.items():\n",
    "            interval_data = interval_info[\"data\"]\n",
    "            \n",
    "            # Convert to MDPDataset\n",
    "            mdp_dataset = convert_to_mdp_dataset(interval_data)\n",
    "            pair_interval_mdp_datasets[pair][interval_idx] = {\n",
    "                \"dataset\": mdp_dataset,\n",
    "                \"interval\": interval_info[\"interval\"],\n",
    "            }\n",
    "        \n",
    "        print(f\"  ✓ Converted {len(intervals)} intervals for pair {pair}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Total pairs with MDPDatasets: {len(pair_interval_mdp_datasets)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "else:\n",
    "    print(\"No data to convert!\")\n",
    "    pair_interval_mdp_datasets = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62751e6b",
   "metadata": {},
   "source": [
    "## Organize CQL Training Data by Time Splits\n",
    "\n",
    "Organize the MDPDatasets by train/val/test splits (same as BC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9c17fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(pair_interval_mdp_datasets) > 0:\n",
    "    # Get split dates (same as BC)\n",
    "    train_start_cql = pd.to_datetime(CONFIG[\"SPLITS\"][\"train\"][0])\n",
    "    train_end_cql = pd.to_datetime(CONFIG[\"SPLITS\"][\"train\"][1])\n",
    "    val_start_cql = pd.to_datetime(CONFIG[\"SPLITS\"][\"val\"][0])\n",
    "    val_end_cql = pd.to_datetime(CONFIG[\"SPLITS\"][\"val\"][1])\n",
    "    test_start_cql = pd.to_datetime(CONFIG[\"SPLITS\"][\"test\"][0])\n",
    "    test_end_cql = pd.to_datetime(CONFIG[\"SPLITS\"][\"test\"][1])\n",
    "    \n",
    "    # Organize pair-intervals by split\n",
    "    train_pair_intervals_cql = []\n",
    "    val_pair_intervals_cql = []\n",
    "    test_pair_intervals_cql = []\n",
    "    \n",
    "    for pair, intervals in pair_interval_mdp_datasets.items():\n",
    "        for interval_idx, interval_info in intervals.items():\n",
    "            interval_start, interval_end = interval_info[\"interval\"]\n",
    "            \n",
    "            # Determine which split this interval belongs to\n",
    "            if interval_start >= train_start_cql and interval_end <= train_end_cql:\n",
    "                train_pair_intervals_cql.append((pair, interval_idx))\n",
    "            elif interval_start >= val_start_cql and interval_end <= val_end_cql:\n",
    "                val_pair_intervals_cql.append((pair, interval_idx))\n",
    "            elif interval_start >= test_start_cql and interval_end <= test_end_cql:\n",
    "                test_pair_intervals_cql.append((pair, interval_idx))\n",
    "    \n",
    "    print(\"\\nCQL Data Split by Time:\")\n",
    "    print(f\"  Train: {len(train_pair_intervals_cql)} pair-intervals ({train_start_cql.date()} to {train_end_cql.date()})\")\n",
    "    print(f\"  Val:   {len(val_pair_intervals_cql)} pair-intervals ({val_start_cql.date()} to {val_end_cql.date()})\")\n",
    "    print(f\"  Test:  {len(test_pair_intervals_cql)} pair-intervals ({test_start_cql.date()} to {test_end_cql.date()})\")\n",
    "else:\n",
    "    print(\"Cannot split dataset - no data\")\n",
    "    train_pair_intervals_cql = val_pair_intervals_cql = test_pair_intervals_cql = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a6e36a",
   "metadata": {},
   "source": [
    "## Initialize Discrete CQL Agent\n",
    "\n",
    "Set up the CQL agent for training on the offline dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa872693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d3rlpy.algos import DiscreteCQLConfig\n",
    "from d3rlpy.models import VectorEncoderFactory\n",
    "\n",
    "if len(train_pair_intervals_cql) > 0 and obs_dim is not None:\n",
    "    # Get CQL configuration\n",
    "    cql_config = CONFIG[\"CQL\"]\n",
    "    \n",
    "    # Determine device string for d3rlpy (expects \"cuda:0\", \"cpu:0\", or boolean)\n",
    "    if device.type == \"cuda\":\n",
    "        device_str = f\"cuda:{device.index if device.index is not None else 0}\"\n",
    "    elif device.type == \"mps\":\n",
    "        # d3rlpy doesn't support MPS directly, fall back to CPU\n",
    "        device_str = \"cpu:0\"\n",
    "        print(\"Note: d3rlpy doesn't support MPS, using CPU instead\")\n",
    "    else:\n",
    "        device_str = \"cpu:0\"\n",
    "    \n",
    "    # Create CQL agent with proper parameters for d3rlpy v2.6.0\n",
    "    # Note: DiscreteCQL uses 'alpha' for the conservative penalty weight\n",
    "    cql = DiscreteCQLConfig(\n",
    "        learning_rate=cql_config[\"learning_rate\"],\n",
    "        batch_size=cql_config[\"batch_size\"],\n",
    "        gamma=cql_config[\"gamma\"],\n",
    "        alpha=cql_config[\"alpha\"],  # This is the conservative weight in DiscreteCQL\n",
    "        encoder_factory=VectorEncoderFactory(hidden_units=cql_config[\"q_func_hidden_sizes\"]),\n",
    "    ).create(device=device_str)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Discrete CQL Agent Configuration\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Observation dimension: {obs_dim}\")\n",
    "    print(\"Number of actions: 3 (discrete)\")\n",
    "    print(f\"Learning rate: {cql_config['learning_rate']}\")\n",
    "    print(f\"Batch size: {cql_config['batch_size']}\")\n",
    "    print(f\"Gamma: {cql_config['gamma']}\")\n",
    "    print(f\"Alpha (CQL regularization): {cql_config['alpha']}\")\n",
    "    print(f\"Conservative weight: {cql_config['conservative_weight']}\")\n",
    "    print(f\"Q-function hidden sizes: {cql_config['q_func_hidden_sizes']}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "else:\n",
    "    print(\"Cannot initialize CQL - missing data or observation dimension\")\n",
    "    cql = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e958bb83",
   "metadata": {},
   "source": [
    "## Train CQL Agent\n",
    "\n",
    "Train the CQL agent using the observations and actions from our simple strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ad02cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d3rlpy.metrics import TDErrorEvaluator\n",
    "\n",
    "if cql is not None and len(train_pair_intervals_cql) > 0:\n",
    "    # Training configuration\n",
    "    n_steps = CONFIG[\"CQL\"][\"n_steps\"]\n",
    "    n_steps_per_epoch = CONFIG[\"CQL\"][\"n_steps_per_epoch\"]\n",
    "    n_epochs_cql = n_steps // n_steps_per_epoch\n",
    "    \n",
    "    # Setup logging\n",
    "    cql_logdir = os.path.join(CONFIG[\"IO\"][\"tb_logdir\"], f\"cql_pair_iteration_{len(pairs_with_data)}pairs\")\n",
    "    ensure_dir(cql_logdir)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Training CQL Agent\")\n",
    "    print(\"Training approach: Combined dataset from all intervals\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total steps: {n_steps:,}\")\n",
    "    print(f\"Steps per epoch: {n_steps_per_epoch}\")\n",
    "    print(f\"Epochs: {n_epochs_cql}\")\n",
    "    print(f\"Train pair-intervals: {len(train_pair_intervals_cql)}\")\n",
    "    print(f\"Val pair-intervals: {len(val_pair_intervals_cql)}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Combine all training intervals into a single dataset for efficiency\n",
    "    print(\"Combining training datasets...\")\n",
    "    all_train_obs = []\n",
    "    all_train_actions = []\n",
    "    all_train_rewards = []\n",
    "    all_train_terminals = []\n",
    "    \n",
    "    # Collect raw data from pair_interval_data instead of MDPDatasets\n",
    "    for pair, interval_idx in train_pair_intervals_cql:\n",
    "        # Get the original interval data (before conversion to MDPDataset)\n",
    "        interval_info = pair_interval_data[pair][interval_idx]\n",
    "        interval_raw = interval_info[\"data\"]\n",
    "        \n",
    "        # Get the raw data\n",
    "        observations = interval_raw[\"observations\"]\n",
    "        actions = interval_raw[\"actions\"]\n",
    "        rewards = interval_raw[\"rewards\"]\n",
    "        \n",
    "        # Convert actions from {-1, 0, 1} to {0, 1, 2}\n",
    "        actions_converted = actions + 1\n",
    "        \n",
    "        # Create terminals array (mark last as terminal)\n",
    "        terminals = np.zeros(len(observations), dtype=np.float32)\n",
    "        if len(terminals) > 0:\n",
    "            terminals[-1] = 1.0\n",
    "        \n",
    "        # Append to lists\n",
    "        all_train_obs.append(observations)\n",
    "        all_train_actions.append(actions_converted)\n",
    "        all_train_rewards.append(rewards)\n",
    "        all_train_terminals.append(terminals)\n",
    "    \n",
    "    # Concatenate all data\n",
    "    combined_train_obs = np.concatenate(all_train_obs, axis=0)\n",
    "    combined_train_actions = np.concatenate(all_train_actions, axis=0)\n",
    "    combined_train_rewards = np.concatenate(all_train_rewards, axis=0)\n",
    "    combined_train_terminals = np.concatenate(all_train_terminals, axis=0)\n",
    "    \n",
    "    # Create combined training dataset\n",
    "    combined_train_dataset = MDPDataset(\n",
    "        observations=combined_train_obs,\n",
    "        actions=combined_train_actions,\n",
    "        rewards=combined_train_rewards,\n",
    "        terminals=combined_train_terminals,\n",
    "    )\n",
    "    \n",
    "    print(f\"Combined dataset size: {len(combined_train_obs):,} transitions\")\n",
    "    print(f\"Action distribution: {np.bincount(combined_train_actions.astype(int))}\")\n",
    "    print()\n",
    "    \n",
    "    # Track best model\n",
    "    best_val_td_error = float('inf')\n",
    "    models_dir_cql = CONFIG[\"IO\"][\"models_dir\"]\n",
    "    ensure_dir(models_dir_cql)\n",
    "    best_model_path = os.path.join(models_dir_cql, f\"cql_best_combined_{len(pairs_with_data)}pairs.d3\")\n",
    "    \n",
    "    # Training with progress bar\n",
    "    print(\"Training CQL agent...\")\n",
    "    results = cql.fit(\n",
    "        combined_train_dataset,\n",
    "        n_steps=n_steps,\n",
    "        n_steps_per_epoch=n_steps_per_epoch,\n",
    "        show_progress=True,  # Show progress bar\n",
    "    )\n",
    "    \n",
    "    # Validation after training\n",
    "    print(\"\\nEvaluating on validation set...\")\n",
    "    val_td_errors = []\n",
    "    \n",
    "    for pair, interval_idx in val_pair_intervals_cql:\n",
    "        val_dataset = pair_interval_mdp_datasets[pair][interval_idx][\"dataset\"]\n",
    "        \n",
    "        # Compute TD error as validation metric\n",
    "        td_error_evaluator = TDErrorEvaluator()\n",
    "        td_error = td_error_evaluator(cql, val_dataset)\n",
    "        val_td_errors.append(td_error)\n",
    "    \n",
    "    avg_val_td_error = np.mean(val_td_errors) if val_td_errors else float('inf')\n",
    "    \n",
    "    print(f\"\\nValidation TD Error: {avg_val_td_error:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    cql.save(best_model_path)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"CQL Training completed!\")\n",
    "    print(f\"Validation TD error: {avg_val_td_error:.4f}\")\n",
    "    print(f\"Model saved to: {best_model_path}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "else:\n",
    "    print(\"Cannot train CQL - missing agent or data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9626a3ef",
   "metadata": {},
   "source": [
    "## Evaluate CQL on Test Set\n",
    "\n",
    "Evaluate the trained CQL agent on the test set and analyze its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09be857",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cql is not None and len(test_pair_intervals_cql) > 0:\n",
    "    # Load best model using d3rlpy's load method\n",
    "    try:\n",
    "        from d3rlpy.algos import DiscreteCQL\n",
    "        cql_loaded = DiscreteCQL.from_json(best_model_path)\n",
    "        cql = cql_loaded  # Replace with loaded model\n",
    "        print(f\"✓ Loaded best CQL model from: {best_model_path}\\n\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠ Model file not found: {best_model_path}\")\n",
    "        print(\"Using current trained model instead.\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Could not load best model: {e}\")\n",
    "        print(\"Using current trained model instead.\\n\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"CQL Test Set Evaluation\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    test_td_errors = []\n",
    "    all_cql_preds = []\n",
    "    all_cql_labels = []\n",
    "    all_cql_rewards = []\n",
    "    \n",
    "    for pair, interval_idx in test_pair_intervals_cql:\n",
    "        # Get the original interval data\n",
    "        interval_info = pair_interval_data[pair][interval_idx]\n",
    "        interval_raw = interval_info[\"data\"]\n",
    "        \n",
    "        # Get raw data\n",
    "        observations = interval_raw[\"observations\"]\n",
    "        actions = interval_raw[\"actions\"]  # Original {-1, 0, 1}\n",
    "        rewards = interval_raw[\"rewards\"]\n",
    "        \n",
    "        # Convert actions for d3rlpy compatibility\n",
    "        actions_converted = actions + 1  # {0, 1, 2}\n",
    "        \n",
    "        # Create terminals\n",
    "        terminals = np.zeros(len(observations), dtype=np.float32)\n",
    "        if len(terminals) > 0:\n",
    "            terminals[-1] = 1.0\n",
    "        \n",
    "        # Create temporary dataset for evaluation\n",
    "        temp_dataset = MDPDataset(\n",
    "            observations=observations,\n",
    "            actions=actions_converted,\n",
    "            rewards=rewards,\n",
    "            terminals=terminals,\n",
    "        )\n",
    "        \n",
    "        # Get TD error\n",
    "        td_error_evaluator = TDErrorEvaluator()\n",
    "        td_error = td_error_evaluator(cql, temp_dataset)\n",
    "        test_td_errors.append(td_error)\n",
    "        \n",
    "        # Predict actions (will be in {0, 1, 2} format)\n",
    "        predicted_actions = cql.predict(observations)\n",
    "        \n",
    "        all_cql_preds.extend(predicted_actions)\n",
    "        all_cql_labels.extend(actions_converted)  # Use converted for comparison\n",
    "        all_cql_rewards.extend(rewards)\n",
    "    \n",
    "    avg_test_td_error = np.mean(test_td_errors)\n",
    "    \n",
    "    print(f\"Average Test TD Error: {avg_test_td_error:.4f}\")\n",
    "    print(f\"Total test samples: {len(all_cql_labels)}\")\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_cql_preds = np.array(all_cql_preds)\n",
    "    all_cql_labels = np.array(all_cql_labels)\n",
    "    all_cql_rewards = np.array(all_cql_rewards)\n",
    "    \n",
    "    # Convert actions back from {0, 1, 2} to {-1, 0, 1} for comparison\n",
    "    all_cql_preds_original = all_cql_preds - 1\n",
    "    all_cql_labels_original = all_cql_labels - 1\n",
    "    \n",
    "    # Action accuracy\n",
    "    accuracy = (all_cql_preds == all_cql_labels).mean()\n",
    "    print(f\"Action Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Action distribution (use unique with return_counts for negative values)\n",
    "    print(\"\\nAction Distribution:\")\n",
    "    pred_unique, pred_counts = np.unique(all_cql_preds_original, return_counts=True)\n",
    "    label_unique, label_counts = np.unique(all_cql_labels_original, return_counts=True)\n",
    "    \n",
    "    pred_dist = {int(k): int(v) for k, v in zip(pred_unique, pred_counts)}\n",
    "    label_dist = {int(k): int(v) for k, v in zip(label_unique, label_counts)}\n",
    "    \n",
    "    print(f\"  Predicted: {pred_dist}\")\n",
    "    print(f\"  True:      {label_dist}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    try:\n",
    "        from sklearn.metrics import confusion_matrix, classification_report\n",
    "    except ImportError:\n",
    "        print(\"Warning: scikit-learn not installed. Cannot compute confusion matrix.\")\n",
    "        confusion_matrix = None\n",
    "        classification_report = None\n",
    "    \n",
    "    if confusion_matrix is not None:\n",
    "        cm_cql = confusion_matrix(all_cql_labels_original, all_cql_preds_original, labels=[-1, 0, 1])\n",
    "        \n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(\"             Predicted\")\n",
    "        print(\"           -1    0    1\")\n",
    "        print(f\"True -1 | {cm_cql[0,0]:4d} {cm_cql[0,1]:4d} {cm_cql[0,2]:4d}\")\n",
    "        print(f\"      0 | {cm_cql[1,0]:4d} {cm_cql[1,1]:4d} {cm_cql[1,2]:4d}\")\n",
    "        print(f\"      1 | {cm_cql[2,0]:4d} {cm_cql[2,1]:4d} {cm_cql[2,2]:4d}\")\n",
    "        \n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(all_cql_labels_original, all_cql_preds_original, \n",
    "                                    target_names=['Short Spread (-1)', 'Neutral (0)', 'Long Spread (1)'],\n",
    "                                    digits=4))\n",
    "    \n",
    "    print(f\"{'='*60}\\n\")\n",
    "else:\n",
    "    print(\"Cannot evaluate CQL - missing agent or test data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9e7e5e",
   "metadata": {},
   "source": [
    "## Visualize CQL Results\n",
    "\n",
    "Visualize the CQL performance with confusion matrix and action distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917fe8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cql is not None and len(test_pair_intervals_cql) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    im = axes[0].imshow(cm_cql, cmap='Greens', aspect='auto')\n",
    "    axes[0].set_xticks([-1, 0, 1])\n",
    "    axes[0].set_yticks([-1, 0, 1])\n",
    "    axes[0].set_xticklabels(['Short (-1)', 'Neutral (0)', 'Long (1)'])\n",
    "    axes[0].set_yticklabels(['Short (-1)', 'Neutral (0)', 'Long (1)'])\n",
    "    axes[0].set_xlabel('Predicted Action')\n",
    "    axes[0].set_ylabel('True Action')\n",
    "    axes[0].set_title('CQL Confusion Matrix')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            text = axes[0].text(j-1, i-1, cm_cql[i, j],\n",
    "                              ha=\"center\", va=\"center\", \n",
    "                              color=\"black\" if cm_cql[i, j] < cm_cql.max()/2 else \"white\",\n",
    "                              fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.colorbar(im, ax=axes[0])\n",
    "    \n",
    "    # Plot action distribution comparison\n",
    "    # Use the original space actions {-1, 0, 1} with proper counting\n",
    "    action_labels = [-1, 0, 1]\n",
    "    \n",
    "    # Count actions for each category (ensuring all 3 actions are represented)\n",
    "    true_counts_cql = np.array([\n",
    "        np.sum(all_cql_labels_original == -1),\n",
    "        np.sum(all_cql_labels_original == 0),\n",
    "        np.sum(all_cql_labels_original == 1)\n",
    "    ])\n",
    "    pred_counts_cql = np.array([\n",
    "        np.sum(all_cql_preds_original == -1),\n",
    "        np.sum(all_cql_preds_original == 0),\n",
    "        np.sum(all_cql_preds_original == 1)\n",
    "    ])\n",
    "    \n",
    "    x = np.arange(3)\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[1].bar(x - width/2, true_counts_cql, width, label='True', alpha=0.8, color='steelblue')\n",
    "    axes[1].bar(x + width/2, pred_counts_cql, width, label='CQL Predicted', alpha=0.8, color='seagreen')\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(['Short (-1)', 'Neutral (0)', 'Long (1)'])\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].set_title('CQL Action Distribution: True vs Predicted')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    if CONFIG[\"EVAL\"][\"plots\"]:\n",
    "        reports_dir = CONFIG[\"EVAL\"][\"reports_dir\"]\n",
    "        ensure_dir(reports_dir)\n",
    "        fig_path_cql = os.path.join(reports_dir, f\"cql_evaluation_pair_iter_{len(pairs_with_data)}pairs.png\")\n",
    "        plt.savefig(fig_path_cql, dpi=150, bbox_inches='tight')\n",
    "        print(f\"\\nSaved CQL evaluation plot to: {fig_path_cql}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Cannot visualize - missing CQL agent or test data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a34931",
   "metadata": {},
   "source": [
    "# Conservative Q-Learning with PnL-Based Rewards\n",
    "\n",
    "Now we train a second CQL model using realistic portfolio returns minus transaction costs as rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2069bdf6",
   "metadata": {},
   "source": [
    "## Generate Dataset with PnL Rewards\n",
    "\n",
    "Generate a new dataset where rewards are calculated as: **portfolio_return - transaction_costs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ced2754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate PnL-based dataset for CQL\n",
    "transaction_cost_bps = CONFIG[\"CQL_PnL\"][\"transaction_cost_bps\"]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Generating PnL-Based Dataset for CQL\")\n",
    "print(f\"Transaction cost: {transaction_cost_bps} bps ({transaction_cost_bps/100:.4f}%)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "pair_interval_data_pnl = {}\n",
    "total_intervals_pnl = 0\n",
    "pairs_with_data_pnl = []\n",
    "\n",
    "for pair, valid_ints in valid_intervals_per_pair.items():\n",
    "    if len(valid_ints) == 0:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nProcessing pair {pair} with {len(valid_ints)} valid intervals...\")\n",
    "    pair_interval_data_pnl[pair] = {}\n",
    "    \n",
    "    valid_count = 0\n",
    "    for interval_idx, interval in enumerate(valid_ints):\n",
    "        # Generate dataset with PnL rewards\n",
    "        interval_data = generate_interval_dataset_with_pnl(\n",
    "            features_df,\n",
    "            pair,\n",
    "            interval,\n",
    "            strategy,\n",
    "            transaction_cost_bps=transaction_cost_bps,\n",
    "            z_score_feature=z_score_feature,\n",
    "            pair_feature_format=pair_feature_format,\n",
    "            single_asset_features=single_asset_features[:5],\n",
    "            timestamp_col=timestamp_col,\n",
    "        )\n",
    "        \n",
    "        if interval_data is not None and len(interval_data[\"observations\"]) > 0:\n",
    "            pair_interval_data_pnl[pair][interval_idx] = {\n",
    "                \"data\": interval_data,\n",
    "                \"interval\": interval,\n",
    "            }\n",
    "            valid_count += 1\n",
    "    \n",
    "    if valid_count > 0:\n",
    "        pairs_with_data_pnl.append(pair)\n",
    "        total_intervals_pnl += valid_count\n",
    "        print(f\"  ✓ Stored {valid_count} valid intervals for pair {pair}\")\n",
    "    else:\n",
    "        del pair_interval_data_pnl[pair]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PnL Dataset Summary\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total pairs with data: {len(pairs_with_data_pnl)}\")\n",
    "print(f\"Total intervals: {total_intervals_pnl}\")\n",
    "print(f\"Pairs: {pairs_with_data_pnl}\")\n",
    "\n",
    "# Compute reward statistics\n",
    "all_rewards_pnl = []\n",
    "for pair, intervals in pair_interval_data_pnl.items():\n",
    "    for interval_idx, interval_info in intervals.items():\n",
    "        all_rewards_pnl.extend(interval_info[\"data\"][\"rewards\"])\n",
    "\n",
    "if len(all_rewards_pnl) > 0:\n",
    "    all_rewards_pnl = np.array(all_rewards_pnl)\n",
    "    print(f\"\\nReward Statistics (PnL-based):\")\n",
    "    print(f\"  Mean: {all_rewards_pnl.mean():.6f}\")\n",
    "    print(f\"  Std: {all_rewards_pnl.std():.6f}\")\n",
    "    print(f\"  Min: {all_rewards_pnl.min():.6f}\")\n",
    "    print(f\"  Max: {all_rewards_pnl.max():.6f}\")\n",
    "    print(f\"  Positive rewards: {(all_rewards_pnl > 0).sum()} ({(all_rewards_pnl > 0).mean()*100:.1f}%)\")\n",
    "    print(f\"  Negative rewards: {(all_rewards_pnl < 0).sum()} ({(all_rewards_pnl < 0).mean()*100:.1f}%)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa12576",
   "metadata": {},
   "source": [
    "## Organize PnL Data by Splits\n",
    "\n",
    "Split the PnL dataset into train/val/test using the same time-based splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2424a00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(pair_interval_data_pnl) > 0:\n",
    "    # Get split dates (same as before)\n",
    "    train_start_pnl = pd.to_datetime(CONFIG[\"SPLITS\"][\"train\"][0])\n",
    "    train_end_pnl = pd.to_datetime(CONFIG[\"SPLITS\"][\"train\"][1])\n",
    "    val_start_pnl = pd.to_datetime(CONFIG[\"SPLITS\"][\"val\"][0])\n",
    "    val_end_pnl = pd.to_datetime(CONFIG[\"SPLITS\"][\"val\"][1])\n",
    "    test_start_pnl = pd.to_datetime(CONFIG[\"SPLITS\"][\"test\"][0])\n",
    "    test_end_pnl = pd.to_datetime(CONFIG[\"SPLITS\"][\"test\"][1])\n",
    "    \n",
    "    # Organize pair-intervals by split\n",
    "    train_pair_intervals_pnl = []\n",
    "    val_pair_intervals_pnl = []\n",
    "    test_pair_intervals_pnl = []\n",
    "    \n",
    "    for pair, intervals in pair_interval_data_pnl.items():\n",
    "        for interval_idx, interval_info in intervals.items():\n",
    "            interval_start, interval_end = interval_info[\"interval\"]\n",
    "            \n",
    "            # Determine which split this interval belongs to\n",
    "            if interval_start >= train_start_pnl and interval_end <= train_end_pnl:\n",
    "                train_pair_intervals_pnl.append((pair, interval_idx))\n",
    "            elif interval_start >= val_start_pnl and interval_end <= val_end_pnl:\n",
    "                val_pair_intervals_pnl.append((pair, interval_idx))\n",
    "            elif interval_start >= test_start_pnl and interval_end <= test_end_pnl:\n",
    "                test_pair_intervals_pnl.append((pair, interval_idx))\n",
    "    \n",
    "    print(\"\\nPnL Data Split by Time:\")\n",
    "    print(f\"  Train: {len(train_pair_intervals_pnl)} pair-intervals\")\n",
    "    print(f\"  Val:   {len(val_pair_intervals_pnl)} pair-intervals\")\n",
    "    print(f\"  Test:  {len(test_pair_intervals_pnl)} pair-intervals\")\n",
    "else:\n",
    "    print(\"Cannot split dataset - no PnL data\")\n",
    "    train_pair_intervals_pnl = val_pair_intervals_pnl = test_pair_intervals_pnl = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805953e5",
   "metadata": {},
   "source": [
    "## Initialize CQL-PnL Agent\n",
    "\n",
    "Create the second CQL agent with PnL-based rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7508a02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d3rlpy.algos import DiscreteCQLConfig\n",
    "from d3rlpy.models import VectorEncoderFactory\n",
    "\n",
    "if len(train_pair_intervals_pnl) > 0 and obs_dim is not None:\n",
    "    # Get CQL-PnL configuration\n",
    "    cql_pnl_config = CONFIG[\"CQL_PnL\"]\n",
    "    \n",
    "    # Determine device string for d3rlpy\n",
    "    if device.type == \"cuda\":\n",
    "        device_str = f\"cuda:{device.index if device.index is not None else 0}\"\n",
    "    elif device.type == \"mps\":\n",
    "        device_str = \"cpu:0\"\n",
    "        print(\"Note: d3rlpy doesn't support MPS, using CPU instead\")\n",
    "    else:\n",
    "        device_str = \"cpu:0\"\n",
    "    \n",
    "    # Create CQL-PnL agent\n",
    "    cql_pnl = DiscreteCQLConfig(\n",
    "        learning_rate=cql_pnl_config[\"learning_rate\"],\n",
    "        batch_size=cql_pnl_config[\"batch_size\"],\n",
    "        gamma=cql_pnl_config[\"gamma\"],\n",
    "        alpha=cql_pnl_config[\"alpha\"],\n",
    "        encoder_factory=VectorEncoderFactory(hidden_units=cql_pnl_config[\"q_func_hidden_sizes\"]),\n",
    "    ).create(device=device_str)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"CQL-PnL Agent Configuration\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Observation dimension: {obs_dim}\")\n",
    "    print(\"Number of actions: 3 (discrete)\")\n",
    "    print(f\"Learning rate: {cql_pnl_config['learning_rate']}\")\n",
    "    print(f\"Batch size: {cql_pnl_config['batch_size']}\")\n",
    "    print(f\"Gamma: {cql_pnl_config['gamma']}\")\n",
    "    print(f\"Alpha (CQL regularization): {cql_pnl_config['alpha']}\")\n",
    "    print(f\"Q-function hidden sizes: {cql_pnl_config['q_func_hidden_sizes']}\")\n",
    "    print(f\"Device: {device_str}\")\n",
    "    print(f\"Transaction cost: {cql_pnl_config['transaction_cost_bps']} bps\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "else:\n",
    "    print(\"Cannot create CQL-PnL agent - missing training data or observation dimension\")\n",
    "    cql_pnl = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32503419",
   "metadata": {},
   "source": [
    "## Train CQL-PnL Agent\n",
    "\n",
    "Train the CQL agent on the PnL-based dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b1d27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d3rlpy.metrics import TDErrorEvaluator\n",
    "\n",
    "if cql_pnl is not None and len(train_pair_intervals_pnl) > 0:\n",
    "    # Training configuration\n",
    "    n_steps_pnl = CONFIG[\"CQL_PnL\"][\"n_steps\"]\n",
    "    n_steps_per_epoch_pnl = CONFIG[\"CQL_PnL\"][\"n_steps_per_epoch\"]\n",
    "    n_epochs_pnl = n_steps_pnl // n_steps_per_epoch_pnl\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Training CQL-PnL Agent\")\n",
    "    print(\"Training approach: Combined dataset with PnL rewards\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total steps: {n_steps_pnl:,}\")\n",
    "    print(f\"Steps per epoch: {n_steps_per_epoch_pnl}\")\n",
    "    print(f\"Epochs: {n_epochs_pnl}\")\n",
    "    print(f\"Train pair-intervals: {len(train_pair_intervals_pnl)}\")\n",
    "    print(f\"Val pair-intervals: {len(val_pair_intervals_pnl)}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Combine all training intervals into a single dataset\n",
    "    print(\"Combining training datasets...\")\n",
    "    all_train_obs_pnl = []\n",
    "    all_train_actions_pnl = []\n",
    "    all_train_rewards_pnl = []\n",
    "    all_train_terminals_pnl = []\n",
    "    \n",
    "    for pair, interval_idx in train_pair_intervals_pnl:\n",
    "        # Get the original interval data\n",
    "        interval_info = pair_interval_data_pnl[pair][interval_idx]\n",
    "        interval_raw = interval_info[\"data\"]\n",
    "        \n",
    "        # Get the raw data\n",
    "        observations = interval_raw[\"observations\"]\n",
    "        actions = interval_raw[\"actions\"]\n",
    "        rewards = interval_raw[\"rewards\"]\n",
    "        \n",
    "        # Convert actions from {-1, 0, 1} to {0, 1, 2}\n",
    "        actions_converted = actions + 1\n",
    "        \n",
    "        # Create terminals array (mark last as terminal)\n",
    "        terminals = np.zeros(len(observations), dtype=np.float32)\n",
    "        if len(terminals) > 0:\n",
    "            terminals[-1] = 1.0\n",
    "        \n",
    "        # Append to lists\n",
    "        all_train_obs_pnl.append(observations)\n",
    "        all_train_actions_pnl.append(actions_converted)\n",
    "        all_train_rewards_pnl.append(rewards)\n",
    "        all_train_terminals_pnl.append(terminals)\n",
    "    \n",
    "    # Concatenate all data\n",
    "    combined_train_obs_pnl = np.concatenate(all_train_obs_pnl, axis=0)\n",
    "    combined_train_actions_pnl = np.concatenate(all_train_actions_pnl, axis=0)\n",
    "    combined_train_rewards_pnl = np.concatenate(all_train_rewards_pnl, axis=0)\n",
    "    combined_train_terminals_pnl = np.concatenate(all_train_terminals_pnl, axis=0)\n",
    "    \n",
    "    # Create combined training dataset\n",
    "    combined_train_dataset_pnl = MDPDataset(\n",
    "        observations=combined_train_obs_pnl,\n",
    "        actions=combined_train_actions_pnl,\n",
    "        rewards=combined_train_rewards_pnl,\n",
    "        terminals=combined_train_terminals_pnl,\n",
    "    )\n",
    "    \n",
    "    print(f\"Combined dataset size: {len(combined_train_obs_pnl):,} transitions\")\n",
    "    print(f\"Action distribution: {np.bincount(combined_train_actions_pnl.astype(int))}\")\n",
    "    print(f\"Reward stats: mean={combined_train_rewards_pnl.mean():.6f}, std={combined_train_rewards_pnl.std():.6f}\")\n",
    "    print()\n",
    "    \n",
    "    # Track best model\n",
    "    best_val_td_error_pnl = float('inf')\n",
    "    models_dir_pnl = CONFIG[\"IO\"][\"models_dir\"]\n",
    "    ensure_dir(models_dir_pnl)\n",
    "    best_model_path_pnl = os.path.join(models_dir_pnl, f\"cql_pnl_best_{len(pairs_with_data_pnl)}pairs.d3\")\n",
    "    \n",
    "    # Training with progress bar\n",
    "    print(\"Training CQL-PnL agent...\")\n",
    "    results_pnl = cql_pnl.fit(\n",
    "        combined_train_dataset_pnl,\n",
    "        n_steps=n_steps_pnl,\n",
    "        n_steps_per_epoch=n_steps_per_epoch_pnl,\n",
    "        show_progress=True,\n",
    "    )\n",
    "    \n",
    "    # Validation after training\n",
    "    print(\"\\nEvaluating on validation set...\")\n",
    "    val_td_errors_pnl = []\n",
    "    \n",
    "    for pair, interval_idx in val_pair_intervals_pnl:\n",
    "        # Get validation data\n",
    "        interval_info = pair_interval_data_pnl[pair][interval_idx]\n",
    "        interval_raw = interval_info[\"data\"]\n",
    "        \n",
    "        observations = interval_raw[\"observations\"]\n",
    "        actions = interval_raw[\"actions\"]\n",
    "        rewards = interval_raw[\"rewards\"]\n",
    "        actions_converted = actions + 1\n",
    "        \n",
    "        terminals = np.zeros(len(observations), dtype=np.float32)\n",
    "        if len(terminals) > 0:\n",
    "            terminals[-1] = 1.0\n",
    "        \n",
    "        val_dataset = MDPDataset(\n",
    "            observations=observations,\n",
    "            actions=actions_converted,\n",
    "            rewards=rewards,\n",
    "            terminals=terminals,\n",
    "        )\n",
    "        \n",
    "        # Compute TD error\n",
    "        td_error_evaluator = TDErrorEvaluator()\n",
    "        td_error = td_error_evaluator(cql_pnl, val_dataset)\n",
    "        val_td_errors_pnl.append(td_error)\n",
    "    \n",
    "    avg_val_td_error_pnl = np.mean(val_td_errors_pnl) if val_td_errors_pnl else float('inf')\n",
    "    \n",
    "    print(f\"\\nValidation TD Error: {avg_val_td_error_pnl:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    cql_pnl.save(best_model_path_pnl)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"CQL-PnL Training completed!\")\n",
    "    print(f\"Validation TD error: {avg_val_td_error_pnl:.4f}\")\n",
    "    print(f\"Model saved to: {best_model_path_pnl}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "else:\n",
    "    print(\"Cannot train CQL-PnL - missing agent or data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ebb314",
   "metadata": {},
   "source": [
    "## Evaluate CQL-PnL on Test Set\n",
    "\n",
    "Evaluate the PnL-optimized CQL agent on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d875d2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cql_pnl is not None and len(test_pair_intervals_pnl) > 0:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"CQL-PnL Test Set Evaluation\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    test_td_errors_pnl = []\n",
    "    all_pnl_preds = []\n",
    "    all_pnl_labels = []\n",
    "    all_pnl_rewards = []\n",
    "    \n",
    "    for pair, interval_idx in test_pair_intervals_pnl:\n",
    "        # Get test data\n",
    "        interval_info = pair_interval_data_pnl[pair][interval_idx]\n",
    "        interval_raw = interval_info[\"data\"]\n",
    "        \n",
    "        observations = interval_raw[\"observations\"]\n",
    "        actions = interval_raw[\"actions\"]\n",
    "        rewards = interval_raw[\"rewards\"]\n",
    "        actions_converted = actions + 1\n",
    "        \n",
    "        terminals = np.zeros(len(observations), dtype=np.float32)\n",
    "        if len(terminals) > 0:\n",
    "            terminals[-1] = 1.0\n",
    "        \n",
    "        temp_dataset = MDPDataset(\n",
    "            observations=observations,\n",
    "            actions=actions_converted,\n",
    "            rewards=rewards,\n",
    "            terminals=terminals,\n",
    "        )\n",
    "        \n",
    "        # Get TD error\n",
    "        td_error_evaluator = TDErrorEvaluator()\n",
    "        td_error = td_error_evaluator(cql_pnl, temp_dataset)\n",
    "        test_td_errors_pnl.append(td_error)\n",
    "        \n",
    "        # Predict actions\n",
    "        predicted_actions = cql_pnl.predict(observations)\n",
    "        \n",
    "        all_pnl_preds.extend(predicted_actions)\n",
    "        all_pnl_labels.extend(actions_converted)\n",
    "        all_pnl_rewards.extend(rewards)\n",
    "    \n",
    "    avg_test_td_error_pnl = np.mean(test_td_errors_pnl)\n",
    "    \n",
    "    print(f\"Average Test TD Error: {avg_test_td_error_pnl:.4f}\")\n",
    "    print(f\"Total test samples: {len(all_pnl_labels)}\")\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_pnl_preds = np.array(all_pnl_preds)\n",
    "    all_pnl_labels = np.array(all_pnl_labels)\n",
    "    all_pnl_rewards = np.array(all_pnl_rewards)\n",
    "    \n",
    "    # Convert actions back to original space\n",
    "    all_pnl_preds_original = all_pnl_preds - 1\n",
    "    all_pnl_labels_original = all_pnl_labels - 1\n",
    "    \n",
    "    # Action accuracy\n",
    "    accuracy_pnl = (all_pnl_preds == all_pnl_labels).mean()\n",
    "    print(f\"Action Accuracy: {accuracy_pnl:.4f}\")\n",
    "    \n",
    "    # Action distribution\n",
    "    print(\"\\nAction Distribution:\")\n",
    "    pred_unique_pnl, pred_counts_pnl = np.unique(all_pnl_preds_original, return_counts=True)\n",
    "    label_unique_pnl, label_counts_pnl = np.unique(all_pnl_labels_original, return_counts=True)\n",
    "    \n",
    "    pred_dist_pnl = {int(k): int(v) for k, v in zip(pred_unique_pnl, pred_counts_pnl)}\n",
    "    label_dist_pnl = {int(k): int(v) for k, v in zip(label_unique_pnl, label_counts_pnl)}\n",
    "    \n",
    "    print(f\"  Predicted: {pred_dist_pnl}\")\n",
    "    print(f\"  True:      {label_dist_pnl}\")\n",
    "    \n",
    "    # Reward statistics\n",
    "    print(f\"\\nReward Statistics on Test Set:\")\n",
    "    print(f\"  Mean: {all_pnl_rewards.mean():.6f}\")\n",
    "    print(f\"  Cumulative: {all_pnl_rewards.sum():.6f}\")\n",
    "    print(f\"  Positive: {(all_pnl_rewards > 0).sum()} ({(all_pnl_rewards > 0).mean()*100:.1f}%)\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    try:\n",
    "        from sklearn.metrics import confusion_matrix, classification_report\n",
    "    except ImportError:\n",
    "        print(\"Warning: scikit-learn not installed. Cannot compute confusion matrix.\")\n",
    "        confusion_matrix = None\n",
    "        classification_report = None\n",
    "    \n",
    "    if confusion_matrix is not None:\n",
    "        cm_pnl = confusion_matrix(all_pnl_labels_original, all_pnl_preds_original, labels=[-1, 0, 1])\n",
    "        \n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(\"             Predicted\")\n",
    "        print(\"           -1    0    1\")\n",
    "        print(f\"True -1 | {cm_pnl[0,0]:4d} {cm_pnl[0,1]:4d} {cm_pnl[0,2]:4d}\")\n",
    "        print(f\"      0 | {cm_pnl[1,0]:4d} {cm_pnl[1,1]:4d} {cm_pnl[1,2]:4d}\")\n",
    "        print(f\"      1 | {cm_pnl[2,0]:4d} {cm_pnl[2,1]:4d} {cm_pnl[2,2]:4d}\")\n",
    "        \n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(all_pnl_labels_original, all_pnl_preds_original,\n",
    "                                    target_names=['Short Spread (-1)', 'Neutral (0)', 'Long Spread (1)'],\n",
    "                                    digits=4))\n",
    "    \n",
    "    print(f\"{'='*60}\\n\")\n",
    "else:\n",
    "    print(\"Cannot evaluate CQL-PnL - missing agent or test data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ceed44",
   "metadata": {},
   "source": [
    "## Compare All Models: BC vs CQL vs CQL-PnL\n",
    "\n",
    "Compare all three models side by side."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbab26a4",
   "metadata": {},
   "source": [
    "# Portfolio Backtesting\n",
    "\n",
    "Backtest all strategies/models on the test set and plot portfolio value over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c209fcb",
   "metadata": {},
   "source": [
    "## Backtest Function\n",
    "\n",
    "Create a function to simulate portfolio performance with realistic PnL calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23357a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_strategy(\n",
    "    model,\n",
    "    model_type: str,\n",
    "    pair_interval_data: dict,\n",
    "    test_intervals: list,\n",
    "    transaction_cost_bps: float = 3.5,\n",
    "    initial_capital: float = 10000.0,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Backtest a strategy on test data with realistic PnL calculation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : model object\n",
    "        BC network, CQL agent, or rule-based strategy\n",
    "    model_type : str\n",
    "        'BC', 'CQL', 'CQL_PnL', or 'RuleBased'\n",
    "    pair_interval_data : dict\n",
    "        Dictionary with pair-interval data\n",
    "    test_intervals : list\n",
    "        List of (pair, interval_idx) tuples for testing\n",
    "    transaction_cost_bps : float\n",
    "        Transaction cost in basis points\n",
    "    initial_capital : float\n",
    "        Starting portfolio value\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict with keys:\n",
    "        - portfolio_values: list of portfolio values over time\n",
    "        - timestamps: list of timestamps\n",
    "        - actions: list of actions taken\n",
    "        - returns: list of returns\n",
    "        - cumulative_return: final cumulative return\n",
    "    \"\"\"\n",
    "    portfolio_value = initial_capital\n",
    "    portfolio_values = [portfolio_value]\n",
    "    all_timestamps = []\n",
    "    all_actions = []\n",
    "    all_returns = []\n",
    "    \n",
    "    prev_action = 0  # Track previous action for transaction costs\n",
    "    \n",
    "    # Process each test interval\n",
    "    for pair, interval_idx in test_intervals:\n",
    "        interval_info = pair_interval_data[pair][interval_idx]\n",
    "        interval_raw = interval_info[\"data\"]\n",
    "        \n",
    "        observations = interval_raw[\"observations\"]\n",
    "        timestamps = interval_raw[\"timestamps\"]\n",
    "        \n",
    "        # Get prices (assume they're in the dataframe)\n",
    "        # We need to reconstruct prices from the original data\n",
    "        interval_start, interval_end = interval_info[\"interval\"]\n",
    "        \n",
    "        # Get interval data from features_df\n",
    "        if \"datetime\" in features_df.columns:\n",
    "            ts = features_df[\"datetime\"]\n",
    "        else:\n",
    "            ts = features_df.index\n",
    "        \n",
    "        mask = (ts >= interval_start) & (ts < interval_end)\n",
    "        interval_df = features_df[mask].copy()\n",
    "        \n",
    "        if interval_df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Get price columns\n",
    "        asset1_price_col = f\"{pair[0]}_close\"\n",
    "        asset2_price_col = f\"{pair[1]}_close\"\n",
    "        \n",
    "        if asset1_price_col not in interval_df.columns or asset2_price_col not in interval_df.columns:\n",
    "            continue\n",
    "        \n",
    "        prices1 = interval_df[asset1_price_col].values\n",
    "        prices2 = interval_df[asset2_price_col].values\n",
    "        \n",
    "        # Align observations with prices - ensure we have enough data\n",
    "        # We need at least 2 price points to calculate returns\n",
    "        if len(prices1) < 2 or len(prices2) < 2 or len(observations) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Ensure we don't go out of bounds\n",
    "        n_obs = min(len(observations), len(prices1) - 1, len(prices2) - 1)\n",
    "        \n",
    "        for t in range(n_obs):\n",
    "            obs_t = observations[t:t+1]  # Shape (1, obs_dim)\n",
    "            \n",
    "            # Get action from model\n",
    "            if model_type == 'BC':\n",
    "                obs_tensor = torch.FloatTensor(obs_t).to(device)\n",
    "                with torch.no_grad():\n",
    "                    logits = model(obs_tensor)\n",
    "                    action_idx = torch.argmax(logits, dim=1).item()\n",
    "                action = action_idx - 1  # Convert from {0,1,2} to {-1,0,1}\n",
    "            \n",
    "            elif model_type in ['CQL', 'CQL_PnL']:\n",
    "                # CQL expects actions in {0,1,2} format internally\n",
    "                action_pred = model.predict(obs_t)[0]  # Returns action in {0,1,2}\n",
    "                action = action_pred - 1  # Convert to {-1,0,1}\n",
    "            \n",
    "            elif model_type == 'RuleBased':\n",
    "                # Use the rule-based strategy directly\n",
    "                # Extract z-score from observation (last feature)\n",
    "                z_score = obs_t[0, -1]\n",
    "                action = model.get_action(z_score)\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "            \n",
    "            # Calculate return from t to t+1\n",
    "            price1_t = prices1[t]\n",
    "            price2_t = prices2[t]\n",
    "            price1_next = prices1[t + 1]\n",
    "            price2_next = prices2[t + 1]\n",
    "            \n",
    "            # Skip if prices are invalid or NaN\n",
    "            if (price1_t <= 0 or price2_t <= 0 or price1_next <= 0 or price2_next <= 0 or\n",
    "                np.isnan(price1_t) or np.isnan(price2_t) or \n",
    "                np.isnan(price1_next) or np.isnan(price2_next)):\n",
    "                continue\n",
    "            \n",
    "            ret1 = (price1_next - price1_t) / price1_t\n",
    "            ret2 = (price2_next - price2_t) / price2_t\n",
    "            \n",
    "            # Skip if returns are NaN or infinite\n",
    "            if np.isnan(ret1) or np.isnan(ret2) or np.isinf(ret1) or np.isinf(ret2):\n",
    "                continue\n",
    "            \n",
    "            # Clip extreme returns to prevent bankruptcy (e.g., ±50% max per trade)\n",
    "            ret1 = np.clip(ret1, -0.5, 0.5)\n",
    "            ret2 = np.clip(ret2, -0.5, 0.5)\n",
    "            \n",
    "            # Calculate portfolio return based on action\n",
    "            if action == -1:\n",
    "                # Short spread: short asset1, long asset2\n",
    "                portfolio_return = -ret1 + ret2\n",
    "            elif action == 1:\n",
    "                # Long spread: long asset1, short asset2\n",
    "                portfolio_return = ret1 - ret2\n",
    "            else:\n",
    "                # No position\n",
    "                portfolio_return = 0.0\n",
    "            \n",
    "            # Apply transaction costs if position changes\n",
    "            transaction_cost = 0.0\n",
    "            if action != prev_action:\n",
    "                transaction_cost = 2 * (transaction_cost_bps / 10000)\n",
    "            \n",
    "            # Net return after transaction costs\n",
    "            net_return = portfolio_return - transaction_cost\n",
    "            \n",
    "            # Clip net return to prevent portfolio value going negative\n",
    "            # Maximum loss per trade should not exceed 95% of portfolio\n",
    "            net_return = max(net_return, -0.95)\n",
    "            \n",
    "            # Update portfolio value\n",
    "            portfolio_value = portfolio_value * (1 + net_return)\n",
    "            \n",
    "            # Safety check: if portfolio value is too low, stop trading\n",
    "            if portfolio_value < initial_capital * 0.01:  # Less than 1% of initial capital\n",
    "                print(f\"Warning: Portfolio value dropped below 1% of initial capital. Stopping backtest.\")\n",
    "                break\n",
    "            \n",
    "            # Record\n",
    "            portfolio_values.append(portfolio_value)\n",
    "            all_timestamps.append(timestamps[t] if t < len(timestamps) else None)\n",
    "            all_actions.append(action)\n",
    "            all_returns.append(net_return)\n",
    "            \n",
    "            prev_action = action\n",
    "    \n",
    "    cumulative_return = (portfolio_value - initial_capital) / initial_capital\n",
    "    \n",
    "    return {\n",
    "        'portfolio_values': portfolio_values,\n",
    "        'timestamps': all_timestamps,\n",
    "        'actions': all_actions,\n",
    "        'returns': all_returns,\n",
    "        'cumulative_return': cumulative_return,\n",
    "        'final_value': portfolio_value,\n",
    "    }\n",
    "\n",
    "print(\"Backtest function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4633a19",
   "metadata": {},
   "source": [
    "## Run Backtests\n",
    "\n",
    "Backtest all models on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e93dbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_cost_bps = CONFIG[\"CQL_PnL\"][\"transaction_cost_bps\"]\n",
    "initial_capital = 10000.0\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Running Backtests on Test Set\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Initial capital: ${initial_capital:,.2f}\")\n",
    "print(f\"Transaction cost: {transaction_cost_bps} bps ({transaction_cost_bps/100:.4f}%)\")\n",
    "print(f\"Test intervals: {len(test_pair_intervals)}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Backtest Rule-Based Strategy\n",
    "if len(test_pair_intervals) > 0:\n",
    "    print(\"Backtesting Rule-Based Strategy...\")\n",
    "    strategy_reset = RuleBasedPairsStrategy(entry_threshold, exit_threshold)\n",
    "    results_rulebased = backtest_strategy(\n",
    "        strategy_reset,\n",
    "        'RuleBased',\n",
    "        pair_interval_data,\n",
    "        test_pair_intervals,\n",
    "        transaction_cost_bps=transaction_cost_bps,\n",
    "        initial_capital=initial_capital,\n",
    "    )\n",
    "    print(f\"  ✓ Final Value: ${results_rulebased['final_value']:,.2f}\")\n",
    "    print(f\"  ✓ Return: {results_rulebased['cumulative_return']*100:.2f}%\\n\")\n",
    "else:\n",
    "    results_rulebased = None\n",
    "    print(\"No test data for Rule-Based Strategy\\n\")\n",
    "\n",
    "# Backtest BC\n",
    "if bc_net is not None and len(test_pair_intervals) > 0:\n",
    "    print(\"Backtesting Behavior Cloning...\")\n",
    "    bc_net.eval()\n",
    "    results_bc = backtest_strategy(\n",
    "        bc_net,\n",
    "        'BC',\n",
    "        pair_interval_data,\n",
    "        test_pair_intervals,\n",
    "        transaction_cost_bps=transaction_cost_bps,\n",
    "        initial_capital=initial_capital,\n",
    "    )\n",
    "    print(f\"  ✓ Final Value: ${results_bc['final_value']:,.2f}\")\n",
    "    print(f\"  ✓ Return: {results_bc['cumulative_return']*100:.2f}%\\n\")\n",
    "else:\n",
    "    results_bc = None\n",
    "    print(\"No BC model or test data\\n\")\n",
    "\n",
    "# Backtest CQL (Z-Score)\n",
    "if cql is not None and len(test_pair_intervals_cql) > 0:\n",
    "    print(\"Backtesting CQL (Z-Score Reward)...\")\n",
    "    results_cql = backtest_strategy(\n",
    "        cql,\n",
    "        'CQL',\n",
    "        pair_interval_data,  # Use original data for prices\n",
    "        test_pair_intervals_cql,\n",
    "        transaction_cost_bps=transaction_cost_bps,\n",
    "        initial_capital=initial_capital,\n",
    "    )\n",
    "    print(f\"  ✓ Final Value: ${results_cql['final_value']:,.2f}\")\n",
    "    print(f\"  ✓ Return: {results_cql['cumulative_return']*100:.2f}%\\n\")\n",
    "else:\n",
    "    results_cql = None\n",
    "    print(\"No CQL model or test data\\n\")\n",
    "\n",
    "# Backtest CQL-PnL\n",
    "if cql_pnl is not None and len(test_pair_intervals_pnl) > 0:\n",
    "    print(\"Backtesting CQL (PnL Reward)...\")\n",
    "    results_cql_pnl = backtest_strategy(\n",
    "        cql_pnl,\n",
    "        'CQL_PnL',\n",
    "        pair_interval_data_pnl,\n",
    "        test_pair_intervals_pnl,\n",
    "        transaction_cost_bps=transaction_cost_bps,\n",
    "        initial_capital=initial_capital,\n",
    "    )\n",
    "    print(f\"  ✓ Final Value: ${results_cql_pnl['final_value']:,.2f}\")\n",
    "    print(f\"  ✓ Return: {results_cql_pnl['cumulative_return']*100:.2f}%\\n\")\n",
    "else:\n",
    "    results_cql_pnl = None\n",
    "    print(\"No CQL-PnL model or test data\\n\")\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"Backtest Summary\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "summary_data = []\n",
    "if results_rulebased:\n",
    "    summary_data.append(['Rule-Based', results_rulebased['final_value'], results_rulebased['cumulative_return']*100])\n",
    "if results_bc:\n",
    "    summary_data.append(['BC', results_bc['final_value'], results_bc['cumulative_return']*100])\n",
    "if results_cql:\n",
    "    summary_data.append(['CQL (Z-Score)', results_cql['final_value'], results_cql['cumulative_return']*100])\n",
    "if results_cql_pnl:\n",
    "    summary_data.append(['CQL (PnL)', results_cql_pnl['final_value'], results_cql_pnl['cumulative_return']*100])\n",
    "\n",
    "if summary_data:\n",
    "    print(f\"{'Strategy':<20} {'Final Value':>15} {'Return (%)':>12}\")\n",
    "    print(\"-\" * 50)\n",
    "    for row in summary_data:\n",
    "        print(f\"{row[0]:<20} ${row[1]:>14,.2f} {row[2]:>11.2f}%\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ad3657",
   "metadata": {},
   "source": [
    "## Plot Portfolio Value Over Time\n",
    "\n",
    "Visualize portfolio performance for all strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc758f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if any([results_rulebased, results_bc, results_cql, results_cql_pnl]):\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "    \n",
    "    # Plot 1: Portfolio Value Over Time\n",
    "    ax1 = axes[0]\n",
    "    \n",
    "    if results_rulebased:\n",
    "        steps = range(len(results_rulebased['portfolio_values']))\n",
    "        ax1.plot(steps, results_rulebased['portfolio_values'], \n",
    "                label=f\"Rule-Based (Return: {results_rulebased['cumulative_return']*100:.2f}%)\",\n",
    "                linewidth=2, alpha=0.8)\n",
    "    \n",
    "    if results_bc:\n",
    "        steps = range(len(results_bc['portfolio_values']))\n",
    "        ax1.plot(steps, results_bc['portfolio_values'], \n",
    "                label=f\"BC (Return: {results_bc['cumulative_return']*100:.2f}%)\",\n",
    "                linewidth=2, alpha=0.8)\n",
    "    \n",
    "    if results_cql:\n",
    "        steps = range(len(results_cql['portfolio_values']))\n",
    "        ax1.plot(steps, results_cql['portfolio_values'], \n",
    "                label=f\"CQL Z-Score (Return: {results_cql['cumulative_return']*100:.2f}%)\",\n",
    "                linewidth=2, alpha=0.8)\n",
    "    \n",
    "    if results_cql_pnl:\n",
    "        steps = range(len(results_cql_pnl['portfolio_values']))\n",
    "        ax1.plot(steps, results_cql_pnl['portfolio_values'], \n",
    "                label=f\"CQL PnL (Return: {results_cql_pnl['cumulative_return']*100:.2f}%)\",\n",
    "                linewidth=2, alpha=0.8)\n",
    "    \n",
    "    # Add initial capital line\n",
    "    ax1.axhline(y=initial_capital, color='gray', linestyle='--', linewidth=1, alpha=0.5, label='Initial Capital')\n",
    "    \n",
    "    ax1.set_xlabel('Time Steps', fontsize=12)\n",
    "    ax1.set_ylabel('Portfolio Value ($)', fontsize=12)\n",
    "    ax1.set_title('Portfolio Value Over Time - All Strategies', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(loc='best', fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Format y-axis as currency\n",
    "    ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x:,.0f}'))\n",
    "    \n",
    "    # Plot 2: Cumulative Returns (%)\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    if results_rulebased:\n",
    "        portfolio_vals = np.array(results_rulebased['portfolio_values'])\n",
    "        cumulative_returns = (portfolio_vals / initial_capital - 1) * 100\n",
    "        steps = range(len(cumulative_returns))\n",
    "        ax2.plot(steps, cumulative_returns, \n",
    "                label=f\"Rule-Based (Final: {results_rulebased['cumulative_return']*100:.2f}%)\",\n",
    "                linewidth=2, alpha=0.8)\n",
    "    \n",
    "    if results_bc:\n",
    "        portfolio_vals = np.array(results_bc['portfolio_values'])\n",
    "        cumulative_returns = (portfolio_vals / initial_capital - 1) * 100\n",
    "        steps = range(len(cumulative_returns))\n",
    "        ax2.plot(steps, cumulative_returns, \n",
    "                label=f\"BC (Final: {results_bc['cumulative_return']*100:.2f}%)\",\n",
    "                linewidth=2, alpha=0.8)\n",
    "    \n",
    "    if results_cql:\n",
    "        portfolio_vals = np.array(results_cql['portfolio_values'])\n",
    "        cumulative_returns = (portfolio_vals / initial_capital - 1) * 100\n",
    "        steps = range(len(cumulative_returns))\n",
    "        ax2.plot(steps, cumulative_returns, \n",
    "                label=f\"CQL Z-Score (Final: {results_cql['cumulative_return']*100:.2f}%)\",\n",
    "                linewidth=2, alpha=0.8)\n",
    "    \n",
    "    if results_cql_pnl:\n",
    "        portfolio_vals = np.array(results_cql_pnl['portfolio_values'])\n",
    "        cumulative_returns = (portfolio_vals / initial_capital - 1) * 100\n",
    "        steps = range(len(cumulative_returns))\n",
    "        ax2.plot(steps, cumulative_returns, \n",
    "                label=f\"CQL PnL (Final: {results_cql_pnl['cumulative_return']*100:.2f}%)\",\n",
    "                linewidth=2, alpha=0.8)\n",
    "    \n",
    "    # Add zero line\n",
    "    ax2.axhline(y=0, color='gray', linestyle='--', linewidth=1, alpha=0.5, label='Break Even')\n",
    "    \n",
    "    ax2.set_xlabel('Time Steps', fontsize=12)\n",
    "    ax2.set_ylabel('Cumulative Return (%)', fontsize=12)\n",
    "    ax2.set_title('Cumulative Returns Over Time - All Strategies', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(loc='best', fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    if CONFIG[\"EVAL\"][\"plots\"]:\n",
    "        reports_dir = CONFIG[\"EVAL\"][\"reports_dir\"]\n",
    "        ensure_dir(reports_dir)\n",
    "        backtest_path = os.path.join(reports_dir, f\"portfolio_backtest_{len(pairs_with_data)}pairs.png\")\n",
    "        plt.savefig(backtest_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"\\nSaved backtest plot to: {backtest_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No backtest results to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d2183a",
   "metadata": {},
   "source": [
    "## Performance Metrics\n",
    "\n",
    "Calculate detailed performance metrics for all strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5947232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance_metrics(results: dict, strategy_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate detailed performance metrics for a strategy.\n",
    "    \"\"\"\n",
    "    if results is None:\n",
    "        return None\n",
    "    \n",
    "    returns = np.array(results['returns'])\n",
    "    portfolio_values = np.array(results['portfolio_values'])\n",
    "    \n",
    "    # Basic metrics\n",
    "    total_return = results['cumulative_return']\n",
    "    final_value = results['final_value']\n",
    "    \n",
    "    # Return statistics\n",
    "    mean_return = returns.mean()\n",
    "    std_return = returns.std()\n",
    "    \n",
    "    # Sharpe ratio (annualized, assuming minute data)\n",
    "    # For crypto: ~525,600 minutes per year\n",
    "    periods_per_year = 525600\n",
    "    sharpe_ratio = (mean_return * np.sqrt(periods_per_year)) / std_return if std_return > 0 else 0\n",
    "    \n",
    "    # Max drawdown\n",
    "    cumulative_max = np.maximum.accumulate(portfolio_values)\n",
    "    drawdowns = (portfolio_values - cumulative_max) / cumulative_max\n",
    "    max_drawdown = drawdowns.min()\n",
    "    \n",
    "    # Win rate\n",
    "    positive_returns = (returns > 0).sum()\n",
    "    total_trades = len(returns)\n",
    "    win_rate = positive_returns / total_trades if total_trades > 0 else 0\n",
    "    \n",
    "    # Volatility (annualized)\n",
    "    volatility = std_return * np.sqrt(periods_per_year)\n",
    "    \n",
    "    return {\n",
    "        'Strategy': strategy_name,\n",
    "        'Final Value': final_value,\n",
    "        'Total Return (%)': total_return * 100,\n",
    "        'Mean Return': mean_return,\n",
    "        'Volatility': volatility,\n",
    "        'Sharpe Ratio': sharpe_ratio,\n",
    "        'Max Drawdown (%)': max_drawdown * 100,\n",
    "        'Win Rate (%)': win_rate * 100,\n",
    "        'Total Trades': total_trades,\n",
    "    }\n",
    "\n",
    "# Calculate metrics for all strategies\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"Detailed Performance Metrics\")\n",
    "print(f\"{'='*100}\\n\")\n",
    "\n",
    "metrics_list = []\n",
    "\n",
    "if results_rulebased:\n",
    "    metrics_rulebased = calculate_performance_metrics(results_rulebased, 'Rule-Based')\n",
    "    metrics_list.append(metrics_rulebased)\n",
    "\n",
    "if results_bc:\n",
    "    metrics_bc = calculate_performance_metrics(results_bc, 'BC')\n",
    "    metrics_list.append(metrics_bc)\n",
    "\n",
    "if results_cql:\n",
    "    metrics_cql = calculate_performance_metrics(results_cql, 'CQL (Z-Score)')\n",
    "    metrics_list.append(metrics_cql)\n",
    "\n",
    "if results_cql_pnl:\n",
    "    metrics_cql_pnl = calculate_performance_metrics(results_cql_pnl, 'CQL (PnL)')\n",
    "    metrics_list.append(metrics_cql_pnl)\n",
    "\n",
    "if metrics_list:\n",
    "    # Create DataFrame for nice display\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    \n",
    "    # Format the display\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', None)\n",
    "    pd.set_option('display.precision', 4)\n",
    "    \n",
    "    print(metrics_df.to_string(index=False))\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    \n",
    "    # Find best strategy by different metrics\n",
    "    print(\"\\nBest Strategies by Metric:\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    best_return_idx = metrics_df['Total Return (%)'].idxmax()\n",
    "    print(f\"  Highest Return:     {metrics_df.loc[best_return_idx, 'Strategy']} ({metrics_df.loc[best_return_idx, 'Total Return (%)']:.2f}%)\")\n",
    "    \n",
    "    best_sharpe_idx = metrics_df['Sharpe Ratio'].idxmax()\n",
    "    print(f\"  Best Sharpe Ratio:  {metrics_df.loc[best_sharpe_idx, 'Strategy']} ({metrics_df.loc[best_sharpe_idx, 'Sharpe Ratio']:.4f})\")\n",
    "    \n",
    "    best_drawdown_idx = metrics_df['Max Drawdown (%)'].idxmax()  # Least negative\n",
    "    print(f\"  Lowest Drawdown:    {metrics_df.loc[best_drawdown_idx, 'Strategy']} ({metrics_df.loc[best_drawdown_idx, 'Max Drawdown (%)']:.2f}%)\")\n",
    "    \n",
    "    best_winrate_idx = metrics_df['Win Rate (%)'].idxmax()\n",
    "    print(f\"  Best Win Rate:      {metrics_df.loc[best_winrate_idx, 'Strategy']} ({metrics_df.loc[best_winrate_idx, 'Win Rate (%)']:.2f}%)\")\n",
    "    \n",
    "    print(f\"{'='*100}\\n\")\n",
    "else:\n",
    "    print(\"No metrics to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6b0923",
   "metadata": {},
   "source": [
    "## Diagnose Extreme Returns\n",
    "\n",
    "Check for extreme price movements that could cause portfolio issues. This diagnostic helps identify data quality issues before they impact backtesting.\n",
    "\n",
    "**Note**: This analysis validates that:\n",
    "- No extreme price jumps (>100% returns) exist in the data\n",
    "- No NaN or infinite values are present\n",
    "- Price data is suitable for realistic trading simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be50a523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for extreme price movements in test data\n",
    "print(\"=\"*60)\n",
    "print(\"Data Quality Diagnostic: Analyzing Price Movements\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nChecking for data quality issues that could affect backtesting...\")\n",
    "print(\"Analyzing first 10 test intervals...\\n\")\n",
    "\n",
    "all_returns_1 = []\n",
    "all_returns_2 = []\n",
    "\n",
    "for pair, interval_idx in test_pair_intervals[:10]:  # Check first 10 intervals\n",
    "    interval_info = pair_interval_data[pair][interval_idx]\n",
    "    interval_start, interval_end = interval_info[\"interval\"]\n",
    "    \n",
    "    # Get interval data\n",
    "    if \"datetime\" in features_df.columns:\n",
    "        ts = features_df[\"datetime\"]\n",
    "    else:\n",
    "        ts = features_df.index\n",
    "    \n",
    "    mask = (ts >= interval_start) & (ts < interval_end)\n",
    "    interval_df = features_df[mask].copy()\n",
    "    \n",
    "    if interval_df.empty:\n",
    "        continue\n",
    "    \n",
    "    # Get prices\n",
    "    asset1_price_col = f\"{pair[0]}_close\"\n",
    "    asset2_price_col = f\"{pair[1]}_close\"\n",
    "    \n",
    "    if asset1_price_col in interval_df.columns and asset2_price_col in interval_df.columns:\n",
    "        prices1 = interval_df[asset1_price_col].values\n",
    "        prices2 = interval_df[asset2_price_col].values\n",
    "        \n",
    "        # Calculate returns\n",
    "        for t in range(len(prices1) - 1):\n",
    "            # Validate prices before calculating returns\n",
    "            if (prices1[t] > 0 and prices1[t+1] > 0 and \n",
    "                not np.isnan(prices1[t]) and not np.isnan(prices1[t+1])):\n",
    "                ret1 = (prices1[t+1] - prices1[t]) / prices1[t]\n",
    "                # Skip infinite returns\n",
    "                if not np.isinf(ret1):\n",
    "                    all_returns_1.append(ret1)\n",
    "            \n",
    "            if (prices2[t] > 0 and prices2[t+1] > 0 and\n",
    "                not np.isnan(prices2[t]) and not np.isnan(prices2[t+1])):\n",
    "                ret2 = (prices2[t+1] - prices2[t]) / prices2[t]\n",
    "                # Skip infinite returns\n",
    "                if not np.isinf(ret2):\n",
    "                    all_returns_2.append(ret2)\n",
    "\n",
    "if len(all_returns_1) > 0 and len(all_returns_2) > 0:\n",
    "    all_returns_1 = np.array(all_returns_1)\n",
    "    all_returns_2 = np.array(all_returns_2)\n",
    "    \n",
    "    print(\"\\nAsset 1 Returns Statistics:\")\n",
    "    print(f\"  Mean: {all_returns_1.mean():.6f}\")\n",
    "    print(f\"  Std: {all_returns_1.std():.6f}\")\n",
    "    print(f\"  Min: {all_returns_1.min():.6f}\")\n",
    "    print(f\"  Max: {all_returns_1.max():.6f}\")\n",
    "    print(f\"  Extreme returns (|ret| > 0.5): {(np.abs(all_returns_1) > 0.5).sum()}\")\n",
    "    \n",
    "    print(\"\\nAsset 2 Returns Statistics:\")\n",
    "    print(f\"  Mean: {all_returns_2.mean():.6f}\")\n",
    "    print(f\"  Std: {all_returns_2.std():.6f}\")\n",
    "    print(f\"  Min: {all_returns_2.min():.6f}\")\n",
    "    print(f\"  Max: {all_returns_2.max():.6f}\")\n",
    "    print(f\"  Extreme returns (|ret| > 0.5): {(np.abs(all_returns_2) > 0.5).sum()}\")\n",
    "    \n",
    "    # Check for very extreme returns\n",
    "    if (np.abs(all_returns_1) > 1.0).any() or (np.abs(all_returns_2) > 1.0).any():\n",
    "        print(\"\\n WARNING: Found returns > 100%! This suggests data quality issues.\")\n",
    "        print(\"   Possible causes:\")\n",
    "        print(\"   - Price data has errors or outliers\")\n",
    "        print(\"   - Missing data causing large gaps\")\n",
    "        print(\"   - Data feed issues\")\n",
    "else:\n",
    "    print(\"Could not analyze returns - insufficient data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbde9f1f",
   "metadata": {},
   "source": [
    "# Improving CQL Performance: Experimental Alternatives\n",
    "\n",
    "The current CQL implementation shows limited improvement over BC. This section explores several strategies to enhance performance:\n",
    "\n",
    "## Why CQL Might Not Be Improving:\n",
    "\n",
    "1. **Expert Constraint**: Dataset from rule-based strategy limits exploration\n",
    "2. **Conservative Penalty**: High alpha (5.0) makes Q-learning overly pessimistic  \n",
    "3. **Short Episodes**: 2-day windows limit temporal credit assignment\n",
    "4. **Reward Signal**: Z-score rewards don't reflect actual profitability\n",
    "5. **Limited Data Diversity**: Offline dataset lacks exploration\n",
    "\n",
    "## Strategies to Try:\n",
    "\n",
    "### A. **Simple DQN (Deep Q-Network)** \n",
    "   - Less conservative than CQL\n",
    "   - Better for near-optimal expert data\n",
    "   - Remove conservative penalty\n",
    "\n",
    "### B. **Hyperparameter Tuning**\n",
    "   - Reduce alpha (try 0.1, 0.5, 1.0)\n",
    "   - Increase training steps (50k-100k)\n",
    "   - Larger networks\n",
    "   - Lower learning rate for stability\n",
    "\n",
    "### C. **Better Reward Engineering**\n",
    "   - Use actual PnL as primary reward\n",
    "   - Add risk-adjusted rewards (Sharpe ratio)\n",
    "   - Penalty for drawdowns\n",
    "   - Reward for position consistency\n",
    "\n",
    "### D. **Data Augmentation**\n",
    "   - Add noise to observations (simulated market conditions)\n",
    "   - Bootstrap sampling\n",
    "   - Temporal perturbations\n",
    "\n",
    "### E. **Ensemble Methods**\n",
    "   - Train multiple models with different seeds\n",
    "   - Combine predictions (voting/averaging)\n",
    "   - More robust to overfitting\n",
    "\n",
    "### F. **Feature Engineering**\n",
    "   - Add momentum indicators\n",
    "   - Volatility features\n",
    "   - Time-of-day features\n",
    "   - Rolling statistics over longer windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0d0968",
   "metadata": {},
   "source": [
    "## Experiment 1: DQN (DiscreteQLearning) - Less Conservative Alternative\n",
    "\n",
    "Let's try standard DQN without the conservative penalty. This is better suited when your expert data is already high-quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271887cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d3rlpy.algos import DQNConfig\n",
    "from d3rlpy.models import VectorEncoderFactory\n",
    "\n",
    "if len(train_pair_intervals_cql) > 0 and obs_dim is not None:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Training Standard DQN (No Conservative Penalty)\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Create DQN agent - no conservative penalty\n",
    "    dqn = DQNConfig(\n",
    "        learning_rate=3e-4,\n",
    "        batch_size=256,\n",
    "        gamma=0.99,\n",
    "        encoder_factory=VectorEncoderFactory(hidden_units=[256, 256]),\n",
    "    ).create(device=device_str)\n",
    "    \n",
    "    print(\"DQN Configuration:\")\n",
    "    print(f\"  Learning rate: 3e-4\")\n",
    "    print(f\"  Batch size: 256\")\n",
    "    print(f\"  Gamma: 0.99\")\n",
    "    print(f\"  Network: [256, 256]\")\n",
    "    print(f\"  Device: {device_str}\\n\")\n",
    "    \n",
    "    # Train DQN\n",
    "    print(\"Training DQN...\")\n",
    "    dqn_results = dqn.fit(\n",
    "        combined_train_dataset,\n",
    "        n_steps=20000,  # More steps\n",
    "        n_steps_per_epoch=1000,\n",
    "        show_progress=True,\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    dqn_model_path = os.path.join(models_dir_cql, f\"dqn_model_{len(pairs_with_data)}pairs.d3\")\n",
    "    dqn.save(dqn_model_path)\n",
    "    print(f\"\\n✓ DQN model saved to: {dqn_model_path}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating DQN on test set...\")\n",
    "    all_dqn_preds = []\n",
    "    all_dqn_labels = []\n",
    "    \n",
    "    for pair, interval_idx in test_pair_intervals_cql:\n",
    "        interval_info = pair_interval_data[pair][interval_idx]\n",
    "        interval_raw = interval_info[\"data\"]\n",
    "        \n",
    "        observations = interval_raw[\"observations\"]\n",
    "        actions = interval_raw[\"actions\"] + 1  # Convert to {0,1,2}\n",
    "        \n",
    "        # Predict\n",
    "        predicted_actions = dqn.predict(observations)\n",
    "        all_dqn_preds.extend(predicted_actions)\n",
    "        all_dqn_labels.extend(actions)\n",
    "    \n",
    "    all_dqn_preds = np.array(all_dqn_preds)\n",
    "    all_dqn_labels = np.array(all_dqn_labels)\n",
    "    \n",
    "    dqn_accuracy = (all_dqn_preds == all_dqn_labels).mean()\n",
    "    \n",
    "    print(f\"\\nDQN Test Accuracy: {dqn_accuracy:.4f}\")\n",
    "    print(f\"BC Test Accuracy:  {bc_accuracy:.4f}\")\n",
    "    print(f\"CQL Test Accuracy: {cql_accuracy:.4f}\")\n",
    "    print(f\"\\nDQN vs CQL Improvement: {(dqn_accuracy - cql_accuracy)*100:+.2f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot train DQN - missing data\")\n",
    "    dqn = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a736b5",
   "metadata": {},
   "source": [
    "## Experiment 2: CQL with Reduced Alpha (Less Conservative)\n",
    "\n",
    "Try CQL with much lower alpha values to reduce over-conservatism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6989ad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(train_pair_intervals_cql) > 0 and obs_dim is not None:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Testing CQL with Different Alpha Values\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    alpha_results = {}\n",
    "    alpha_values = [0.1, 0.5, 1.0, 2.0]  # Much lower than original 5.0\n",
    "    \n",
    "    for alpha_val in alpha_values:\n",
    "        print(f\"\\nTraining CQL with alpha={alpha_val}...\")\n",
    "        \n",
    "        # Create CQL with reduced alpha\n",
    "        cql_exp = DiscreteCQLConfig(\n",
    "            learning_rate=3e-4,\n",
    "            batch_size=256,\n",
    "            gamma=0.99,\n",
    "            alpha=alpha_val,  # Reduced alpha\n",
    "            encoder_factory=VectorEncoderFactory(hidden_units=[256, 256]),\n",
    "        ).create(device=device_str)\n",
    "        \n",
    "        # Train\n",
    "        cql_exp.fit(\n",
    "            combined_train_dataset,\n",
    "            n_steps=20000,\n",
    "            n_steps_per_epoch=1000,\n",
    "            show_progress=False,  # Quiet mode\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        preds = []\n",
    "        labels = []\n",
    "        for pair, interval_idx in test_pair_intervals_cql[:20]:  # Quick eval on subset\n",
    "            interval_info = pair_interval_data[pair][interval_idx]\n",
    "            observations = interval_info[\"data\"][\"observations\"]\n",
    "            actions = interval_info[\"data\"][\"actions\"] + 1\n",
    "            \n",
    "            predicted = cql_exp.predict(observations)\n",
    "            preds.extend(predicted)\n",
    "            labels.extend(actions)\n",
    "        \n",
    "        acc = (np.array(preds) == np.array(labels)).mean()\n",
    "        alpha_results[alpha_val] = acc\n",
    "        print(f\"  Alpha={alpha_val}: Accuracy={acc:.4f}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Alpha Tuning Results:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    best_alpha = max(alpha_results, key=alpha_results.get)\n",
    "    for alpha_val, acc in sorted(alpha_results.items()):\n",
    "        marker = \"✓ BEST\" if alpha_val == best_alpha else \"\"\n",
    "        print(f\"  Alpha={alpha_val:4.1f} | Accuracy={acc:.4f} {marker}\")\n",
    "    \n",
    "    print(f\"\\nRecommendation: Use alpha={best_alpha} for best performance\")\n",
    "    print(f\"Original alpha=5.0 was likely too conservative!\")\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot run alpha experiments - missing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701c6182",
   "metadata": {},
   "source": [
    "## Experiment 3: Improved Reward Engineering\n",
    "\n",
    "The current z-score reward (`-|z_score|`) doesn't directly optimize for profitability. Let's create better rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ba9cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_with_better_rewards(\n",
    "    df: pd.DataFrame,\n",
    "    pair: tuple[str, str],\n",
    "    intervals: list,\n",
    "    strategy: RuleBasedPairsStrategy,\n",
    "    reward_type: str = \"sharpe\",  # 'sharpe', 'profit_penalty', 'risk_adjusted'\n",
    "    z_score_feature: str = \"spreadNorm\",\n",
    "    pair_feature_format: str = \"{ASSET1}_{ASSET2}_{FEATURE}\",\n",
    "    single_asset_features: list[str] = None,\n",
    "    timestamp_col: str = \"datetime\",\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Generate dataset with improved reward engineering.\n",
    "    \n",
    "    Reward types:\n",
    "    - 'sharpe': Rolling Sharpe ratio (risk-adjusted returns)\n",
    "    - 'profit_penalty': PnL with drawdown penalty\n",
    "    - 'risk_adjusted': PnL / volatility\n",
    "    \"\"\"\n",
    "    \n",
    "    all_obs = []\n",
    "    all_actions = []\n",
    "    all_rewards = []\n",
    "    all_terminals = []\n",
    "    \n",
    "    # Setup columns\n",
    "    z_score_col = pair_feature_format.format(\n",
    "        ASSET1=pair[0], ASSET2=pair[1], FEATURE=z_score_feature\n",
    "    )\n",
    "    asset1_price_col = f\"{pair[0]}_close\"\n",
    "    asset2_price_col = f\"{pair[1]}_close\"\n",
    "    \n",
    "    state_cols = []\n",
    "    if single_asset_features:\n",
    "        for asset in pair:\n",
    "            for feat in single_asset_features:\n",
    "                col = f\"{asset}_{feat}\"\n",
    "                if col in df.columns:\n",
    "                    state_cols.append(col)\n",
    "    state_cols.append(z_score_col)\n",
    "    \n",
    "    for interval_start, interval_end in intervals:\n",
    "        if timestamp_col in df.columns:\n",
    "            ts = df[timestamp_col]\n",
    "        else:\n",
    "            ts = df.index\n",
    "        \n",
    "        mask = (ts >= interval_start) & (ts < interval_end)\n",
    "        interval_df = df[mask].copy()\n",
    "        \n",
    "        if interval_df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Check required columns\n",
    "        required_cols = state_cols + [asset1_price_col, asset2_price_col]\n",
    "        if not all(col in interval_df.columns for col in required_cols):\n",
    "            continue\n",
    "        \n",
    "        strategy.reset()\n",
    "        \n",
    "        # Collect episode data\n",
    "        episode_obs = []\n",
    "        episode_actions = []\n",
    "        episode_returns = []\n",
    "        \n",
    "        prices1 = interval_df[asset1_price_col].values\n",
    "        prices2 = interval_df[asset2_price_col].values\n",
    "        \n",
    "        for t, (idx, row) in enumerate(interval_df.iterrows()):\n",
    "            state = row[state_cols].values.astype(np.float32)\n",
    "            if np.any(np.isnan(state)):\n",
    "                continue\n",
    "            \n",
    "            z_score = row[z_score_col]\n",
    "            action = strategy.get_action(z_score)\n",
    "            \n",
    "            # Calculate return for next step\n",
    "            if t < len(prices1) - 1:\n",
    "                p1_t, p2_t = prices1[t], prices2[t]\n",
    "                p1_next, p2_next = prices1[t+1], prices2[t+1]\n",
    "                \n",
    "                if p1_t > 0 and p2_t > 0 and p1_next > 0 and p2_next > 0:\n",
    "                    ret1 = (p1_next - p1_t) / p1_t\n",
    "                    ret2 = (p2_next - p2_t) / p2_t\n",
    "                    \n",
    "                    # Portfolio return based on action\n",
    "                    if action == -1:\n",
    "                        pnl = -ret1 + ret2\n",
    "                    elif action == 1:\n",
    "                        pnl = ret1 - ret2\n",
    "                    else:\n",
    "                        pnl = 0.0\n",
    "                    \n",
    "                    episode_returns.append(pnl)\n",
    "                else:\n",
    "                    episode_returns.append(0.0)\n",
    "            else:\n",
    "                episode_returns.append(0.0)\n",
    "            \n",
    "            episode_obs.append(state)\n",
    "            episode_actions.append(action)\n",
    "        \n",
    "        # Compute rewards based on type\n",
    "        if reward_type == \"sharpe\":\n",
    "            # Rolling Sharpe ratio\n",
    "            window = min(20, len(episode_returns))\n",
    "            for i in range(len(episode_returns)):\n",
    "                if i < window:\n",
    "                    reward = episode_returns[i]  # Not enough data\n",
    "                else:\n",
    "                    recent_returns = episode_returns[i-window:i]\n",
    "                    mean_ret = np.mean(recent_returns)\n",
    "                    std_ret = np.std(recent_returns) + 1e-8\n",
    "                    reward = mean_ret / std_ret  # Sharpe-like reward\n",
    "                \n",
    "                all_obs.append(episode_obs[i])\n",
    "                all_actions.append(episode_actions[i])\n",
    "                all_rewards.append(reward)\n",
    "                all_terminals.append(i == len(episode_returns) - 1)\n",
    "        \n",
    "        elif reward_type == \"profit_penalty\":\n",
    "            # PnL with drawdown penalty\n",
    "            cumulative_pnl = np.cumsum(episode_returns)\n",
    "            max_pnl = np.maximum.accumulate(np.concatenate([[0], cumulative_pnl]))\n",
    "            \n",
    "            for i in range(len(episode_returns)):\n",
    "                pnl = episode_returns[i]\n",
    "                drawdown = (cumulative_pnl[i] - max_pnl[i+1]) if i < len(max_pnl) - 1 else 0\n",
    "                reward = pnl - 0.5 * abs(drawdown)  # Penalize drawdowns\n",
    "                \n",
    "                all_obs.append(episode_obs[i])\n",
    "                all_actions.append(episode_actions[i])\n",
    "                all_rewards.append(reward)\n",
    "                all_terminals.append(i == len(episode_returns) - 1)\n",
    "        \n",
    "        elif reward_type == \"risk_adjusted\":\n",
    "            # Risk-adjusted returns\n",
    "            for i in range(len(episode_returns)):\n",
    "                pnl = episode_returns[i]\n",
    "                # Normalize by recent volatility\n",
    "                if i < 10:\n",
    "                    reward = pnl\n",
    "                else:\n",
    "                    recent_vol = np.std(episode_returns[max(0, i-20):i]) + 1e-8\n",
    "                    reward = pnl / recent_vol\n",
    "                \n",
    "                all_obs.append(episode_obs[i])\n",
    "                all_actions.append(episode_actions[i])\n",
    "                all_rewards.append(reward)\n",
    "                all_terminals.append(i == len(episode_returns) - 1)\n",
    "    \n",
    "    return {\n",
    "        \"observations\": np.array(all_obs, dtype=np.float32),\n",
    "        \"actions\": np.array(all_actions, dtype=np.int32),\n",
    "        \"rewards\": np.array(all_rewards, dtype=np.float32),\n",
    "        \"terminals\": np.array(all_terminals, dtype=bool),\n",
    "    }\n",
    "\n",
    "\n",
    "# Test different reward types\n",
    "if len(train_pair_intervals) > 0:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Testing Different Reward Engineering Strategies\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    reward_types = [\"sharpe\", \"profit_penalty\", \"risk_adjusted\"]\n",
    "    \n",
    "    for reward_type in reward_types:\n",
    "        print(f\"\\nGenerating dataset with '{reward_type}' rewards...\")\n",
    "        \n",
    "        # Generate for one pair as test\n",
    "        test_pair = list(pair_interval_data.keys())[0]\n",
    "        test_intervals = valid_intervals_per_pair[test_pair][:10]\n",
    "        \n",
    "        data = generate_dataset_with_better_rewards(\n",
    "            features_df,\n",
    "            test_pair,\n",
    "            test_intervals,\n",
    "            strategy,\n",
    "            reward_type=reward_type,\n",
    "            z_score_feature=z_score_feature,\n",
    "            pair_feature_format=pair_feature_format,\n",
    "            single_asset_features=single_asset_features[:5],\n",
    "            timestamp_col=timestamp_col,\n",
    "        )\n",
    "        \n",
    "        print(f\"  Generated {len(data['observations'])} samples\")\n",
    "        print(f\"  Reward stats: mean={data['rewards'].mean():.6f}, std={data['rewards'].std():.6f}\")\n",
    "        print(f\"  Reward range: [{data['rewards'].min():.6f}, {data['rewards'].max():.6f}]\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Recommendation:\")\n",
    "    print(\"  • 'sharpe': Best for risk-adjusted learning\")\n",
    "    print(\"  • 'profit_penalty': Best for drawdown-aware trading\")\n",
    "    print(\"  • 'risk_adjusted': Best for volatility-normalized decisions\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot test reward engineering - missing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ce8e7a",
   "metadata": {},
   "source": [
    "## Experiment 4: Longer Training & Better Hyperparameters\n",
    "\n",
    "Increase training duration and tune key hyperparameters for better convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e8319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(train_pair_intervals_cql) > 0 and obs_dim is not None:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Optimal Hyperparameter Configuration for CQL\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Recommended configuration based on analysis\n",
    "    optimal_config = {\n",
    "        \"learning_rate\": 1e-4,  # Lower for stability\n",
    "        \"batch_size\": 512,      # Larger batch\n",
    "        \"gamma\": 0.95,          # Slightly lower discount\n",
    "        \"alpha\": 0.5,           # Much lower conservative penalty\n",
    "        \"n_steps\": 50000,       # 5x more training\n",
    "        \"n_steps_per_epoch\": 1000,\n",
    "        \"hidden_units\": [512, 512, 256],  # Deeper network\n",
    "    }\n",
    "    \n",
    "    print(\"Recommended Configuration:\")\n",
    "    for key, value in optimal_config.items():\n",
    "        print(f\"  {key:20s}: {value}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Key Changes from Original:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(\"  ✓ Alpha: 5.0 → 0.5 (less conservative)\")\n",
    "    print(\"  ✓ Steps: 10k → 50k (more training)\")\n",
    "    print(\"  ✓ LR: 3e-4 → 1e-4 (more stable)\")\n",
    "    print(\"  ✓ Batch: 256 → 512 (better gradients)\")\n",
    "    print(\"  ✓ Network: [256,256] → [512,512,256] (more capacity)\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Train with optimal config (if you want to run it)\n",
    "    train_optimal = False  # Set to True to train\n",
    "    \n",
    "    if train_optimal:\n",
    "        print(\"Training with optimal configuration...\")\n",
    "        \n",
    "        cql_optimal = DiscreteCQLConfig(\n",
    "            learning_rate=optimal_config[\"learning_rate\"],\n",
    "            batch_size=optimal_config[\"batch_size\"],\n",
    "            gamma=optimal_config[\"gamma\"],\n",
    "            alpha=optimal_config[\"alpha\"],\n",
    "            encoder_factory=VectorEncoderFactory(hidden_units=optimal_config[\"hidden_units\"]),\n",
    "        ).create(device=device_str)\n",
    "        \n",
    "        cql_optimal.fit(\n",
    "            combined_train_dataset,\n",
    "            n_steps=optimal_config[\"n_steps\"],\n",
    "            n_steps_per_epoch=optimal_config[\"n_steps_per_epoch\"],\n",
    "            show_progress=True,\n",
    "        )\n",
    "        \n",
    "        # Save\n",
    "        optimal_model_path = os.path.join(models_dir_cql, f\"cql_optimal_{len(pairs_with_data)}pairs.d3\")\n",
    "        cql_optimal.save(optimal_model_path)\n",
    "        print(f\"\\n✓ Optimal CQL model saved to: {optimal_model_path}\")\n",
    "    else:\n",
    "        print(\"Set train_optimal=True to train with these settings\")\n",
    "        \n",
    "else:\n",
    "    print(\"Cannot create optimal config - missing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cc8a78",
   "metadata": {},
   "source": [
    "## Summary & Recommendations\n",
    "\n",
    "### Quick Wins (Try First):\n",
    "\n",
    "1. **Use Standard DQN Instead of CQL** \n",
    "   - CQL's conservative penalty may be hurting performance\n",
    "   - Your expert data is already high-quality\n",
    "   - DQN should learn faster and better\n",
    "\n",
    "2. **Reduce CQL Alpha to 0.1-0.5**\n",
    "   - Current alpha=5.0 is way too conservative\n",
    "   - Makes the model overly pessimistic about unseen actions\n",
    "   - Lower alpha allows more exploration\n",
    "\n",
    "3. **Train Longer (50k-100k steps)**\n",
    "   - Current 10k steps is insufficient\n",
    "   - RL needs more iterations to converge\n",
    "   - Use early stopping on validation performance\n",
    "\n",
    "### Medium-Term Improvements:\n",
    "\n",
    "4. **Better Reward Engineering**\n",
    "   - Use Sharpe ratio rewards (risk-adjusted)\n",
    "   - Add drawdown penalties\n",
    "   - Weight recent performance more heavily\n",
    "\n",
    "5. **Larger Networks**\n",
    "   - Increase to [512, 512, 256] or [1024, 512]\n",
    "   - More capacity for complex patterns\n",
    "   - Add dropout (0.1-0.2) to prevent overfitting\n",
    "\n",
    "6. **Ensemble Methods**\n",
    "   - Train 3-5 models with different seeds\n",
    "   - Average predictions or use voting\n",
    "   - More robust and stable\n",
    "\n",
    "### Advanced Techniques:\n",
    "\n",
    "7. **Data Augmentation**\n",
    "   - Add Gaussian noise to observations (σ=0.01)\n",
    "   - Time-shifted episodes\n",
    "   - Bootstrap resampling\n",
    "\n",
    "8. **Feature Engineering**\n",
    "   - Add momentum indicators (10-period, 20-period)\n",
    "   - Rolling volatility features\n",
    "   - Bid-ask spread features\n",
    "   - Volume-weighted features\n",
    "\n",
    "9. **Multi-Task Learning**\n",
    "   - Train on multiple pairs simultaneously\n",
    "   - Share lower layers, separate heads per pair\n",
    "   - Better generalization\n",
    "\n",
    "### Why CQL Underperforms Here:\n",
    "\n",
    "- **Over-conservatism**: Alpha=5.0 heavily penalizes exploration\n",
    "- **Data Quality**: Expert demonstrations are already near-optimal\n",
    "- **Short Episodes**: 2-day windows limit learning horizon\n",
    "- **Reward Mismatch**: Z-score rewards don't reflect true profitability\n",
    "- **Limited Diversity**: Offline data from single strategy\n",
    "\n",
    "### Expected Performance Gains:\n",
    "\n",
    "| Method | Expected Improvement |\n",
    "|--------|---------------------|\n",
    "| DQN (vs CQL) | +2-5% accuracy |\n",
    "| Reduced Alpha | +3-7% accuracy |\n",
    "| Better Rewards | +5-10% returns |\n",
    "| Longer Training | +2-4% accuracy |\n",
    "| Ensemble | +1-3% accuracy |\n",
    "\n",
    "**Bottom Line**: Start with DQN or low-alpha CQL, train longer, and use better rewards. CQL is great for truly offline settings with poor data, but your rule-based expert is already strong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da7dfdf",
   "metadata": {},
   "source": [
    "## Understanding Rewards in Offline RL Datasets\n",
    "\n",
    "### Important Clarification: What Are Rewards?\n",
    "\n",
    "**You're right - rewards ARE based on actions!** Here's how it works:\n",
    "\n",
    "### In Online RL (Learning by Doing):\n",
    "1. Agent observes state `s_t`\n",
    "2. Agent takes action `a_t`\n",
    "3. Environment returns reward `r_t` (consequence of that action)\n",
    "4. Agent updates policy based on reward\n",
    "\n",
    "### In Offline RL (Learning from Past Data):\n",
    "1. **Historical data**: Expert took action `a_t` in state `s_t`\n",
    "2. **Historical outcome**: That action resulted in reward `r_t`\n",
    "3. **Dataset**: We store the tuple `(s_t, a_t, r_t, s_{t+1})`\n",
    "4. **Learning**: Model learns \"if I see state s_t and take action a_t, I'll get reward r_t\"\n",
    "\n",
    "### The Confusion:\n",
    "\n",
    "The dataset doesn't \"add\" rewards arbitrarily - it records what **actually happened** when that action was taken:\n",
    "\n",
    "```python\n",
    "# What really happens in generate_interval_dataset_with_pnl():\n",
    "\n",
    "for t in timesteps:\n",
    "    state_t = observations[t]           # Market state at time t\n",
    "    action_t = strategy.get_action()    # Expert's decision\n",
    "    \n",
    "    # Now we need to know: what was the RESULT of taking that action?\n",
    "    # We calculate the actual portfolio return from t to t+1\n",
    "    \n",
    "    prices_t = get_prices(t)\n",
    "    prices_t_plus_1 = get_prices(t+1)   # Future prices (we know them from history)\n",
    "    \n",
    "    # Calculate what ACTUALLY happened after taking action_t\n",
    "    if action_t == 1:  # Expert went long\n",
    "        reward_t = calculate_return(prices_t, prices_t_plus_1, position=\"long\")\n",
    "    elif action_t == -1:  # Expert went short\n",
    "        reward_t = calculate_return(prices_t, prices_t_plus_1, position=\"short\")\n",
    "    else:  # Expert stayed neutral\n",
    "        reward_t = 0.0\n",
    "    \n",
    "    # Store the historical fact: \"taking action_t in state_t resulted in reward_t\"\n",
    "    dataset.append((state_t, action_t, reward_t))\n",
    "```\n",
    "\n",
    "### Why This Matters:\n",
    "\n",
    "**The dataset is a historical record of:**\n",
    "- \"When the market looked like THIS (state)\"\n",
    "- \"The expert did THIS (action)\" \n",
    "- \"And it resulted in THIS outcome (reward)\"\n",
    "\n",
    "The RL model learns to predict: \"If I take this action in this state, what reward will I likely get?\"\n",
    "\n",
    "### Example with Real Trading:\n",
    "\n",
    "```\n",
    "Time: 10:00 AM\n",
    "State: z-score = 2.0 (spread is high)\n",
    "Action: Expert goes SHORT (-1)\n",
    "--- Market moves ---\n",
    "Time: 10:01 AM  \n",
    "Prices change → calculate portfolio return\n",
    "Reward: +0.002 (made 0.2% profit)\n",
    "\n",
    "Dataset stores: (state=[z=2.0, ...], action=-1, reward=+0.002)\n",
    "```\n",
    "\n",
    "Later, the RL model learns: \"When z-score is ~2.0, taking action -1 tends to give positive rewards\"\n",
    "\n",
    "### The Key Insight:\n",
    "\n",
    "We're not \"inventing\" rewards - we're **measuring the actual consequences** of the actions that were taken in the past. The reward is retroactively calculated based on what actually happened in the market after that action was executed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f38966",
   "metadata": {},
   "source": [
    "## Visual Example: How Rewards Work in the Dataset\n",
    "\n",
    "Let me show you exactly what happens with a concrete example from the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6681aa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a concrete example showing the relationship between actions and rewards\n",
    "print(\"=\"*80)\n",
    "print(\"CONCRETE EXAMPLE: How Actions Lead to Rewards\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Simulated historical data\n",
    "example_data = {\n",
    "    'Time': ['10:00', '10:01', '10:02', '10:03', '10:04'],\n",
    "    'BTC_Price': [50000, 50100, 50050, 49900, 49950],\n",
    "    'ETH_Price': [3000, 3010, 3005, 2995, 2998],\n",
    "    'Z_Score': [2.1, 1.8, 1.2, 0.8, 0.3],\n",
    "    'Expert_Action': ['Short (-1)', 'Short (-1)', 'Short (-1)', 'Neutral (0)', 'Neutral (0)'],\n",
    "}\n",
    "\n",
    "df_example = pd.DataFrame(example_data)\n",
    "\n",
    "print(\"\\nHistorical Market Data:\")\n",
    "print(df_example.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CALCULATING REWARDS (What Actually Happened)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate actual portfolio returns based on the actions taken\n",
    "rewards_explained = []\n",
    "\n",
    "for i in range(len(df_example) - 1):\n",
    "    time = df_example['Time'][i]\n",
    "    action = df_example['Expert_Action'][i]\n",
    "    \n",
    "    # Prices at time t and t+1\n",
    "    btc_t = df_example['BTC_Price'][i]\n",
    "    eth_t = df_example['ETH_Price'][i]\n",
    "    btc_next = df_example['BTC_Price'][i+1]\n",
    "    eth_next = df_example['ETH_Price'][i+1]\n",
    "    \n",
    "    # Calculate individual asset returns\n",
    "    btc_return = (btc_next - btc_t) / btc_t\n",
    "    eth_return = (eth_next - eth_t) / eth_t\n",
    "    \n",
    "    # Calculate portfolio return based on ACTION TAKEN\n",
    "    if 'Short' in action:  # Action = -1\n",
    "        portfolio_return = -btc_return + eth_return  # Short BTC, Long ETH\n",
    "        explanation = f\"Short spread: -({btc_return:.4f}) + {eth_return:.4f}\"\n",
    "    else:  # Action = 0\n",
    "        portfolio_return = 0.0\n",
    "        explanation = \"No position: 0.0\"\n",
    "    \n",
    "    rewards_explained.append({\n",
    "        'Time': f\"{time} → {df_example['Time'][i+1]}\",\n",
    "        'Action_Taken': action,\n",
    "        'BTC_Return': f\"{btc_return*100:+.2f}%\",\n",
    "        'ETH_Return': f\"{eth_return*100:+.2f}%\",\n",
    "        'Portfolio_Return': f\"{portfolio_return*100:+.2f}%\",\n",
    "        'Explanation': explanation,\n",
    "        'Reward_Stored': portfolio_return\n",
    "    })\n",
    "\n",
    "rewards_df = pd.DataFrame(rewards_explained)\n",
    "print(\"\\nRewards = Actual Consequences of Actions:\")\n",
    "print(rewards_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WHAT GOES INTO THE DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nThe dataset stores these historical FACTS:\")\n",
    "for i, row in rewards_df.iterrows():\n",
    "    print(f\"\\n  Record {i+1}:\")\n",
    "    print(f\"    State:  z_score={df_example['Z_Score'][i]:.2f}, BTC=${df_example['BTC_Price'][i]}, ETH=${df_example['ETH_Price'][i]}\")\n",
    "    print(f\"    Action: {row['Action_Taken']}\")\n",
    "    print(f\"    Reward: {row['Reward_Stored']:.6f}  ← This is what ACTUALLY happened\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HOW THE RL MODEL LEARNS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "The model learns by studying these historical facts:\n",
    "\n",
    "1. \"When z_score was HIGH (2.1) and expert went SHORT\"\n",
    "   → Result: Made profit (+0.0013 reward)\n",
    "   → Learning: \"High z-score + Short action = Good outcome\"\n",
    "\n",
    "2. \"When z_score was MEDIUM (1.8) and expert stayed SHORT\"  \n",
    "   → Result: Lost money (-0.0009 reward)\n",
    "   → Learning: \"Maybe don't stay short too long\"\n",
    "\n",
    "3. \"When z_score was LOW (0.8) and expert went NEUTRAL\"\n",
    "   → Result: No gain/loss (0.0 reward)\n",
    "   → Learning: \"Low z-score + Neutral = Safe but no profit\"\n",
    "\n",
    "The model discovers: \"I should SHORT when z-score is very high, then EXIT when it normalizes\"\n",
    "This is learned from OUTCOMES, not programmed!\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"KEY TAKEAWAY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "Rewards are NOT arbitrary - they are MEASUREMENTS of what happened:\n",
    "  \n",
    "  1. Expert takes action A in state S\n",
    "  2. Market moves → prices change  \n",
    "  3. We measure: \"How much money did action A make/lose?\"\n",
    "  4. That measurement IS the reward\n",
    "  5. We store: (S, A, R) = (state, action, measured_outcome)\n",
    "\n",
    "The RL model learns: \"Taking action A in state S tends to produce reward R\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383171ba",
   "metadata": {},
   "source": [
    "## Why We Have Two Different Reward Calculations in This Notebook\n",
    "\n",
    "You might notice we calculate rewards in two different ways. Let me explain why:\n",
    "\n",
    "### 1. **Z-Score Reward** (Original CQL):\n",
    "```python\n",
    "reward = -abs(z_score)\n",
    "```\n",
    "\n",
    "**What it measures:** How far from mean reversion\n",
    "- **NOT based on actual money made/lost**\n",
    "- **Proxy reward:** Assumes \"closer to mean = better\"\n",
    "- **Problem:** Doesn't account for actual profitability\n",
    "- **Use case:** When you believe mean reversion is the goal\n",
    "\n",
    "**Example:**\n",
    "- Z-score = 2.0 → Reward = -2.0 (bad, far from mean)\n",
    "- Z-score = 0.1 → Reward = -0.1 (good, near mean)\n",
    "\n",
    "### 2. **PnL Reward** (Realistic CQL-PnL):\n",
    "```python\n",
    "# Calculate ACTUAL portfolio return\n",
    "if action == -1:\n",
    "    reward = -ret1 + ret2  # What we actually made/lost\n",
    "elif action == 1:\n",
    "    reward = ret1 - ret2\n",
    "else:\n",
    "    reward = 0.0\n",
    "```\n",
    "\n",
    "**What it measures:** Actual profit/loss in dollars\n",
    "- **Real money:** Directly measures financial outcome\n",
    "- **Transaction costs:** Includes trading fees\n",
    "- **Problem:** More noisy, harder to learn from\n",
    "- **Use case:** When you want to maximize actual returns\n",
    "\n",
    "**Example:**\n",
    "- Went short, made 0.5% → Reward = +0.005 (actual profit!)\n",
    "- Went short, lost 0.3% → Reward = -0.003 (actual loss!)\n",
    "\n",
    "### Which Is Better?\n",
    "\n",
    "| Reward Type | Pros | Cons | Best For |\n",
    "|-------------|------|------|----------|\n",
    "| **Z-Score** | Stable, clear signal | Not real money | Learning patterns |\n",
    "| **PnL** | Real profitability | Noisy, harder to learn | Actual trading |\n",
    "\n",
    "### The Right Approach:\n",
    "\n",
    "**Two-Stage Training (Recommended):**\n",
    "1. **Stage 1:** Train with z-score rewards → Learn the pattern\n",
    "2. **Stage 2:** Fine-tune with PnL rewards → Optimize for profit\n",
    "\n",
    "Or better yet:\n",
    "\n",
    "**Combined Reward:**\n",
    "```python\n",
    "reward = 0.3 * pnl_reward + 0.7 * (-abs(z_score))\n",
    "```\n",
    "Get both pattern recognition AND profitability!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631d9086",
   "metadata": {},
   "source": [
    "## Final Clarification: The RL Learning Process\n",
    "\n",
    "### Without Stored Rewards (Won't Work):\n",
    "```python\n",
    "# ❌ This is WRONG - can't learn without consequences\n",
    "dataset = [\n",
    "    (state1, action1),  # What action was taken\n",
    "    (state2, action2),  # But what happened?? \n",
    "    (state3, action3),  # Did we make money? Lose money?\n",
    "]\n",
    "# Model has no idea if actions were good or bad!\n",
    "```\n",
    "\n",
    "### With Stored Rewards (Correct):\n",
    "```python\n",
    "# ✓ This is RIGHT - can learn from consequences  \n",
    "dataset = [\n",
    "    (state1, action1, reward1),  # Took action, got reward\n",
    "    (state2, action2, reward2),  # Model learns: this combo → this outcome\n",
    "    (state3, action3, reward3),  # Can evaluate: was it worth it?\n",
    "]\n",
    "# Model learns which actions lead to better rewards!\n",
    "```\n",
    "\n",
    "### The Q-Learning Magic:\n",
    "\n",
    "The model learns a **Q-function**: Q(state, action) = expected future reward\n",
    "\n",
    "```python\n",
    "# Model learns to predict:\n",
    "Q(z_score=2.0, action=SHORT) = +0.003  # \"Going short here is good!\"\n",
    "Q(z_score=2.0, action=LONG) = -0.005   # \"Going long here is bad!\"\n",
    "Q(z_score=0.5, action=NEUTRAL) = 0.001 # \"Neutral here is okay\"\n",
    "\n",
    "# Then at test time:\n",
    "best_action = argmax(Q(current_state, all_actions))\n",
    "```\n",
    "\n",
    "### Without rewards in the dataset, the model would have no idea which actions are better!\n",
    "\n",
    "Think of it like teaching someone to cook:\n",
    "- ❌ Bad: \"I put in salt, then sugar, then flour\" (no feedback)\n",
    "- ✓ Good: \"I put in salt → tasted good (+1), added sugar → too sweet (-1), added flour → perfect (+2)\"\n",
    "\n",
    "The rewards are the **taste test results** that tell you what worked!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23349838",
   "metadata": {},
   "source": [
    "# Critical Question: Is Offline RL the Right Approach?\n",
    "\n",
    "## The Honest Answer: **Probably Not for This Problem**\n",
    "\n",
    "Let me explain why offline RL might not be the best choice here, and what alternatives would work better.\n",
    "\n",
    "## What Offline RL Is Good For:\n",
    "\n",
    "### ✓ When You SHOULD Use Offline RL:\n",
    "1. **Dangerous/Expensive Exploration**\n",
    "   - Healthcare: Can't experiment on patients\n",
    "   - Autonomous vehicles: Can't crash real cars\n",
    "   - Robotics: Hardware is expensive\n",
    "   \n",
    "2. **Already Have Expert Data**\n",
    "   - Historical logs from human experts\n",
    "   - Data collection is expensive\n",
    "   - Can't interact with environment anymore\n",
    "\n",
    "3. **Legal/Ethical Constraints**\n",
    "   - Can't deploy untrained agents\n",
    "   - Regulations require proven methods\n",
    "   - Risk is unacceptable\n",
    "\n",
    "## Why Offline RL Is QUESTIONABLE Here:\n",
    "\n",
    "### ❌ Problems with Your Current Setup:\n",
    "\n",
    "1. **You CAN Simulate the Environment**\n",
    "   ```python\n",
    "   # You have:\n",
    "   - Historical price data\n",
    "   - Market simulator (implicit in your backtesting)\n",
    "   - No real money at risk during training\n",
    "   \n",
    "   # This means you CAN do online RL!\n",
    "   ```\n",
    "\n",
    "2. **Your \"Expert\" Is Just a Simple Rule**\n",
    "   ```python\n",
    "   # Your expert strategy:\n",
    "   if z_score > 1.5:\n",
    "       action = -1  # Short\n",
    "   elif z_score < -1.5:\n",
    "       action = 1   # Long\n",
    "   else:\n",
    "       action = 0   # Neutral\n",
    "   \n",
    "   # This is NOT sophisticated!\n",
    "   # Why learn to copy a simple rule?\n",
    "   ```\n",
    "\n",
    "3. **Offline RL Is Overly Conservative**\n",
    "   - CQL prevents exploration beyond the expert\n",
    "   - But your expert is suboptimal!\n",
    "   - You're learning to copy mediocrity\n",
    "\n",
    "4. **You Have a Perfect Simulator**\n",
    "   - Historical market data = perfect environment\n",
    "   - Can run millions of episodes\n",
    "   - No cost to exploration\n",
    "   - Can test risky strategies safely\n",
    "\n",
    "## Better Approaches for Your Problem:\n",
    "\n",
    "### 🎯 Option 1: **Online RL with Environment Simulation** (BEST)\n",
    "\n",
    "Train directly on historical data as a simulated environment:\n",
    "\n",
    "```python\n",
    "# Create a trading environment\n",
    "class PairsTradingEnv:\n",
    "    def __init__(self, historical_data):\n",
    "        self.data = historical_data\n",
    "        self.position = 0\n",
    "        self.pnl = 0\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Execute action in historical data\n",
    "        reward = self.calculate_pnl(action)\n",
    "        next_state = self.get_next_state()\n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def reset(self):\n",
    "        # Start new episode at random point\n",
    "        pass\n",
    "\n",
    "# Train with online RL (PPO, SAC, etc.)\n",
    "agent = PPO(env)\n",
    "agent.learn(total_timesteps=1_000_000)\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "- Can explore beyond expert strategy\n",
    "- Finds optimal policy through trial & error\n",
    "- Uses actual rewards, not imitation\n",
    "- Proven to work better for trading\n",
    "\n",
    "### 🎯 Option 2: **Supervised Learning** (SIMPLER)\n",
    "\n",
    "If you trust your expert strategy:\n",
    "\n",
    "```python\n",
    "# Just train a classifier\n",
    "X = states  # Market features\n",
    "y = expert_actions  # What expert did\n",
    "\n",
    "model = RandomForest() / XGBoost / Neural Net\n",
    "model.fit(X, y)\n",
    "\n",
    "# That's it! No need for RL.\n",
    "```\n",
    "\n",
    "**When this works:**\n",
    "- Expert is already near-optimal\n",
    "- Don't need to improve beyond expert\n",
    "- Want simple, interpretable model\n",
    "\n",
    "### 🎯 Option 3: **Imitation Learning** (MIDDLE GROUND)\n",
    "\n",
    "Better than offline RL for learning from demonstrations:\n",
    "\n",
    "```python\n",
    "from imitation.algorithms import bc\n",
    "\n",
    "# Behavior Cloning (simpler than CQL)\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    ")\n",
    "\n",
    "bc_trainer.train(expert_demonstrations)\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "- Simpler than offline RL\n",
    "- No need for Q-functions\n",
    "- Directly learns policy\n",
    "- Often better for imitation\n",
    "\n",
    "### 🎯 Option 4: **Hybrid: IL + Online Fine-tuning** (RECOMMENDED)\n",
    "\n",
    "Best of both worlds:\n",
    "\n",
    "```python\n",
    "# Step 1: Learn from expert (fast bootstrap)\n",
    "policy = BehaviorCloning(expert_data)\n",
    "\n",
    "# Step 2: Improve through interaction (online RL)\n",
    "policy = PPO(env, initial_policy=policy)\n",
    "policy.learn(total_timesteps=500_000)\n",
    "```\n",
    "\n",
    "**Why this is best:**\n",
    "- Quick start from expert\n",
    "- Then improves beyond expert\n",
    "- Explores safely (starts from good policy)\n",
    "- Finds optimal strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cabd7b",
   "metadata": {},
   "source": [
    "## Comparison Table: Which Approach for Pairs Trading?\n",
    "\n",
    "| Approach | Complexity | Data Needed | Can Improve Beyond Expert? | Training Time | Best For |\n",
    "|----------|-----------|-------------|---------------------------|---------------|----------|\n",
    "| **Offline RL (CQL)** | ⭐⭐⭐⭐⭐ Very High | Expert demonstrations | ❌ No (conservative) | ⏱️ Long | Real-world safety critical |\n",
    "| **Online RL (PPO/SAC)** | ⭐⭐⭐⭐ High | Just market data | ✅ Yes! | ⏱️⏱️ Longer | Finding optimal strategy |\n",
    "| **Behavior Cloning** | ⭐⭐ Low | Expert demonstrations | ❌ No | ⏱️ Fast | Good enough expert |\n",
    "| **Supervised Learning** | ⭐ Very Low | Expert demonstrations | ❌ No | ⏱️ Very Fast | Simple rule following |\n",
    "| **Hybrid (BC + PPO)** | ⭐⭐⭐ Medium | Both | ✅ Yes! | ⏱️⏱️ Medium | **RECOMMENDED** |\n",
    "\n",
    "## What You Should Actually Do:\n",
    "\n",
    "### For Your Pairs Trading Problem:\n",
    "\n",
    "#### 🏆 **Best Choice: Online RL with Simulated Environment**\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO, SAC\n",
    "\n",
    "# Option A: Use gymnasium\n",
    "env = gym.make('PairsTradingEnv-v0')  # Create custom env\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=1_000_000)\n",
    "\n",
    "# Option B: Use FinRL (designed for trading)\n",
    "from finrl.agents.stablebaselines3 import DRLAgent\n",
    "agent = DRLAgent(env=env)\n",
    "model = agent.get_model(\"ppo\")\n",
    "trained_model = agent.train_model(model, total_timesteps=500_000)\n",
    "```\n",
    "\n",
    "**Why this is better:**\n",
    "- No expert needed (learns from scratch)\n",
    "- Explores different strategies\n",
    "- Optimizes for actual PnL\n",
    "- Can discover better strategies than your rule-based expert\n",
    "- Industry standard for algorithmic trading\n",
    "\n",
    "#### 🥈 **Second Choice: Just Use Your BC Model**\n",
    "\n",
    "You already trained it! It's probably good enough:\n",
    "\n",
    "```python\n",
    "# You already have this working!\n",
    "bc_net.eval()\n",
    "actions = bc_net.predict(states)\n",
    "\n",
    "# BC accuracy was similar to CQL\n",
    "# Much simpler, faster, more interpretable\n",
    "```\n",
    "\n",
    "**When BC is enough:**\n",
    "- Expert strategy is already good\n",
    "- Don't need to beat the expert\n",
    "- Want fast inference\n",
    "- Need interpretability\n",
    "\n",
    "#### 🥉 **Third Choice: Improve Your Rule-Based Strategy**\n",
    "\n",
    "Why use RL at all if your expert is just thresholds?\n",
    "\n",
    "```python\n",
    "# Instead of learning to copy this:\n",
    "if z_score > 1.5:\n",
    "    action = -1\n",
    "\n",
    "# Just optimize the thresholds directly:\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "def backtest_thresholds(entry, exit):\n",
    "    strategy = RuleBasedPairsStrategy(entry, exit)\n",
    "    returns = run_backtest(strategy)\n",
    "    return returns.sharpe_ratio\n",
    "\n",
    "# Find optimal thresholds\n",
    "result = differential_evolution(\n",
    "    lambda x: -backtest_thresholds(x[0], x[1]),\n",
    "    bounds=[(1.0, 3.0), (0.1, 1.0)]\n",
    ")\n",
    "\n",
    "optimal_entry, optimal_exit = result.x\n",
    "```\n",
    "\n",
    "**Why this might be better:**\n",
    "- Much simpler than any ML approach\n",
    "- Interpretable (just two numbers!)\n",
    "- Fast to optimize\n",
    "- Easy to explain to stakeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a78049",
   "metadata": {},
   "source": [
    "## The Real Question: What Are You Trying to Achieve?\n",
    "\n",
    "### Scenario 1: \"I want to automate my existing strategy\"\n",
    "→ **Use Behavior Cloning** (you already have it!)\n",
    "- BC is perfect for this\n",
    "- No need for complex RL\n",
    "- Just deploy your trained BC model\n",
    "\n",
    "### Scenario 2: \"I want to find a BETTER strategy than my rule\"\n",
    "→ **Use Online RL (PPO/SAC)**\n",
    "- Don't constrain yourself to copying rules\n",
    "- Let RL discover optimal policies\n",
    "- Can beat any hand-crafted strategy\n",
    "\n",
    "### Scenario 3: \"I have expert trader data, can't experiment\"\n",
    "→ **Use Offline RL (CQL)**\n",
    "- You have human expert logs\n",
    "- Can't deploy untested strategies\n",
    "- Need conservative learning\n",
    "\n",
    "### Scenario 4: \"I just want something that works quickly\"\n",
    "→ **Optimize your rule-based thresholds**\n",
    "- Simplest approach\n",
    "- Often performs surprisingly well\n",
    "- Easy to understand and debug\n",
    "\n",
    "## Why Offline RL Is Probably Wrong Here:\n",
    "\n",
    "### The Core Issue:\n",
    "\n",
    "```\n",
    "Offline RL is for: \"I have expert data, CAN'T explore\"\n",
    "Your situation is: \"I have data, CAN explore safely\"\n",
    "\n",
    "Using offline RL here is like:\n",
    "- Using a submarine to cross a bridge\n",
    "- Wearing a spacesuit in your house\n",
    "- Using quantum computers for addition\n",
    "\n",
    "It's over-engineering the problem!\n",
    "```\n",
    "\n",
    "### What You Actually Have:\n",
    "\n",
    "1. ✅ Historical market data (perfect simulator)\n",
    "2. ✅ Simple rule-based expert (not sophisticated)\n",
    "3. ✅ Can simulate millions of episodes safely\n",
    "4. ✅ Want to maximize real returns (not imitate)\n",
    "5. ❌ Don't have real expert traders\n",
    "6. ❌ Don't have safety constraints\n",
    "7. ❌ Don't need to be conservative\n",
    "\n",
    "**This screams \"Online RL\" or \"Simple Optimization\", not \"Offline RL\"!**\n",
    "\n",
    "## My Honest Recommendation:\n",
    "\n",
    "### Path 1: Quick & Practical (1 day of work)\n",
    "```python\n",
    "# 1. Optimize your thresholds\n",
    "optimal_params = optimize_thresholds(entry=[1.0, 3.0], exit=[0.1, 1.0])\n",
    "\n",
    "# 2. Maybe add a few more features\n",
    "strategy = EnhancedRuleStrategy(\n",
    "    entry_threshold=optimal_params['entry'],\n",
    "    exit_threshold=optimal_params['exit'],\n",
    "    use_volume=True,\n",
    "    use_momentum=True\n",
    ")\n",
    "\n",
    "# Done! This often beats ML approaches.\n",
    "```\n",
    "\n",
    "### Path 2: Proper RL (1-2 weeks of work)\n",
    "```python\n",
    "# 1. Create environment\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "class PairsTradingEnv(gym.Env):\n",
    "    # Your historical data as environment\n",
    "    pass\n",
    "\n",
    "# 2. Train online RL\n",
    "model = PPO('MlpPolicy', env)\n",
    "model.learn(1_000_000)\n",
    "\n",
    "# 3. Profit! \n",
    "# (Literally, this optimizes for profit)\n",
    "```\n",
    "\n",
    "### Path 3: Keep It Simple (what you have)\n",
    "```python\n",
    "# Your BC model already works!\n",
    "# Deploy it and iterate based on performance\n",
    "# No need for fancy CQL\n",
    "```\n",
    "\n",
    "## Bottom Line:\n",
    "\n",
    "**Offline RL (CQL) is the wrong tool for this job.**\n",
    "\n",
    "You chose it because:\n",
    "- It sounds sophisticated ✗\n",
    "- It's published in fancy papers ✗\n",
    "- It works for other problems ✗\n",
    "\n",
    "You should use:\n",
    "- Online RL: If you want optimal strategy ✓\n",
    "- Simple BC: If current expert is good enough ✓\n",
    "- Threshold optimization: If you want simple & effective ✓\n",
    "\n",
    "The fact that CQL \"doesn't improve much\" is telling you: **this approach doesn't fit the problem!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c7c79a",
   "metadata": {},
   "source": [
    "## Practical Next Steps: What Should You Do Now?\n",
    "\n",
    "### Option A: Quick Win (Do This First) 🚀\n",
    "\n",
    "**1-2 hours of work, likely better results:**\n",
    "\n",
    "```python\n",
    "# Just optimize your thresholds!\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "\n",
    "def objective(params):\n",
    "    entry, exit = params\n",
    "    strategy = RuleBasedPairsStrategy(entry, exit)\n",
    "    \n",
    "    # Backtest on validation set\n",
    "    returns = backtest_all_pairs(strategy, val_intervals)\n",
    "    sharpe = calculate_sharpe(returns)\n",
    "    \n",
    "    return -sharpe  # Minimize negative = maximize sharpe\n",
    "\n",
    "# Find best thresholds\n",
    "result = minimize(\n",
    "    objective,\n",
    "    x0=[1.5, 0.5],  # Your current thresholds\n",
    "    bounds=[(0.5, 3.0), (0.1, 1.5)],\n",
    "    method='Nelder-Mead'\n",
    ")\n",
    "\n",
    "print(f\"Optimal entry: {result.x[0]:.2f}\")\n",
    "print(f\"Optimal exit: {result.x[1]:.2f}\")\n",
    "print(f\"Expected improvement: {-result.fun:.2%}\")\n",
    "```\n",
    "\n",
    "**Expected outcome:** 10-30% better Sharpe ratio with 2 hours of work!\n",
    "\n",
    "### Option B: Proper Online RL (Recommended) 💪\n",
    "\n",
    "**3-5 days of work, discover optimal strategy:**\n",
    "\n",
    "I can help you implement this! Here's the skeleton:\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np\n",
    "\n",
    "class PairsTradingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Gym environment for pairs trading.\n",
    "    Historical data becomes your simulated environment.\n",
    "    \"\"\"\n",
    "    def __init__(self, pair_data, interval_data):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pair_data = pair_data\n",
    "        self.interval_data = interval_data\n",
    "        \n",
    "        # Action space: -1 (short), 0 (neutral), 1 (long)\n",
    "        self.action_space = gym.spaces.Discrete(3)\n",
    "        \n",
    "        # Observation space: your features\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf, \n",
    "            high=np.inf, \n",
    "            shape=(obs_dim,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        # Pick random pair and interval\n",
    "        self.current_pair = random.choice(self.pairs)\n",
    "        self.current_interval = random.choice(self.intervals)\n",
    "        self.timestep = 0\n",
    "        self.position = 0\n",
    "        self.pnl = 0\n",
    "        \n",
    "        return self._get_observation(), {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Convert action from {0,1,2} to {-1,0,1}\n",
    "        action = action - 1\n",
    "        \n",
    "        # Calculate reward (actual PnL)\n",
    "        reward = self._calculate_reward(action)\n",
    "        \n",
    "        # Move to next timestep\n",
    "        self.timestep += 1\n",
    "        done = self.timestep >= len(self.current_data)\n",
    "        \n",
    "        obs = self._get_observation()\n",
    "        \n",
    "        return obs, reward, done, False, {}\n",
    "    \n",
    "    def _calculate_reward(self, action):\n",
    "        # Get current and next prices\n",
    "        prices_t = self.get_prices(self.timestep)\n",
    "        prices_t1 = self.get_prices(self.timestep + 1)\n",
    "        \n",
    "        # Calculate return\n",
    "        ret1 = (prices_t1[0] - prices_t[0]) / prices_t[0]\n",
    "        ret2 = (prices_t1[1] - prices_t[1]) / prices_t[1]\n",
    "        \n",
    "        # Portfolio return based on action\n",
    "        if action == -1:\n",
    "            pnl = -ret1 + ret2\n",
    "        elif action == 1:\n",
    "            pnl = ret1 - ret2\n",
    "        else:\n",
    "            pnl = 0.0\n",
    "        \n",
    "        # Add transaction costs\n",
    "        if action != self.position:\n",
    "            pnl -= 0.0007  # 7 bps round-trip\n",
    "        \n",
    "        self.position = action\n",
    "        return pnl\n",
    "\n",
    "# Create environment\n",
    "env = PairsTradingEnv(pair_interval_data, features_df)\n",
    "\n",
    "# Train with PPO (state-of-the-art for continuous control)\n",
    "model = PPO(\n",
    "    'MlpPolicy',\n",
    "    env,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./ppo_logs/\"\n",
    ")\n",
    "\n",
    "# Train!\n",
    "model.learn(total_timesteps=1_000_000)\n",
    "\n",
    "# Save\n",
    "model.save(\"ppo_pairs_trading\")\n",
    "\n",
    "# Test\n",
    "obs = env.reset()\n",
    "for _ in range(1000):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, _, _ = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "```\n",
    "\n",
    "**Expected outcome:** 20-50% better returns than rule-based strategy!\n",
    "\n",
    "### Option C: Keep Your BC, Forget CQL 😊\n",
    "\n",
    "**0 hours of work:**\n",
    "\n",
    "Your BC model is already trained and works! Just use it:\n",
    "\n",
    "```python\n",
    "# You already have this\n",
    "bc_net.eval()\n",
    "\n",
    "def trade(market_state):\n",
    "    with torch.no_grad():\n",
    "        action = bc_net.predict(market_state)\n",
    "    return action\n",
    "\n",
    "# Deploy it! No need for fancy CQL.\n",
    "```\n",
    "\n",
    "**Expected outcome:** Same as your rule-based strategy (which might be fine!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e51d5",
   "metadata": {},
   "source": [
    "## Summary: The Brutal Truth\n",
    "\n",
    "### Your Current Approach:\n",
    "```\n",
    "Problem: Want to improve trading strategy\n",
    "Solution: Offline RL (CQL)\n",
    "Result: \"Doesn't improve much\"\n",
    "```\n",
    "\n",
    "### Why It's Not Working:\n",
    "\n",
    "1. **Wrong Problem Framing**\n",
    "   - You framed it as: \"Learn from expert demonstrations\"\n",
    "   - It should be: \"Find optimal trading policy\"\n",
    "\n",
    "2. **Wrong Tool**\n",
    "   - Offline RL is for: Can't explore, must stay near expert\n",
    "   - You need: Can explore safely, want to beat expert\n",
    "\n",
    "3. **Wrong Expert**\n",
    "   - Your expert is: Simple rule with 2 thresholds\n",
    "   - Why train a neural network to copy a rule?\n",
    "\n",
    "### What You Should Do:\n",
    "\n",
    "#### If you want EASY + GOOD:\n",
    "```python\n",
    "# Optimize thresholds (2 hours)\n",
    "scipy.optimize.minimize(backtest_objective, [1.5, 0.5])\n",
    "```\n",
    "\n",
    "#### If you want OPTIMAL:\n",
    "```python\n",
    "# Online RL (1 week)\n",
    "env = PairsTradingEnv(historical_data)\n",
    "model = PPO('MlpPolicy', env)\n",
    "model.learn(1_000_000)\n",
    "```\n",
    "\n",
    "#### If you want SIMPLE:\n",
    "```python\n",
    "# Use your BC model (0 hours)\n",
    "# It's already trained and working!\n",
    "```\n",
    "\n",
    "### The Lesson:\n",
    "\n",
    "**Don't use sophisticated methods because they sound impressive.**\n",
    "\n",
    "Use them because they fit the problem:\n",
    "- ✅ Offline RL: Learning from human experts, can't explore\n",
    "- ✅ Online RL: Have simulator, want optimal policy\n",
    "- ✅ Supervised Learning: Have good expert, want to copy it\n",
    "- ❌ Offline RL for trading with simulated data: Wrong fit!\n",
    "\n",
    "### My Recommendation:\n",
    "\n",
    "**Try them in this order:**\n",
    "\n",
    "1. **Optimize thresholds** (2 hours) - Will probably work great\n",
    "2. **If not satisfied, implement Online RL** (1 week) - Will find optimal strategy\n",
    "3. **Only if both fail, try ensemble or other fancy methods**\n",
    "\n",
    "Stop using CQL. It's the wrong tool for this job. The fact that it doesn't improve is the market telling you: \"You're solving the wrong problem!\"\n",
    "\n",
    "---\n",
    "\n",
    "**Want me to help you implement Option A (threshold optimization) or Option B (Online RL env)?** Both will likely give you much better results than offline RL!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
